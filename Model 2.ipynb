{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predict Single Morphology Variable (Sersic n) Based on Multiple Star-formation Variables (M*, SFR and more)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize,hiddenSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, hiddenSize)\n",
    "        self.linear1 = torch.nn.Linear(hiddenSize, hiddenSize)\n",
    "        self.linear2= torch.nn.Linear(hiddenSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "        self.Sigmoid= torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        x= self.linear1(x)\n",
    "        x= self.Sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=StandardScaler().fit_transform(n.reshape(-1,1))\n",
    "n=torch.from_numpy(n).to('cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Prep the input data to go into a DataLoader "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3638, 2])\ntorch.Size([3638, 1])\n(tensor([0.6979, 0.1057], device='cuda:0'), tensor([1.1469], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs=[log_SFR,log_mass]\n",
    "inputs_transformed=[]\n",
    "\n",
    "def data_preparer(inputs):  \n",
    "    \"\"\"\n",
    "    Takes in a list in which each element is an input variable and then preps\n",
    "    it accordingly to return it as one combined GPU pytorch tensor. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #makes all inputs np arrays of np.float32\n",
    "    #and makes scailing to mean 0 and std of 1\n",
    "    for i in range (len(inputs)):\n",
    "        inputs_transformed.append(StandardScaler().fit_transform(np.array(inputs[i],dtype=np.float32).reshape(-1,1))) \n",
    "                                                                                                    \n",
    "    reshape=np.column_stack(inputs_transformed)\n",
    "    out=torch.from_numpy(reshape).to('cuda:0')\n",
    "    return(out)\n",
    "\n",
    "inputs_tensor=data_preparer(inputs)\n",
    "\n",
    "print(np.shape(inputs_tensor))\n",
    "print(np.shape(n))\n",
    "\n",
    "#Create Tensor Datasets \n",
    "train_ds, test_ds, validate_ds=torch.utils.data.random_split(TensorDataset(inputs_tensor,n),[2183,727,728]) #Better way to automate these splits? \n",
    "\n",
    "#Create Data Loaders\n",
    "train_dl=DataLoader(train_ds,batch_size=64,shuffle=True)\n",
    "test_dl=DataLoader(test_ds,batch_size=64,shuffle=True)\n",
    "validate_dl=DataLoader(validate_ds,batch_size=64,shuffle=True)\n",
    "\n",
    "print(train_ds[0])\n",
    "\n"
   ]
  },
  {
   "source": [
    "# The Model (N inputs, 1 output, CUDA Enabled)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0512332916\n",
      "epoch 400, loss 0.4389958679676056\n",
      "epoch 400, loss 0.41286420822143555\n",
      "epoch 400, loss 0.4008233845233917\n",
      "epoch 400, loss 0.6174427270889282\n",
      "epoch 400, loss 0.3712519407272339\n",
      "epoch 400, loss 0.7104957699775696\n",
      "epoch 400, loss 0.37399137020111084\n",
      "epoch 400, loss 0.4280857443809509\n",
      "epoch 400, loss 0.38624340295791626\n",
      "epoch 400, loss 0.40641289949417114\n",
      "epoch 400, loss 0.30524739623069763\n",
      "epoch 400, loss 0.32532042264938354\n",
      "epoch 400, loss 0.3429272472858429\n",
      "epoch 400, loss 0.4553031623363495\n",
      "epoch 400, loss 0.6017602682113647\n",
      "epoch 400, loss 0.24063220620155334\n",
      "epoch 400, loss 0.5068678855895996\n",
      "epoch 400, loss 0.41979438066482544\n",
      "epoch 400, loss 0.5264060497283936\n",
      "epoch 400, loss 0.452410489320755\n",
      "epoch 400, loss 0.48807889223098755\n",
      "epoch 400, loss 0.4526447653770447\n",
      "epoch 400, loss 0.5561189651489258\n",
      "epoch 400, loss 0.4648810625076294\n",
      "epoch 400, loss 0.43251490592956543\n",
      "epoch 400, loss 0.5307060480117798\n",
      "epoch 400, loss 0.3074912428855896\n",
      "epoch 500, loss 0.5780608654022217\n",
      "epoch 500, loss 0.5080133080482483\n",
      "epoch 500, loss 0.3352851867675781\n",
      "epoch 500, loss 0.3466368019580841\n",
      "epoch 500, loss 0.4073168635368347\n",
      "epoch 500, loss 0.4109634757041931\n",
      "epoch 500, loss 0.40598636865615845\n",
      "epoch 500, loss 0.32932305335998535\n",
      "epoch 500, loss 0.4852294921875\n",
      "epoch 500, loss 0.4903419613838196\n",
      "epoch 500, loss 0.3859413266181946\n",
      "epoch 500, loss 0.3992615342140198\n",
      "epoch 500, loss 0.5192570686340332\n",
      "epoch 500, loss 0.3203989863395691\n",
      "epoch 500, loss 0.5663371086120605\n",
      "epoch 500, loss 0.3420127332210541\n",
      "epoch 500, loss 0.3480571508407593\n",
      "epoch 500, loss 0.4025351405143738\n",
      "epoch 500, loss 0.5761804580688477\n",
      "epoch 500, loss 0.4087737202644348\n",
      "epoch 500, loss 0.4846269488334656\n",
      "epoch 500, loss 0.3233942687511444\n",
      "epoch 500, loss 0.3825722336769104\n",
      "epoch 500, loss 0.3916405439376831\n",
      "epoch 500, loss 0.528037428855896\n",
      "epoch 500, loss 0.563188910484314\n",
      "epoch 500, loss 0.36791181564331055\n",
      "epoch 500, loss 0.3486011326313019\n",
      "epoch 500, loss 0.41726139187812805\n",
      "epoch 500, loss 0.4689297676086426\n",
      "epoch 500, loss 0.4018633961677551\n",
      "epoch 500, loss 0.5817177295684814\n",
      "epoch 500, loss 0.6028851270675659\n",
      "epoch 500, loss 0.4038618803024292\n",
      "epoch 500, loss 0.32745179533958435\n",
      "epoch 600, loss 0.508503258228302\n",
      "epoch 600, loss 0.45870864391326904\n",
      "epoch 600, loss 0.23337411880493164\n",
      "epoch 600, loss 0.4178764820098877\n",
      "epoch 600, loss 0.4826359152793884\n",
      "epoch 600, loss 0.3623855710029602\n",
      "epoch 600, loss 0.5161709189414978\n",
      "epoch 600, loss 0.358889102935791\n",
      "epoch 600, loss 0.5104024410247803\n",
      "epoch 600, loss 0.42584553360939026\n",
      "epoch 600, loss 0.4733947217464447\n",
      "epoch 600, loss 0.27402979135513306\n",
      "epoch 600, loss 0.343126118183136\n",
      "epoch 600, loss 0.46365663409233093\n",
      "epoch 600, loss 0.3670543432235718\n",
      "epoch 600, loss 0.5459617376327515\n",
      "epoch 600, loss 0.3425469398498535\n",
      "epoch 600, loss 0.4963449537754059\n",
      "epoch 600, loss 0.43412160873413086\n",
      "epoch 600, loss 0.4974786639213562\n",
      "epoch 600, loss 0.3406662344932556\n",
      "epoch 600, loss 0.5080960392951965\n",
      "epoch 600, loss 0.46011191606521606\n",
      "epoch 600, loss 0.4717445969581604\n",
      "epoch 600, loss 0.4593237042427063\n",
      "epoch 600, loss 0.4424128532409668\n",
      "epoch 600, loss 0.5301805734634399\n",
      "epoch 600, loss 0.3752339482307434\n",
      "epoch 600, loss 0.3881547451019287\n",
      "epoch 600, loss 0.39497655630111694\n",
      "epoch 600, loss 0.4430628716945648\n",
      "epoch 600, loss 0.4674375653266907\n",
      "epoch 600, loss 0.4234883785247803\n",
      "epoch 600, loss 0.5151902437210083\n",
      "epoch 600, loss 0.25713327527046204\n",
      "epoch 700, loss 0.4983294606208801\n",
      "epoch 700, loss 0.47113531827926636\n",
      "epoch 700, loss 0.33449792861938477\n",
      "epoch 700, loss 0.4570922553539276\n",
      "epoch 700, loss 0.4077703356742859\n",
      "epoch 700, loss 0.343308687210083\n",
      "epoch 700, loss 0.3838878273963928\n",
      "epoch 700, loss 0.4201662540435791\n",
      "epoch 700, loss 0.42241370677948\n",
      "epoch 700, loss 0.4505046010017395\n",
      "epoch 700, loss 0.41942116618156433\n",
      "epoch 700, loss 0.45760077238082886\n",
      "epoch 700, loss 0.5149946212768555\n",
      "epoch 700, loss 0.5477604866027832\n",
      "epoch 700, loss 0.43766868114471436\n",
      "epoch 700, loss 0.2503893971443176\n",
      "epoch 700, loss 0.4686558246612549\n",
      "epoch 700, loss 0.41994595527648926\n",
      "epoch 700, loss 0.5113576054573059\n",
      "epoch 700, loss 0.23225735127925873\n",
      "epoch 700, loss 0.6249885559082031\n",
      "epoch 700, loss 0.34055912494659424\n",
      "epoch 700, loss 0.41971105337142944\n",
      "epoch 700, loss 0.25284719467163086\n",
      "epoch 700, loss 0.5261814594268799\n",
      "epoch 700, loss 0.47746795415878296\n",
      "epoch 700, loss 0.5741298198699951\n",
      "epoch 700, loss 0.4768122434616089\n",
      "epoch 700, loss 0.5058126449584961\n",
      "epoch 700, loss 0.47324347496032715\n",
      "epoch 700, loss 0.3396337628364563\n",
      "epoch 700, loss 0.32474827766418457\n",
      "epoch 700, loss 0.299669086933136\n",
      "epoch 700, loss 0.4569753408432007\n",
      "epoch 700, loss 0.417965829372406\n",
      "epoch 800, loss 0.40952998399734497\n",
      "epoch 800, loss 0.345095694065094\n",
      "epoch 800, loss 0.20577529072761536\n",
      "epoch 800, loss 0.37031733989715576\n",
      "epoch 800, loss 0.5111323595046997\n",
      "epoch 800, loss 0.5309057235717773\n",
      "epoch 800, loss 0.5481420159339905\n",
      "epoch 800, loss 0.4566666781902313\n",
      "epoch 800, loss 0.398653507232666\n",
      "epoch 800, loss 0.44476720690727234\n",
      "epoch 800, loss 0.3487979769706726\n",
      "epoch 800, loss 0.6401751041412354\n",
      "epoch 800, loss 0.4575299620628357\n",
      "epoch 800, loss 0.7755447626113892\n",
      "epoch 800, loss 0.29798412322998047\n",
      "epoch 800, loss 0.3454843759536743\n",
      "epoch 800, loss 0.4239323139190674\n",
      "epoch 800, loss 0.46362143754959106\n",
      "epoch 800, loss 0.38195425271987915\n",
      "epoch 800, loss 0.32146310806274414\n",
      "epoch 800, loss 0.4924459457397461\n",
      "epoch 800, loss 0.3882475197315216\n",
      "epoch 800, loss 0.5007656812667847\n",
      "epoch 800, loss 0.42631542682647705\n",
      "epoch 800, loss 0.3647482097148895\n",
      "epoch 800, loss 0.42244869470596313\n",
      "epoch 800, loss 0.35715559124946594\n",
      "epoch 800, loss 0.35182690620422363\n",
      "epoch 800, loss 0.4006851315498352\n",
      "epoch 800, loss 0.5724870562553406\n",
      "epoch 800, loss 0.3370201587677002\n",
      "epoch 800, loss 0.49425047636032104\n",
      "epoch 800, loss 0.3972760736942291\n",
      "epoch 800, loss 0.4617682993412018\n",
      "epoch 800, loss 0.12074872106313705\n",
      "epoch 900, loss 0.4907262325286865\n",
      "epoch 900, loss 0.46526122093200684\n",
      "epoch 900, loss 0.5121874213218689\n",
      "epoch 900, loss 0.38997939229011536\n",
      "epoch 900, loss 0.3509189784526825\n",
      "epoch 900, loss 0.4595804214477539\n",
      "epoch 900, loss 0.3416112959384918\n",
      "epoch 900, loss 0.3335110545158386\n",
      "epoch 900, loss 0.35139650106430054\n",
      "epoch 900, loss 0.33165234327316284\n",
      "epoch 900, loss 0.3706906735897064\n",
      "epoch 900, loss 0.3050163686275482\n",
      "epoch 900, loss 0.349767804145813\n",
      "epoch 900, loss 0.5753574371337891\n",
      "epoch 900, loss 0.42408809065818787\n",
      "epoch 900, loss 0.5485486388206482\n",
      "epoch 900, loss 0.3444250822067261\n",
      "epoch 900, loss 0.35928982496261597\n",
      "epoch 900, loss 0.42206811904907227\n",
      "epoch 900, loss 0.4128365218639374\n",
      "epoch 900, loss 0.37682074308395386\n",
      "epoch 900, loss 0.3811410963535309\n",
      "epoch 900, loss 0.558418333530426\n",
      "epoch 900, loss 0.35717982053756714\n",
      "epoch 900, loss 0.4827539324760437\n",
      "epoch 900, loss 0.4693303108215332\n",
      "epoch 900, loss 0.5163772106170654\n",
      "epoch 900, loss 0.4983176290988922\n",
      "epoch 900, loss 0.4235004782676697\n",
      "epoch 900, loss 0.6026325821876526\n",
      "epoch 900, loss 0.46314457058906555\n",
      "epoch 900, loss 0.5598053932189941\n",
      "epoch 900, loss 0.3478868007659912\n",
      "epoch 900, loss 0.2873741388320923\n",
      "epoch 900, loss 0.2503010332584381\n",
      "epoch 1000, loss 0.46837878227233887\n",
      "epoch 1000, loss 0.34174424409866333\n",
      "epoch 1000, loss 0.43851929903030396\n",
      "epoch 1000, loss 0.3605067729949951\n",
      "epoch 1000, loss 0.5378466844558716\n",
      "epoch 1000, loss 0.39224857091903687\n",
      "epoch 1000, loss 0.44398272037506104\n",
      "epoch 1000, loss 0.28282758593559265\n",
      "epoch 1000, loss 0.37321794033050537\n",
      "epoch 1000, loss 0.7359027862548828\n",
      "epoch 1000, loss 0.4120616316795349\n",
      "epoch 1000, loss 0.49865931272506714\n",
      "epoch 1000, loss 0.44885560870170593\n",
      "epoch 1000, loss 0.4825707972049713\n",
      "epoch 1000, loss 0.5097808837890625\n",
      "epoch 1000, loss 0.4244452714920044\n",
      "epoch 1000, loss 0.6710705161094666\n",
      "epoch 1000, loss 0.30502018332481384\n",
      "epoch 1000, loss 0.4327380359172821\n",
      "epoch 1000, loss 0.2798246741294861\n",
      "epoch 1000, loss 0.3603029251098633\n",
      "epoch 1000, loss 0.3572981357574463\n",
      "epoch 1000, loss 0.4102863073348999\n",
      "epoch 1000, loss 0.4786454439163208\n",
      "epoch 1000, loss 0.44248998165130615\n",
      "epoch 1000, loss 0.3129100799560547\n",
      "epoch 1000, loss 0.5131320953369141\n",
      "epoch 1000, loss 0.24606123566627502\n",
      "epoch 1000, loss 0.40732160210609436\n",
      "epoch 1000, loss 0.4934329092502594\n",
      "epoch 1000, loss 0.3778170049190521\n",
      "epoch 1000, loss 0.34845882654190063\n",
      "epoch 1000, loss 0.3508516848087311\n",
      "epoch 1000, loss 0.43683308362960815\n",
      "epoch 1000, loss 0.7816628217697144\n",
      "epoch 1100, loss 0.4638744294643402\n",
      "epoch 1100, loss 0.4762979745864868\n",
      "epoch 1100, loss 0.3979529142379761\n",
      "epoch 1100, loss 0.3412420153617859\n",
      "epoch 1100, loss 0.28725606203079224\n",
      "epoch 1100, loss 0.28692376613616943\n",
      "epoch 1100, loss 0.37258708477020264\n",
      "epoch 1100, loss 0.25869837403297424\n",
      "epoch 1100, loss 0.2875993251800537\n",
      "epoch 1100, loss 0.38798612356185913\n",
      "epoch 1100, loss 0.6093494892120361\n",
      "epoch 1100, loss 0.3658072352409363\n",
      "epoch 1100, loss 0.3987520933151245\n",
      "epoch 1100, loss 0.4341188073158264\n",
      "epoch 1100, loss 0.5436075925827026\n",
      "epoch 1100, loss 0.4293832778930664\n",
      "epoch 1100, loss 0.37636786699295044\n",
      "epoch 1100, loss 0.6840014457702637\n",
      "epoch 1100, loss 0.44303232431411743\n",
      "epoch 1100, loss 0.47344231605529785\n",
      "epoch 1100, loss 0.6343796849250793\n",
      "epoch 1100, loss 0.4570547044277191\n",
      "epoch 1100, loss 0.44920751452445984\n",
      "epoch 1100, loss 0.37194377183914185\n",
      "epoch 1100, loss 0.4505532383918762\n",
      "epoch 1100, loss 0.47485628724098206\n",
      "epoch 1100, loss 0.46231549978256226\n",
      "epoch 1100, loss 0.4113609790802002\n",
      "epoch 1100, loss 0.206705242395401\n",
      "epoch 1100, loss 0.3963642716407776\n",
      "epoch 1100, loss 0.2994800806045532\n",
      "epoch 1100, loss 0.6505659818649292\n",
      "epoch 1100, loss 0.524687647819519\n",
      "epoch 1100, loss 0.48422011733055115\n",
      "epoch 1100, loss 0.17754976451396942\n",
      "epoch 1200, loss 0.48609012365341187\n",
      "epoch 1200, loss 0.4392460584640503\n",
      "epoch 1200, loss 0.5566791296005249\n",
      "epoch 1200, loss 0.2899647355079651\n",
      "epoch 1200, loss 0.40229958295822144\n",
      "epoch 1200, loss 0.6036192178726196\n",
      "epoch 1200, loss 0.5663074254989624\n",
      "epoch 1200, loss 0.5630314946174622\n",
      "epoch 1200, loss 0.3875690698623657\n",
      "epoch 1200, loss 0.4414610266685486\n",
      "epoch 1200, loss 0.41236037015914917\n",
      "epoch 1200, loss 0.31292724609375\n",
      "epoch 1200, loss 0.30812472105026245\n",
      "epoch 1200, loss 0.3803836703300476\n",
      "epoch 1200, loss 0.4106910228729248\n",
      "epoch 1200, loss 0.28667354583740234\n",
      "epoch 1200, loss 0.4800739884376526\n",
      "epoch 1200, loss 0.5711160898208618\n",
      "epoch 1200, loss 0.42316770553588867\n",
      "epoch 1200, loss 0.513372540473938\n",
      "epoch 1200, loss 0.22765497863292694\n",
      "epoch 1200, loss 0.5380922555923462\n",
      "epoch 1200, loss 0.31513476371765137\n",
      "epoch 1200, loss 0.3269370198249817\n",
      "epoch 1200, loss 0.516502857208252\n",
      "epoch 1200, loss 0.41602763533592224\n",
      "epoch 1200, loss 0.32651007175445557\n",
      "epoch 1200, loss 0.5592941045761108\n",
      "epoch 1200, loss 0.425009161233902\n",
      "epoch 1200, loss 0.34664690494537354\n",
      "epoch 1200, loss 0.3515111207962036\n",
      "epoch 1200, loss 0.44881850481033325\n",
      "epoch 1200, loss 0.5762486457824707\n",
      "epoch 1200, loss 0.4075927436351776\n",
      "epoch 1200, loss 0.15583670139312744\n",
      "epoch 1300, loss 0.34802284836769104\n",
      "epoch 1300, loss 0.22980257868766785\n",
      "epoch 1300, loss 0.2655319571495056\n",
      "epoch 1300, loss 0.45345693826675415\n",
      "epoch 1300, loss 0.39438483119010925\n",
      "epoch 1300, loss 0.3822632431983948\n",
      "epoch 1300, loss 0.49309319257736206\n",
      "epoch 1300, loss 0.573426365852356\n",
      "epoch 1300, loss 0.39549732208251953\n",
      "epoch 1300, loss 0.4651980400085449\n",
      "epoch 1300, loss 0.5944132804870605\n",
      "epoch 1300, loss 0.4408820569515228\n",
      "epoch 1300, loss 0.45988357067108154\n",
      "epoch 1300, loss 0.5412064790725708\n",
      "epoch 1300, loss 0.44775140285491943\n",
      "epoch 1300, loss 0.4854753911495209\n",
      "epoch 1300, loss 0.39046385884284973\n",
      "epoch 1300, loss 0.45312342047691345\n",
      "epoch 1300, loss 0.42157453298568726\n",
      "epoch 1300, loss 0.36620140075683594\n",
      "epoch 1300, loss 0.3831818401813507\n",
      "epoch 1300, loss 0.4685806930065155\n",
      "epoch 1300, loss 0.5331840515136719\n",
      "epoch 1300, loss 0.388529896736145\n",
      "epoch 1300, loss 0.32834237813949585\n",
      "epoch 1300, loss 0.4237614870071411\n",
      "epoch 1300, loss 0.353241503238678\n",
      "epoch 1300, loss 0.5353078842163086\n",
      "epoch 1300, loss 0.45911645889282227\n",
      "epoch 1300, loss 0.4917200803756714\n",
      "epoch 1300, loss 0.42756587266921997\n",
      "epoch 1300, loss 0.5883431434631348\n",
      "epoch 1300, loss 0.4154675602912903\n",
      "epoch 1300, loss 0.4976625442504883\n",
      "epoch 1300, loss 0.184920996427536\n",
      "epoch 1400, loss 0.6637347340583801\n",
      "epoch 1400, loss 0.44823455810546875\n",
      "epoch 1400, loss 0.48811596632003784\n",
      "epoch 1400, loss 0.5051325559616089\n",
      "epoch 1400, loss 0.5071897506713867\n",
      "epoch 1400, loss 0.34438851475715637\n",
      "epoch 1400, loss 0.28737348318099976\n",
      "epoch 1400, loss 0.37990182638168335\n",
      "epoch 1400, loss 0.35206514596939087\n",
      "epoch 1400, loss 0.3531041741371155\n",
      "epoch 1400, loss 0.3441564440727234\n",
      "epoch 1400, loss 0.36324554681777954\n",
      "epoch 1400, loss 0.4813275635242462\n",
      "epoch 1400, loss 0.42962008714675903\n",
      "epoch 1400, loss 0.5032617449760437\n",
      "epoch 1400, loss 0.39235377311706543\n",
      "epoch 1400, loss 0.4806193709373474\n",
      "epoch 1400, loss 0.38766777515411377\n",
      "epoch 1400, loss 0.3632151186466217\n",
      "epoch 1400, loss 0.380359947681427\n",
      "epoch 1400, loss 0.3738245964050293\n",
      "epoch 1400, loss 0.582981526851654\n",
      "epoch 1400, loss 0.42687153816223145\n",
      "epoch 1400, loss 0.40437716245651245\n",
      "epoch 1400, loss 0.34925955533981323\n",
      "epoch 1400, loss 0.30119431018829346\n",
      "epoch 1400, loss 0.46790245175361633\n",
      "epoch 1400, loss 0.49212440848350525\n",
      "epoch 1400, loss 0.3579091429710388\n",
      "epoch 1400, loss 0.5030815601348877\n",
      "epoch 1400, loss 0.5075698494911194\n",
      "epoch 1400, loss 0.4275866448879242\n",
      "epoch 1400, loss 0.27634304761886597\n",
      "epoch 1400, loss 0.46943899989128113\n",
      "epoch 1400, loss 0.4028550684452057\n",
      "epoch 1500, loss 0.4666804075241089\n",
      "epoch 1500, loss 0.48669716715812683\n",
      "epoch 1500, loss 0.4802733361721039\n",
      "epoch 1500, loss 0.43896931409835815\n",
      "epoch 1500, loss 0.39200401306152344\n",
      "epoch 1500, loss 0.42446571588516235\n",
      "epoch 1500, loss 0.36567965149879456\n",
      "epoch 1500, loss 0.6522817015647888\n",
      "epoch 1500, loss 0.4313843548297882\n",
      "epoch 1500, loss 0.2957070469856262\n",
      "epoch 1500, loss 0.38230758905410767\n",
      "epoch 1500, loss 0.25345003604888916\n",
      "epoch 1500, loss 0.3388505280017853\n",
      "epoch 1500, loss 0.37690579891204834\n",
      "epoch 1500, loss 0.500959038734436\n",
      "epoch 1500, loss 0.27409106492996216\n",
      "epoch 1500, loss 0.4072345495223999\n",
      "epoch 1500, loss 0.33478987216949463\n",
      "epoch 1500, loss 0.7661146521568298\n",
      "epoch 1500, loss 0.3441225290298462\n",
      "epoch 1500, loss 0.4981878697872162\n",
      "epoch 1500, loss 0.4382284879684448\n",
      "epoch 1500, loss 0.48889631032943726\n",
      "epoch 1500, loss 0.4267857074737549\n",
      "epoch 1500, loss 0.5013540983200073\n",
      "epoch 1500, loss 0.5427791476249695\n",
      "epoch 1500, loss 0.6527231931686401\n",
      "epoch 1500, loss 0.25772595405578613\n",
      "epoch 1500, loss 0.5040221810340881\n",
      "epoch 1500, loss 0.35771632194519043\n",
      "epoch 1500, loss 0.26991814374923706\n",
      "epoch 1500, loss 0.48945415019989014\n",
      "epoch 1500, loss 0.42514511942863464\n",
      "epoch 1500, loss 0.63242506980896\n",
      "epoch 1500, loss 0.13634760677814484\n",
      "epoch 1600, loss 0.4564385712146759\n",
      "epoch 1600, loss 0.4285714030265808\n",
      "epoch 1600, loss 0.3225571811199188\n",
      "epoch 1600, loss 0.22496066987514496\n",
      "epoch 1600, loss 0.5704165101051331\n",
      "epoch 1600, loss 0.3180534541606903\n",
      "epoch 1600, loss 0.48713287711143494\n",
      "epoch 1600, loss 0.3734406530857086\n",
      "epoch 1600, loss 0.5513590574264526\n",
      "epoch 1600, loss 0.5132774710655212\n",
      "epoch 1600, loss 0.4129222631454468\n",
      "epoch 1600, loss 0.40419554710388184\n",
      "epoch 1600, loss 0.3875667452812195\n",
      "epoch 1600, loss 0.551774799823761\n",
      "epoch 1600, loss 0.3568218946456909\n",
      "epoch 1600, loss 0.3820717930793762\n",
      "epoch 1600, loss 0.6090767979621887\n",
      "epoch 1600, loss 0.6390643119812012\n",
      "epoch 1600, loss 0.3758392333984375\n",
      "epoch 1600, loss 0.4222450852394104\n",
      "epoch 1600, loss 0.39143070578575134\n",
      "epoch 1600, loss 0.5340467691421509\n",
      "epoch 1600, loss 0.4209229350090027\n",
      "epoch 1600, loss 0.3116941750049591\n",
      "epoch 1600, loss 0.43023332953453064\n",
      "epoch 1600, loss 0.4520656168460846\n",
      "epoch 1600, loss 0.33212992548942566\n",
      "epoch 1600, loss 0.3069370985031128\n",
      "epoch 1600, loss 0.4324427843093872\n",
      "epoch 1600, loss 0.23851870000362396\n",
      "epoch 1600, loss 0.5048540830612183\n",
      "epoch 1600, loss 0.5184677839279175\n",
      "epoch 1600, loss 0.4954656958580017\n",
      "epoch 1600, loss 0.4773857295513153\n",
      "epoch 1600, loss 0.1273587942123413\n",
      "epoch 1700, loss 0.34477144479751587\n",
      "epoch 1700, loss 0.5068914294242859\n",
      "epoch 1700, loss 0.4036639928817749\n",
      "epoch 1700, loss 0.4682118594646454\n",
      "epoch 1700, loss 0.22632654011249542\n",
      "epoch 1700, loss 0.36461371183395386\n",
      "epoch 1700, loss 0.3277348279953003\n",
      "epoch 1700, loss 0.5143797993659973\n",
      "epoch 1700, loss 0.4578608274459839\n",
      "epoch 1700, loss 0.4154309630393982\n",
      "epoch 1700, loss 0.43088558316230774\n",
      "epoch 1700, loss 0.32416826486587524\n",
      "epoch 1700, loss 0.5501135587692261\n",
      "epoch 1700, loss 0.6497830748558044\n",
      "epoch 1700, loss 0.3986799418926239\n",
      "epoch 1700, loss 0.4370962083339691\n",
      "epoch 1700, loss 0.6978837251663208\n",
      "epoch 1700, loss 0.5890710353851318\n",
      "epoch 1700, loss 0.3480682373046875\n",
      "epoch 1700, loss 0.38956189155578613\n",
      "epoch 1700, loss 0.42107468843460083\n",
      "epoch 1700, loss 0.5411673784255981\n",
      "epoch 1700, loss 0.42978930473327637\n",
      "epoch 1700, loss 0.36895236372947693\n",
      "epoch 1700, loss 0.39439496397972107\n",
      "epoch 1700, loss 0.4297742545604706\n",
      "epoch 1700, loss 0.5608264803886414\n",
      "epoch 1700, loss 0.40570130944252014\n",
      "epoch 1700, loss 0.20641177892684937\n",
      "epoch 1700, loss 0.37409982085227966\n",
      "epoch 1700, loss 0.42528021335601807\n",
      "epoch 1700, loss 0.4680899381637573\n",
      "epoch 1700, loss 0.31983304023742676\n",
      "epoch 1700, loss 0.35952043533325195\n",
      "epoch 1700, loss 0.07423751801252365\n",
      "epoch 1800, loss 0.3145429491996765\n",
      "epoch 1800, loss 0.4827491343021393\n",
      "epoch 1800, loss 0.3696064352989197\n",
      "epoch 1800, loss 0.3780202865600586\n",
      "epoch 1800, loss 0.34669870138168335\n",
      "epoch 1800, loss 0.5428009033203125\n",
      "epoch 1800, loss 0.5852604508399963\n",
      "epoch 1800, loss 0.27787721157073975\n",
      "epoch 1800, loss 0.5038069486618042\n",
      "epoch 1800, loss 0.560789167881012\n",
      "epoch 1800, loss 0.4700447618961334\n",
      "epoch 1800, loss 0.37334537506103516\n",
      "epoch 1800, loss 0.45247989892959595\n",
      "epoch 1800, loss 0.3422672152519226\n",
      "epoch 1800, loss 0.6529712080955505\n",
      "epoch 1800, loss 0.34330785274505615\n",
      "epoch 1800, loss 0.5172378420829773\n",
      "epoch 1800, loss 0.447201132774353\n",
      "epoch 1800, loss 0.4108580946922302\n",
      "epoch 1800, loss 0.3075895309448242\n",
      "epoch 1800, loss 0.44357216358184814\n",
      "epoch 1800, loss 0.4467613101005554\n",
      "epoch 1800, loss 0.3045462965965271\n",
      "epoch 1800, loss 0.3652139902114868\n",
      "epoch 1800, loss 0.3435750901699066\n",
      "epoch 1800, loss 0.4915456771850586\n",
      "epoch 1800, loss 0.37455829977989197\n",
      "epoch 1800, loss 0.4847725033760071\n",
      "epoch 1800, loss 0.3742964565753937\n",
      "epoch 1800, loss 0.33985042572021484\n",
      "epoch 1800, loss 0.5016740560531616\n",
      "epoch 1800, loss 0.44594472646713257\n",
      "epoch 1800, loss 0.41027921438217163\n",
      "epoch 1800, loss 0.4610109329223633\n",
      "epoch 1800, loss 0.4775155186653137\n",
      "epoch 1900, loss 0.5840818881988525\n",
      "epoch 1900, loss 0.3621767461299896\n",
      "epoch 1900, loss 0.33028459548950195\n",
      "epoch 1900, loss 0.45255863666534424\n",
      "epoch 1900, loss 0.5433187484741211\n",
      "epoch 1900, loss 0.4065846800804138\n",
      "epoch 1900, loss 0.40051913261413574\n",
      "epoch 1900, loss 0.4947811961174011\n",
      "epoch 1900, loss 0.3251149654388428\n",
      "epoch 1900, loss 0.5459156632423401\n",
      "epoch 1900, loss 0.360156387090683\n",
      "epoch 1900, loss 0.20180299878120422\n",
      "epoch 1900, loss 0.2825436592102051\n",
      "epoch 1900, loss 0.4448857307434082\n",
      "epoch 1900, loss 0.43461018800735474\n",
      "epoch 1900, loss 0.36346346139907837\n",
      "epoch 1900, loss 0.519156813621521\n",
      "epoch 1900, loss 0.6114990711212158\n",
      "epoch 1900, loss 0.4828161597251892\n",
      "epoch 1900, loss 0.6257392168045044\n",
      "epoch 1900, loss 0.4067079424858093\n",
      "epoch 1900, loss 0.4741874039173126\n",
      "epoch 1900, loss 0.4176562428474426\n",
      "epoch 1900, loss 0.4401434063911438\n",
      "epoch 1900, loss 0.31698814034461975\n",
      "epoch 1900, loss 0.5006589889526367\n",
      "epoch 1900, loss 0.5933904051780701\n",
      "epoch 1900, loss 0.3220774233341217\n",
      "epoch 1900, loss 0.33581793308258057\n",
      "epoch 1900, loss 0.460004597902298\n",
      "epoch 1900, loss 0.32160186767578125\n",
      "epoch 1900, loss 0.3958941400051117\n",
      "epoch 1900, loss 0.32218989729881287\n",
      "epoch 1900, loss 0.4550943672657013\n",
      "epoch 1900, loss 0.66800856590271\n",
      "tensor([[ 0.5361],\n",
      "        [ 0.5362],\n",
      "        [-0.6742],\n",
      "        ...,\n",
      "        [-0.7003],\n",
      "        [-0.7003],\n",
      "        [-0.1192]], device='cuda:0')\n",
      "0.8356808423995972\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputDim=len(inputs)\n",
    "outputDim=1 \n",
    "hiddenSize=50\n",
    "learningRate=0.1\n",
    "\n",
    "model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "#Training Loop\n",
    "epochs=2000\n",
    "\n",
    "epoch_array=np.zeros(epochs)\n",
    "loss_array=np.zeros(epochs)\n",
    "for epoch in range(epochs): #Forward Pass and loss\n",
    "    for xb,yb in train_dl:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = model(xb)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, yb)\n",
    "        # print(loss)\n",
    "        # get gradients w.r.t to parameters, (backward pass)\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_array[epoch]=epoch \n",
    "        loss_array[epoch]=loss.item()\n",
    "\n",
    "        if epoch %100==0:\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    predicted = model(inputs_tensor)\n",
    "    print(predicted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7ff1cc772d90>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib qt\n",
    "fig=plt.figure()\n",
    "ax= plt.axes(projection='3d')\n",
    "ax.scatter3D(inputs_tensor.cpu().detach().numpy()[:,0].flatten(),inputs_tensor.cpu().detach().numpy()[:,1].flatten(),predicted.cpu().detach().numpy().flatten(),'red')\n",
    "ax.scatter3D(inputs_tensor.cpu().detach().numpy()[:,0],inputs_tensor.cpu().detach().numpy()[:,1],n.cpu().detach().numpy(),alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),n.cpu().detach().numpy(),alpha=0.2)\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),predicted.cpu().detach().numpy())\n"
   ]
  },
  {
   "source": [
    "# Changing Epoch while Keeping Learning Rate and Hidden Size Constant "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done rep  0\n",
      "Done rep  1\n",
      "Done rep  2\n",
      "Done rep  3\n",
      "Done rep  4\n",
      "Done rep  5\n",
      "Done rep  6\n",
      "Done rep  7\n",
      "Done rep  8\n",
      "Done rep  9\n",
      "Done rep  10\n",
      "Done rep  11\n",
      "Done rep  12\n",
      "Done rep  13\n",
      "Done rep  14\n",
      "Done rep  15\n",
      "Done rep  16\n",
      "Done rep  17\n",
      "Done rep  18\n",
      "Done rep  19\n",
      "Done rep  20\n",
      "Done rep  21\n",
      "Done rep  22\n",
      "Done rep  23\n",
      "Done rep  24\n",
      "Done rep  25\n",
      "Done rep  26\n",
      "Done rep  27\n",
      "Done rep  28\n",
      "Done rep  29\n",
      "Done rep  30\n",
      "Done rep  31\n",
      "Done rep  32\n",
      "Done rep  33\n",
      "Done rep  34\n",
      "Done rep  35\n",
      "Done rep  36\n",
      "Done rep  37\n",
      "Done rep  38\n",
      "Done rep  39\n",
      "Done rep  40\n",
      "Done rep  41\n",
      "Done rep  42\n",
      "Done rep  43\n",
      "Done rep  44\n",
      "Done rep  45\n",
      "Done rep  46\n",
      "Done rep  47\n",
      "Done rep  48\n",
      "Done rep  49\n"
     ]
    }
   ],
   "source": [
    "inputDim=len(inputs)\n",
    "outputDim=1 \n",
    "hiddenSize=50\n",
    "learningRate=0.1\n",
    "\n",
    "model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "reps=50\n",
    "last_loss=np.zeros(reps) #Stores final loss returned value after training loop is done for a given number of epoch\n",
    "epoch_range=np.linspace(400,4000,num=reps,dtype=int)\n",
    "\n",
    "\n",
    "for i in range (reps):\n",
    "    #Training Loop\n",
    "    epochs=epoch_range[i]\n",
    "    \n",
    "    # epoch_array=np.zeros(epochs)\n",
    "    loss_array=np.zeros(epochs)\n",
    "    for epoch in range(epochs): #Forward Pass and loss\n",
    "        for xb,yb in train_dl:\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(xb)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = criterion(outputs, yb)\n",
    "            # print(loss)\n",
    "            # get gradients w.r.t to parameters, (backward pass)\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # epoch_array[epoch]=epoch \n",
    "            loss_array[epoch]=loss.item()\n",
    "    \n",
    " \n",
    "    last_loss[i]=loss_array[-1] #Store final loss value\n",
    "    print('Done rep ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff213273a50>]"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "plt.title('Number of Epochs and Final Value of Loss')\n",
    "plt.ylabel('Last Loss Value')\n",
    "plt.xlabel('Number of Epochs Trained')\n",
    "plt.plot(epoch_range,last_loss)"
   ]
  },
  {
   "source": [
    "# Changing Hidden Size while Keeping Learning Rate and Epoch Constant "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done rep  0\n",
      "Done rep  1\n",
      "Done rep  2\n",
      "Done rep  3\n",
      "Done rep  4\n",
      "Done rep  5\n",
      "Done rep  6\n",
      "Done rep  7\n",
      "Done rep  8\n",
      "Done rep  9\n",
      "Done rep  10\n",
      "Done rep  11\n",
      "Done rep  12\n",
      "Done rep  13\n",
      "Done rep  14\n",
      "Done rep  15\n",
      "Done rep  16\n",
      "Done rep  17\n",
      "Done rep  18\n",
      "Done rep  19\n",
      "Done rep  20\n",
      "Done rep  21\n",
      "Done rep  22\n",
      "Done rep  23\n",
      "Done rep  24\n",
      "Done rep  25\n",
      "Done rep  26\n",
      "Done rep  27\n",
      "Done rep  28\n",
      "Done rep  29\n",
      "Done rep  30\n",
      "Done rep  31\n",
      "Done rep  32\n",
      "Done rep  33\n",
      "Done rep  34\n",
      "Done rep  35\n",
      "Done rep  36\n",
      "Done rep  37\n",
      "Done rep  38\n",
      "Done rep  39\n",
      "Done rep  40\n",
      "Done rep  41\n",
      "Done rep  42\n",
      "Done rep  43\n",
      "Done rep  44\n",
      "Done rep  45\n",
      "Done rep  46\n",
      "Done rep  47\n",
      "Done rep  48\n",
      "Done rep  49\n"
     ]
    }
   ],
   "source": [
    "reps=50\n",
    "hidden_size_range=np.linspace(10,200,num=reps,dtype=int)\n",
    "last_loss_hidden_size=np.zeros(reps) #Stores final loss returned value after training loop is done for a given number of epoch\n",
    "\n",
    "for i in range (reps):\n",
    "\n",
    "    inputDim=len(inputs)\n",
    "    outputDim=1 \n",
    "    hiddenSize=hidden_size_range[i]\n",
    "    learningRate=0.1\n",
    "\n",
    "    model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "    ##### For GPU #######\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "    #Training Loop\n",
    "    epochs=1000 \n",
    "\n",
    "    \n",
    "    loss_array=np.zeros(epochs)\n",
    "    for epoch in range(epochs): #Forward Pass and loss\n",
    "        for xb,yb in train_dl:\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(xb)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = criterion(outputs, yb)\n",
    "            # print(loss)\n",
    "            # get gradients w.r.t to parameters, (backward pass)\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_array[epoch]=loss.item()\n",
    "\n",
    "\n",
    "    last_loss_hidden_size[i]=loss_array[-1] #Store final loss value\n",
    "    print('Done rep ',i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff181538150>]"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "plt.title('Hidden Size and Final Value of Loss')\n",
    "plt.ylabel('Last Loss Value')\n",
    "plt.xlabel('Hidden Size')\n",
    "plt.plot(hidden_size_range,last_loss)"
   ]
  },
  {
   "source": [
    "# Changing Hidden Size while Keeping Learning Rate and Epoch Constant "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-5779c957d737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reps=50\n",
    "learningRate_range=np.linspace(0.01,1,num=reps,dtype=int)\n",
    "last_loss_learningRate=np.zeros(reps) #Stores final loss returned value after training loop is done for a given number of epoch\n",
    "\n",
    "for i in range (reps):\n",
    "\n",
    "    inputDim=len(inputs)\n",
    "    outputDim=1 \n",
    "    hiddenSize= 50\n",
    "    learningRate=learningRate_range[i]\n",
    "\n",
    "    model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "    ##### For GPU #######\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "    #Training Loop\n",
    "    epochs= 1000 \n",
    "\n",
    "    \n",
    "    loss_array=np.zeros(epochs)\n",
    "    for epoch in range(epochs): #Forward Pass and loss\n",
    "        for xb,yb in train_dl:\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(xb)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = criterion(outputs, yb)\n",
    "            # print(loss)\n",
    "            # get gradients w.r.t to parameters, (backward pass)\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_array[epoch]=loss.item()\n",
    "\n",
    "\n",
    "    last_loss_learningRate[i]=loss_array[-1] #Store final loss value\n",
    "    print('Done rep ',i)"
   ]
  }
 ]
}