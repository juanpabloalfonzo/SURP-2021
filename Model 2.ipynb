{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predict Single Morphology Variable (Sersic n) Based on Multiple Star-formation Variables (M*, SFR and more)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,r2_score\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('ticks')\n",
    "%matplotlib qt\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize,hiddenSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, hiddenSize)\n",
    "        self.linear1 = torch.nn.Linear(hiddenSize, hiddenSize)\n",
    "        self.linear2 = torch.nn.Linear(hiddenSize,hiddenSize)\n",
    "        self.linear3 = torch.nn.Linear(hiddenSize,hiddenSize)\n",
    "        self.linear4= torch.nn.Linear(hiddenSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "        self.ReLU6= torch.nn.ReLU6()\n",
    "        self.Sigmoid= torch.nn.Sigmoid()\n",
    "        self.ELU=torch.nn.ELU()\n",
    "        self.LeakyReLU=torch.nn.LeakyReLU()\n",
    "        self.PReLU=torch.nn.PReLU()\n",
    "        self.RReLU= torch.nn.RReLU()\n",
    "        self.CELU=torch.nn.CELU()\n",
    "        self.SELU=torch.nn.SELU()\n",
    "        self.Softsign=torch.nn.Softsign()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.Softsign(x)\n",
    "        # x= self.linear1(x)\n",
    "        # x= self.Sigmoid(x)\n",
    "        # x= self.linear2(x)\n",
    "        # x= self.Sigmoid(x)\n",
    "        # x = self.linear3(x)\n",
    "        # x= self.Sigmoid(x)\n",
    "        x=self.linear4(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Data from Schema Table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "log_SFR=np.array(log_SFR,dtype=np.float32)\n",
    "log_SFR=StandardScaler().fit_transform(log_SFR.reshape(-1,1))\n",
    "log_SFR=torch.from_numpy(log_SFR).to('cuda:0')\n",
    "\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "# n=np.array(n,dtype=np.float32)\n",
    "# n=StandardScaler().fit_transform(n.reshape(-1,1))\n",
    "# n=torch.from_numpy(n).to('cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prep the input data to go into a DataLoader "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "inputs=[n]\n",
    "inputs_transformed=[]\n",
    "\n",
    "def data_preparer(inputs):  \n",
    "    \"\"\"\n",
    "    Takes in a list in which each element is an input variable and then preps\n",
    "    it accordingly to return it as one combined GPU pytorch tensor. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #makes all inputs np arrays of np.float32\n",
    "    #and makes scailing to mean 0 and std of 1\n",
    "    for i in range (len(inputs)):\n",
    "        inputs_transformed.append(StandardScaler().fit_transform(np.array(inputs[i],dtype=np.float32).reshape(-1,1))) \n",
    "                                                                                                    \n",
    "    reshape=np.column_stack(inputs_transformed)\n",
    "    out=torch.from_numpy(reshape).to('cuda:0')\n",
    "    return(out)\n",
    "\n",
    "inputs_tensor=data_preparer(inputs)\n",
    "\n",
    "print(np.shape(inputs_tensor))\n",
    "print(np.shape(n))\n",
    "\n",
    "#Create Tensor Datasets \n",
    "train_ds, test_ds, validate_ds=torch.utils.data.random_split(TensorDataset(inputs_tensor,log_SFR),[2183,727,728]) #Better way to automate these splits? \n",
    "\n",
    "#Create Data Loaders\n",
    "train_dl=DataLoader(train_ds,batch_size=64,shuffle=True)\n",
    "test_dl=DataLoader(test_ds,batch_size=64,shuffle=True)\n",
    "validate_dl=DataLoader(validate_ds,batch_size=64,shuffle=True)\n",
    "\n",
    "print(train_ds[0])\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3638, 1])\n",
      "(3638,)\n",
      "(tensor([-1.1627], device='cuda:0'), tensor([0.8928], device='cuda:0'))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Model (N inputs, 1 output, CUDA Enabled)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "inputDim=len(inputs)\n",
    "outputDim=1 \n",
    "hiddenSize=50\n",
    "learningRate=0.1\n",
    "\n",
    "model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "#Training Loop\n",
    "epochs=1800\n",
    "\n",
    "epoch_array=np.zeros(epochs)\n",
    "loss_array=np.zeros(epochs)\n",
    "for epoch in range(epochs): #Forward Pass and loss\n",
    "    for xb,yb in train_dl:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = model(xb)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, yb)\n",
    "        # print(loss)\n",
    "        # get gradients w.r.t to parameters, (backward pass)\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_array[epoch]=epoch \n",
    "        loss_array[epoch]=loss.item()\n",
    "\n",
    "        if epoch %200==0:\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    predicted = model(inputs_tensor)\n",
    "    print(predicted)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0, loss 1.104949951171875\n",
      "epoch 0, loss 1.4006925821304321\n",
      "epoch 0, loss 1.0645980834960938\n",
      "epoch 0, loss 0.8183423280715942\n",
      "epoch 0, loss 1.1405807733535767\n",
      "epoch 0, loss 1.133859395980835\n",
      "epoch 0, loss 1.705528736114502\n",
      "epoch 0, loss 1.107046365737915\n",
      "epoch 0, loss 0.9649994373321533\n",
      "epoch 0, loss 1.3919175863265991\n",
      "epoch 0, loss 1.0924874544143677\n",
      "epoch 0, loss 0.6765725612640381\n",
      "epoch 0, loss 1.1120597124099731\n",
      "epoch 0, loss 1.1490452289581299\n",
      "epoch 0, loss 0.8737654685974121\n",
      "epoch 0, loss 0.8999685645103455\n",
      "epoch 0, loss 0.82944655418396\n",
      "epoch 0, loss 1.093996524810791\n",
      "epoch 0, loss 0.8679929971694946\n",
      "epoch 0, loss 0.9959161281585693\n",
      "epoch 0, loss 0.8277015686035156\n",
      "epoch 0, loss 1.2117087841033936\n",
      "epoch 0, loss 0.9333987236022949\n",
      "epoch 0, loss 0.6077208518981934\n",
      "epoch 0, loss 1.0722203254699707\n",
      "epoch 0, loss 0.8419002294540405\n",
      "epoch 0, loss 1.2697412967681885\n",
      "epoch 0, loss 0.749763011932373\n",
      "epoch 0, loss 1.0136630535125732\n",
      "epoch 0, loss 1.3159120082855225\n",
      "epoch 0, loss 1.2951102256774902\n",
      "epoch 0, loss 0.8296008110046387\n",
      "epoch 0, loss 0.7955601215362549\n",
      "epoch 0, loss 1.1806228160858154\n",
      "epoch 0, loss 1.3402869701385498\n",
      "epoch 200, loss 1.0047527551651\n",
      "epoch 200, loss 0.7053911685943604\n",
      "epoch 200, loss 0.9371840953826904\n",
      "epoch 200, loss 1.3572282791137695\n",
      "epoch 200, loss 0.9035017490386963\n",
      "epoch 200, loss 0.9611369967460632\n",
      "epoch 200, loss 0.9450287818908691\n",
      "epoch 200, loss 0.9841753840446472\n",
      "epoch 200, loss 1.0186245441436768\n",
      "epoch 200, loss 0.751314640045166\n",
      "epoch 200, loss 0.9140809774398804\n",
      "epoch 200, loss 0.7091542482376099\n",
      "epoch 200, loss 0.8925867080688477\n",
      "epoch 200, loss 1.0790963172912598\n",
      "epoch 200, loss 0.9669461846351624\n",
      "epoch 200, loss 0.8072453737258911\n",
      "epoch 200, loss 0.9073027968406677\n",
      "epoch 200, loss 0.680266261100769\n",
      "epoch 200, loss 1.0755255222320557\n",
      "epoch 200, loss 1.0488070249557495\n",
      "epoch 200, loss 0.7515042424201965\n",
      "epoch 200, loss 0.620402455329895\n",
      "epoch 200, loss 1.168372631072998\n",
      "epoch 200, loss 1.1058595180511475\n",
      "epoch 200, loss 0.5470272302627563\n",
      "epoch 200, loss 0.8479814529418945\n",
      "epoch 200, loss 1.0362415313720703\n",
      "epoch 200, loss 1.1117104291915894\n",
      "epoch 200, loss 1.1011148691177368\n",
      "epoch 200, loss 0.7564823031425476\n",
      "epoch 200, loss 0.9859887361526489\n",
      "epoch 200, loss 1.0490772724151611\n",
      "epoch 200, loss 1.250672698020935\n",
      "epoch 200, loss 0.8905200362205505\n",
      "epoch 200, loss 0.672159731388092\n",
      "epoch 400, loss 1.4434387683868408\n",
      "epoch 400, loss 0.9246869087219238\n",
      "epoch 400, loss 0.980518639087677\n",
      "epoch 400, loss 1.0170083045959473\n",
      "epoch 400, loss 1.3341810703277588\n",
      "epoch 400, loss 0.9293042421340942\n",
      "epoch 400, loss 1.0055063962936401\n",
      "epoch 400, loss 0.9831070303916931\n",
      "epoch 400, loss 0.8692718744277954\n",
      "epoch 400, loss 0.8632073402404785\n",
      "epoch 400, loss 0.6528720855712891\n",
      "epoch 400, loss 0.8610399961471558\n",
      "epoch 400, loss 0.8913203477859497\n",
      "epoch 400, loss 0.931359052658081\n",
      "epoch 400, loss 1.1311407089233398\n",
      "epoch 400, loss 0.7206555008888245\n",
      "epoch 400, loss 0.9829859733581543\n",
      "epoch 400, loss 0.9864052534103394\n",
      "epoch 400, loss 1.1897125244140625\n",
      "epoch 400, loss 0.9366458058357239\n",
      "epoch 400, loss 1.2452752590179443\n",
      "epoch 400, loss 0.9607642292976379\n",
      "epoch 400, loss 0.8679811954498291\n",
      "epoch 400, loss 1.1331232786178589\n",
      "epoch 400, loss 0.7546100616455078\n",
      "epoch 400, loss 0.8224478960037231\n",
      "epoch 400, loss 1.0115914344787598\n",
      "epoch 400, loss 0.8092822432518005\n",
      "epoch 400, loss 1.0805745124816895\n",
      "epoch 400, loss 0.8113094568252563\n",
      "epoch 400, loss 1.059516429901123\n",
      "epoch 400, loss 0.7612189650535583\n",
      "epoch 400, loss 0.8925545811653137\n",
      "epoch 400, loss 1.4214880466461182\n",
      "epoch 400, loss 1.824950933456421\n",
      "epoch 600, loss 1.0946422815322876\n",
      "epoch 600, loss 0.7898405194282532\n",
      "epoch 600, loss 0.9519839286804199\n",
      "epoch 600, loss 0.8648196458816528\n",
      "epoch 600, loss 0.8272264003753662\n",
      "epoch 600, loss 0.7265613079071045\n",
      "epoch 600, loss 0.7063827514648438\n",
      "epoch 600, loss 0.9231642484664917\n",
      "epoch 600, loss 1.1060965061187744\n",
      "epoch 600, loss 0.8300843834877014\n",
      "epoch 600, loss 0.9601221680641174\n",
      "epoch 600, loss 1.1545462608337402\n",
      "epoch 600, loss 1.0506556034088135\n",
      "epoch 600, loss 0.9161301851272583\n",
      "epoch 600, loss 0.7253832817077637\n",
      "epoch 600, loss 1.0738073587417603\n",
      "epoch 600, loss 0.8122923374176025\n",
      "epoch 600, loss 1.1597869396209717\n",
      "epoch 600, loss 1.7050111293792725\n",
      "epoch 600, loss 0.9375011324882507\n",
      "epoch 600, loss 0.8230219483375549\n",
      "epoch 600, loss 0.8592068552970886\n",
      "epoch 600, loss 1.5315680503845215\n",
      "epoch 600, loss 0.7872329950332642\n",
      "epoch 600, loss 0.6827993392944336\n",
      "epoch 600, loss 0.8239774703979492\n",
      "epoch 600, loss 1.1298766136169434\n",
      "epoch 600, loss 0.7035579681396484\n",
      "epoch 600, loss 0.8575806617736816\n",
      "epoch 600, loss 1.4729788303375244\n",
      "epoch 600, loss 0.9943181276321411\n",
      "epoch 600, loss 1.0962865352630615\n",
      "epoch 600, loss 1.3717834949493408\n",
      "epoch 600, loss 0.8906049728393555\n",
      "epoch 600, loss 0.7191740274429321\n",
      "epoch 800, loss 0.7495970129966736\n",
      "epoch 800, loss 0.8028484582901001\n",
      "epoch 800, loss 1.0627882480621338\n",
      "epoch 800, loss 1.0022504329681396\n",
      "epoch 800, loss 1.3204790353775024\n",
      "epoch 800, loss 0.7580535411834717\n",
      "epoch 800, loss 0.7290049195289612\n",
      "epoch 800, loss 0.5791791677474976\n",
      "epoch 800, loss 0.8963946104049683\n",
      "epoch 800, loss 1.0584418773651123\n",
      "epoch 800, loss 0.9967852830886841\n",
      "epoch 800, loss 0.45706620812416077\n",
      "epoch 800, loss 1.1583116054534912\n",
      "epoch 800, loss 0.9753533601760864\n",
      "epoch 800, loss 1.1232109069824219\n",
      "epoch 800, loss 1.0283093452453613\n",
      "epoch 800, loss 0.960257351398468\n",
      "epoch 800, loss 0.843702495098114\n",
      "epoch 800, loss 0.8008835911750793\n",
      "epoch 800, loss 0.7233875393867493\n",
      "epoch 800, loss 0.7533841133117676\n",
      "epoch 800, loss 1.1538957357406616\n",
      "epoch 800, loss 0.5911272764205933\n",
      "epoch 800, loss 1.2038719654083252\n",
      "epoch 800, loss 0.8932777643203735\n",
      "epoch 800, loss 0.9721355438232422\n",
      "epoch 800, loss 0.8197237253189087\n",
      "epoch 800, loss 1.0656511783599854\n",
      "epoch 800, loss 0.7913033962249756\n",
      "epoch 800, loss 0.914420485496521\n",
      "epoch 800, loss 0.9588251113891602\n",
      "epoch 800, loss 0.9514618515968323\n",
      "epoch 800, loss 1.024947166442871\n",
      "epoch 800, loss 0.9558702707290649\n",
      "epoch 800, loss 1.6109344959259033\n",
      "epoch 1000, loss 1.1910985708236694\n",
      "epoch 1000, loss 0.8808445930480957\n",
      "epoch 1000, loss 1.0618607997894287\n",
      "epoch 1000, loss 0.9094767570495605\n",
      "epoch 1000, loss 0.8171550631523132\n",
      "epoch 1000, loss 1.0575730800628662\n",
      "epoch 1000, loss 1.1679582595825195\n",
      "epoch 1000, loss 1.1123604774475098\n",
      "epoch 1000, loss 1.2160345315933228\n",
      "epoch 1000, loss 1.1407833099365234\n",
      "epoch 1000, loss 1.1190564632415771\n",
      "epoch 1000, loss 1.0698519945144653\n",
      "epoch 1000, loss 0.9858123660087585\n",
      "epoch 1000, loss 0.7793845534324646\n",
      "epoch 1000, loss 1.0002555847167969\n",
      "epoch 1000, loss 0.5791860818862915\n",
      "epoch 1000, loss 0.6196717619895935\n",
      "epoch 1000, loss 1.1020381450653076\n",
      "epoch 1000, loss 0.9659390449523926\n",
      "epoch 1000, loss 0.9722062349319458\n",
      "epoch 1000, loss 0.8621107339859009\n",
      "epoch 1000, loss 1.0010184049606323\n",
      "epoch 1000, loss 0.7225151658058167\n",
      "epoch 1000, loss 1.0678154230117798\n",
      "epoch 1000, loss 0.8008432388305664\n",
      "epoch 1000, loss 0.9701330065727234\n",
      "epoch 1000, loss 0.8987639546394348\n",
      "epoch 1000, loss 0.7826638221740723\n",
      "epoch 1000, loss 0.9475656151771545\n",
      "epoch 1000, loss 1.1948072910308838\n",
      "epoch 1000, loss 0.7950396537780762\n",
      "epoch 1000, loss 0.9053044319152832\n",
      "epoch 1000, loss 0.8010779619216919\n",
      "epoch 1000, loss 1.0810351371765137\n",
      "epoch 1000, loss 1.261054515838623\n",
      "epoch 1200, loss 0.8039480447769165\n",
      "epoch 1200, loss 1.222118616104126\n",
      "epoch 1200, loss 1.0513904094696045\n",
      "epoch 1200, loss 0.9585134983062744\n",
      "epoch 1200, loss 0.9824272990226746\n",
      "epoch 1200, loss 0.6909239292144775\n",
      "epoch 1200, loss 1.1795315742492676\n",
      "epoch 1200, loss 0.6824436187744141\n",
      "epoch 1200, loss 1.3461577892303467\n",
      "epoch 1200, loss 0.8357712626457214\n",
      "epoch 1200, loss 0.7310380935668945\n",
      "epoch 1200, loss 1.0767967700958252\n",
      "epoch 1200, loss 0.9594779014587402\n",
      "epoch 1200, loss 0.8119300603866577\n",
      "epoch 1200, loss 0.6726806163787842\n",
      "epoch 1200, loss 1.0006103515625\n",
      "epoch 1200, loss 0.9723435640335083\n",
      "epoch 1200, loss 0.7929410934448242\n",
      "epoch 1200, loss 0.7174661159515381\n",
      "epoch 1200, loss 0.7985047101974487\n",
      "epoch 1200, loss 0.891892671585083\n",
      "epoch 1200, loss 1.4077801704406738\n",
      "epoch 1200, loss 1.1920948028564453\n",
      "epoch 1200, loss 0.7992414236068726\n",
      "epoch 1200, loss 0.792757511138916\n",
      "epoch 1200, loss 0.5619509220123291\n",
      "epoch 1200, loss 0.9149606227874756\n",
      "epoch 1200, loss 0.9017171859741211\n",
      "epoch 1200, loss 0.9142595529556274\n",
      "epoch 1200, loss 1.1770869493484497\n",
      "epoch 1200, loss 0.8756194710731506\n",
      "epoch 1200, loss 0.9758277535438538\n",
      "epoch 1200, loss 0.8894376754760742\n",
      "epoch 1200, loss 0.5947837829589844\n",
      "epoch 1200, loss 2.424136161804199\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib qt\n",
    "fig=plt.figure()\n",
    "ax= plt.axes(projection='3d')\n",
    "ax.scatter3D(inputs_tensor.cpu().detach().numpy()[:,0].flatten(),inputs_tensor.cpu().detach().numpy()[:,1].flatten(),predicted.cpu().detach().numpy().flatten(),label='Model Predictions')\n",
    "\n",
    "ax.scatter3D(inputs_tensor.cpu().detach().numpy()[:,0],inputs_tensor.cpu().detach().numpy()[:,1],n.cpu().detach().numpy(),alpha=0.5,label='Actual Data')\n",
    "\n",
    "ax.set_xlabel('log SFR')\n",
    "ax.set_ylabel('log Mass')\n",
    "ax.set_zlabel('Sersic n')\n",
    "ax.set_title('Mass and SFR as Indicators of Galaxy Morphology')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),n.cpu().detach().numpy(),alpha=0.2)\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),predicted.cpu().detach().numpy())\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc11f1019d0>"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size=64 \n",
    "device='cuda'\n",
    "\n",
    "all_truths_train = [] \n",
    "all_preds_train = [] \n",
    "for (data,target) in train_dl:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_train.append(target.cpu().detach().numpy())\n",
    "    all_preds_train.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_train=len(all_truths_train)-1\n",
    "\n",
    "remainder_train=len(all_truths_train[incomplete_batch_id_train])\n",
    "\n",
    "total_values_train=(len(all_truths_train)*batch_size)-(batch_size-remainder_train)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_train_array=np.zeros(total_values_train)\n",
    "all_preds_train_array=np.zeros(total_values_train)\n",
    "k=0\n",
    "while k < total_values_train:\n",
    "    for i in range(len(all_truths_train)):\n",
    "        if i<incomplete_batch_id_train:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_train\n",
    "            for j in range(remainder_train):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "all_truths_test = [] \n",
    "all_preds_test = [] \n",
    "for (data,target) in test_dl:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_test.append(target.cpu().detach().numpy())\n",
    "    all_preds_test.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_test=len(all_truths_test)-1\n",
    "\n",
    "remainder_test=len(all_truths_test[incomplete_batch_id_test])\n",
    "\n",
    "\n",
    "total_values_test=(len(all_truths_test)*batch_size)-(batch_size-remainder_test)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_test_array=np.zeros(total_values_test)\n",
    "all_preds_test_array=np.zeros(total_values_test)\n",
    "k=0\n",
    "while k < total_values_test:\n",
    "    for i in range(len(all_truths_test)):\n",
    "        if i<incomplete_batch_id_test:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_test\n",
    "            for j in range(remainder_test):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "print(all_truths_test[3][43])\n",
    "print(all_preds_test[3][43])\n",
    "print(all_truths_test_array[235])\n",
    "print(all_preds_test_array[235])\n",
    "# all_truths_test=all_truths_test_array\n",
    "# all_preds_test=all_preds_test_array\n",
    "\n",
    "\n",
    "\n",
    "def MSE(pred,truth,n):\n",
    "    return((1/n)*np.sum((pred-truth)**2))\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.7357753]\n",
      "[-0.58383316]\n",
      "-0.7357752919197083\n",
      "-0.5838331580162048\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "r2_train=r2_score(all_truths_train_array,all_preds_train_array)\n",
    "r2_test=r2_score(all_truths_test_array,all_preds_test_array)\n",
    "MSE_train=MSE(all_preds_train_array,all_truths_train_array,len(all_preds_train_array))\n",
    "MSE_test=MSE(all_preds_test_array,all_truths_test_array,len(all_truths_test_array))\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.suptitle('Predicting Log SFR Based on Sersic Index',fontsize=16,weight='bold')\n",
    "plt.subplot(1,2,1) \n",
    "plt.title('Test Data Set')\n",
    "plt.scatter(all_truths_test_array, all_preds_test_array,color = 'b', alpha = 0.1*7/3)\n",
    "plt.xlabel('Test True Values')\n",
    "plt.ylabel('Test Predicted Value')\n",
    "plt.xlim(left=-4)\n",
    "plt.text(0.9,-0.85,'R$^2$='+ str(round(r2_test,4)),fontsize=14)\n",
    "plt.text(0.9,-0.90,'MSE='+ str(round(MSE_test,4)),fontsize=14)\n",
    "plt.plot([-1,1],[-1,1],'r--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Train Data Set')\n",
    "plt.scatter(all_truths_train_array, all_preds_train_array,color = 'k', alpha = 0.1)\n",
    "plt.xlabel('Train True Values')\n",
    "plt.ylabel('Train Predicted Value')\n",
    "plt.plot([-1,1],[-1,1],'r--') \n",
    "plt.text(0.9,-0.85,'R$^2$='+str(round(r2_train,4)),fontsize=14)\n",
    "plt.text(0.9,-0.90,'MSE='+ str(round(MSE_train,4)),fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('/home/juanp/Documents/SURP-2021/Plots/Model 2/SFR and n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chaning/looping over Hyperparamters below "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changing Epoch while Keeping Learning Rate and Hidden Size Constant "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "inputDim=len(inputs)\n",
    "outputDim=1 \n",
    "hiddenSize=50\n",
    "learningRate=0.1\n",
    "\n",
    "model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "reps=50\n",
    "last_loss=np.zeros(reps) #Stores final loss returned value after training loop is done for a given number of epoch\n",
    "epoch_range=np.linspace(400,4000,num=reps,dtype=int)\n",
    "\n",
    "\n",
    "for i in range (reps):\n",
    "    #Training Loop\n",
    "    epochs=epoch_range[i]\n",
    "    \n",
    "    # epoch_array=np.zeros(epochs)\n",
    "    loss_array=np.zeros(epochs)\n",
    "    for epoch in range(epochs): #Forward Pass and loss\n",
    "        for xb,yb in train_dl:\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(xb)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = criterion(outputs, yb)\n",
    "            # print(loss)\n",
    "            # get gradients w.r.t to parameters, (backward pass)\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # epoch_array[epoch]=epoch \n",
    "            loss_array[epoch]=loss.item()\n",
    "    \n",
    " \n",
    "    last_loss[i]=loss_array[-1] #Store final loss value\n",
    "    print('Done rep ',i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done rep  0\n",
      "Done rep  1\n",
      "Done rep  2\n",
      "Done rep  3\n",
      "Done rep  4\n",
      "Done rep  5\n",
      "Done rep  6\n",
      "Done rep  7\n",
      "Done rep  8\n",
      "Done rep  9\n",
      "Done rep  10\n",
      "Done rep  11\n",
      "Done rep  12\n",
      "Done rep  13\n",
      "Done rep  14\n",
      "Done rep  15\n",
      "Done rep  16\n",
      "Done rep  17\n",
      "Done rep  18\n",
      "Done rep  19\n",
      "Done rep  20\n",
      "Done rep  21\n",
      "Done rep  22\n",
      "Done rep  23\n",
      "Done rep  24\n",
      "Done rep  25\n",
      "Done rep  26\n",
      "Done rep  27\n",
      "Done rep  28\n",
      "Done rep  29\n",
      "Done rep  30\n",
      "Done rep  31\n",
      "Done rep  32\n",
      "Done rep  33\n",
      "Done rep  34\n",
      "Done rep  35\n",
      "Done rep  36\n",
      "Done rep  37\n",
      "Done rep  38\n",
      "Done rep  39\n",
      "Done rep  40\n",
      "Done rep  41\n",
      "Done rep  42\n",
      "Done rep  43\n",
      "Done rep  44\n",
      "Done rep  45\n",
      "Done rep  46\n",
      "Done rep  47\n",
      "Done rep  48\n",
      "Done rep  49\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "plt.title('Number of Epochs and Final Value of Loss')\n",
    "plt.ylabel('Last Loss Value')\n",
    "plt.xlabel('Number of Epochs Trained')\n",
    "plt.plot(epoch_range,last_loss)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff213273a50>]"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changing Hidden Size while Keeping Learning Rate and Epoch Constant "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "reps=50\n",
    "hidden_size_range=np.linspace(10,200,num=reps,dtype=int)\n",
    "last_loss_hidden_size=np.zeros(reps) #Stores final loss returned value after training loop is done for a given number of epoch\n",
    "\n",
    "for i in range (reps):\n",
    "\n",
    "    inputDim=len(inputs)\n",
    "    outputDim=1 \n",
    "    hiddenSize=hidden_size_range[i]\n",
    "    learningRate=0.1\n",
    "\n",
    "    model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "    ##### For GPU #######\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "    #Training Loop\n",
    "    epochs=1000 \n",
    "\n",
    "    \n",
    "    loss_array=np.zeros(epochs)\n",
    "    for epoch in range(epochs): #Forward Pass and loss\n",
    "        for xb,yb in train_dl:\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(xb)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = criterion(outputs, yb)\n",
    "            # print(loss)\n",
    "            # get gradients w.r.t to parameters, (backward pass)\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_array[epoch]=loss.item()\n",
    "\n",
    "\n",
    "    last_loss_hidden_size[i]=loss_array[-1] #Store final loss value\n",
    "    print('Done rep ',i)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done rep  0\n",
      "Done rep  1\n",
      "Done rep  2\n",
      "Done rep  3\n",
      "Done rep  4\n",
      "Done rep  5\n",
      "Done rep  6\n",
      "Done rep  7\n",
      "Done rep  8\n",
      "Done rep  9\n",
      "Done rep  10\n",
      "Done rep  11\n",
      "Done rep  12\n",
      "Done rep  13\n",
      "Done rep  14\n",
      "Done rep  15\n",
      "Done rep  16\n",
      "Done rep  17\n",
      "Done rep  18\n",
      "Done rep  19\n",
      "Done rep  20\n",
      "Done rep  21\n",
      "Done rep  22\n",
      "Done rep  23\n",
      "Done rep  24\n",
      "Done rep  25\n",
      "Done rep  26\n",
      "Done rep  27\n",
      "Done rep  28\n",
      "Done rep  29\n",
      "Done rep  30\n",
      "Done rep  31\n",
      "Done rep  32\n",
      "Done rep  33\n",
      "Done rep  34\n",
      "Done rep  35\n",
      "Done rep  36\n",
      "Done rep  37\n",
      "Done rep  38\n",
      "Done rep  39\n",
      "Done rep  40\n",
      "Done rep  41\n",
      "Done rep  42\n",
      "Done rep  43\n",
      "Done rep  44\n",
      "Done rep  45\n",
      "Done rep  46\n",
      "Done rep  47\n",
      "Done rep  48\n",
      "Done rep  49\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "plt.title('Hidden Size and Final Value of Loss')\n",
    "plt.ylabel('Last Loss Value')\n",
    "plt.xlabel('Hidden Size')\n",
    "plt.plot(hidden_size_range,last_loss_hidden_size)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff18397d210>]"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changing Hidden Size while Keeping Learning Rate and Epoch Constant "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "reps=50\n",
    "learningRate_range=np.linspace(0.01,1,num=reps,dtype=float)\n",
    "last_loss_learningRate=np.zeros(reps) #Stores final loss returned value after training loop is done for a given number of epoch\n",
    "\n",
    "for i in range (reps):\n",
    "\n",
    "    inputDim=len(inputs)\n",
    "    outputDim=1 \n",
    "    hiddenSize= 50\n",
    "    learningRate=learningRate_range[i]\n",
    "\n",
    "    model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "    ##### For GPU #######\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "    #Training Loop\n",
    "    epochs= 1000 \n",
    "\n",
    "    \n",
    "    loss_array=np.zeros(epochs)\n",
    "    for epoch in range(epochs): #Forward Pass and loss\n",
    "        for xb,yb in train_dl:\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(xb)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = criterion(outputs, yb)\n",
    "            # print(loss)\n",
    "            # get gradients w.r.t to parameters, (backward pass)\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_array[epoch]=loss.item()\n",
    "\n",
    "\n",
    "    last_loss_learningRate[i]=loss_array[-1] #Store final loss value\n",
    "    print('Done rep ',i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done rep  0\n",
      "Done rep  1\n",
      "Done rep  2\n",
      "Done rep  3\n",
      "Done rep  4\n",
      "Done rep  5\n",
      "Done rep  6\n",
      "Done rep  7\n",
      "Done rep  8\n",
      "Done rep  9\n",
      "Done rep  10\n",
      "Done rep  11\n",
      "Done rep  12\n",
      "Done rep  13\n",
      "Done rep  14\n",
      "Done rep  15\n",
      "Done rep  16\n",
      "Done rep  17\n",
      "Done rep  18\n",
      "Done rep  19\n",
      "Done rep  20\n",
      "Done rep  21\n",
      "Done rep  22\n",
      "Done rep  23\n",
      "Done rep  24\n",
      "Done rep  25\n",
      "Done rep  26\n",
      "Done rep  27\n",
      "Done rep  28\n",
      "Done rep  29\n",
      "Done rep  30\n",
      "Done rep  31\n",
      "Done rep  32\n",
      "Done rep  33\n",
      "Done rep  34\n",
      "Done rep  35\n",
      "Done rep  36\n",
      "Done rep  37\n",
      "Done rep  38\n",
      "Done rep  39\n",
      "Done rep  40\n",
      "Done rep  41\n",
      "Done rep  42\n",
      "Done rep  43\n",
      "Done rep  44\n",
      "Done rep  45\n",
      "Done rep  46\n",
      "Done rep  47\n",
      "Done rep  48\n",
      "Done rep  49\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "plt.title('Learning Rate and Final Value of Loss')\n",
    "plt.ylabel('Last Loss Value')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.plot(learningRate_range,last_loss_learningRate)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff17a9f9b50>]"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}