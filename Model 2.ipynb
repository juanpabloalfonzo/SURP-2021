{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predict Single Morphology Variable (Sersic n) Based on Multiple Star-formation Variables (M*, SFR and more)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize,hiddenSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, hiddenSize)\n",
    "        self.linear1 = torch.nn.Linear(hiddenSize, hiddenSize)\n",
    "        self.linear2= torch.nn.Linear(hiddenSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "        self.Sigmoid= torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        x= self.linear1(x)\n",
    "        x= self.Sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Prep the input data to go into a DataLoader "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3638, 2])\ntorch.Size([3638, 1])\n3638\n(tensor([-1.0401, 10.4696], device='cuda:0'), tensor([1.4706], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs=[log_SFR,log_mass]\n",
    "\n",
    "def data_preparer(inputs):  \n",
    "    \"\"\"\n",
    "    Takes in a list in which each element is an input variable and then preps\n",
    "    it accordingly to return it as one combined GPU pytorch tensor. \n",
    "    \"\"\"\n",
    "    for i in range (len(inputs)):\n",
    "        inputs[i]=np.array(inputs[i],dtype=np.float32) #makes all inputs np arrays of np.float 32\n",
    "    reshape=np.column_stack(inputs)\n",
    "    out=torch.from_numpy(reshape).to('cuda:0')\n",
    "    return(out)\n",
    "\n",
    "inputs_tensor=data_preparer(inputs)\n",
    "\n",
    "print(np.shape(inputs_tensor))\n",
    "print(np.shape(n))\n",
    "print(len(n))\n",
    "\n",
    "#Create Tensor Datasets \n",
    "train_ds, test_ds, validate_ds=torch.utils.data.random_split(TensorDataset(inputs_tensor,n),[2183,727,728]) #Better way to automate these splits? \n",
    "\n",
    "#Create Data Loaders\n",
    "train_dl=DataLoader(train_ds,batch_size=64,shuffle=True)\n",
    "test_dl=DataLoader(test_ds,batch_size=64,shuffle=True)\n",
    "validate_dl=DataLoader(validate_ds,batch_size=64,shuffle=True)\n",
    "\n",
    "print(train_ds[0])\n",
    "\n"
   ]
  },
  {
   "source": [
    "# The Model (N inputs, 1 output, CUDA Enabled)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4123, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4003, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4930, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5388, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4973, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1423, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1629, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6699, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0543, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5456, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1557, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2564, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7410, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9167, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1621, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6535, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8607, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9175, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9987, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3842, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9594, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6904, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1559, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0425, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6451, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1961, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9098, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.5915, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.3933, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2463, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2630, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8096, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2644, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1924, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1330, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8652, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3698, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1463, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4871, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7555, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6504, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1447, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4864, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4091, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2603, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4454, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4956, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2161, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3810, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8038, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6123, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7187, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.3602, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4685, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1087, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1183, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2843, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(3.4815, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8315, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7936, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.8849, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4620, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5696, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8274, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2825, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9850, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4142, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7333, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0894, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6970, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7362, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2711, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4090, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4954, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0142, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0018, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2230, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.3528, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2572, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8973, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1966, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9044, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3808, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(3.0403, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7317, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1084, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2140, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.5923, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.9184, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8758, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2349, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7666, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6939, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4121, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8848, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5011, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6435, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7503, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.8134, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2482, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6226, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.8963, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9235, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2970, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0161, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7042, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0167, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4842, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5214, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1213, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.9524, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9128, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2038, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7300, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.7161, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2770, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.4906, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5234, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.6718, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7491, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3415, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9287, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.5819, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9169, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8316, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3192, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1829, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.7448, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6946, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8968, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.8627, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9575, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9744, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8216, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7756, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1205, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4350, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5551, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5943, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0717, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8522, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1039, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0946, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4255, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6187, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5965, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4751, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0143, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.4874, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3748, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9937, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4394, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0750, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5098, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2643, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3486, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2618, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7610, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5709, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6320, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8785, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7789, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8501, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2578, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4529, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1035, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6335, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4460, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2706, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6708, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5267, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6305, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0980, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.9230, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4163, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1007, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7977, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4102, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9218, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4339, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1199, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1625, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(3.0589, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.3163, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5665, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8393, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1466, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7337, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.3101, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8767, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8233, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7339, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0100, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1655, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6370, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9616, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.7395, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4833, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8613, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0739, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8441, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4494, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5958, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0172, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9102, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0284, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2351, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5681, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4918, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5686, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7064, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6230, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0346, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4038, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6577, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3094, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2532, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0822, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3115, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3567, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2460, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.9854, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0859, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4244, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0722, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7040, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8822, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1570, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7140, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2863, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.3025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7362, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3488, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4322, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7872, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5073, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9632, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2876, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7841, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9778, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7431, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4630, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3537, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5536, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7141, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7636, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.6896, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8907, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9439, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9014, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9397, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1737, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9651, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7703, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7535, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.8882, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0768, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1856, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2692, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1157, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4652, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3858, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9314, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7947, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6580, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8048, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4799, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2069, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1070, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6434, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4111, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.4471, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3344, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1426, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4839, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4266, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9857, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6140, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5339, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6159, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6584, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0424, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2648, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8986, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5809, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5727, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4574, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9372, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0180, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2508, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0987, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7661, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0750, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1812, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4259, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.7433, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6258, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0496, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1203, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4232, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.0900, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2820, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7229, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2954, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.4366, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.3393, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9482, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.8467, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2863, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.6269, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.1861, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.7611, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.9263, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.2800, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.2230, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(1.5438, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.6829, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor([[4.7618],\n",
      "        [4.8108],\n",
      "        [1.6087],\n",
      "        ...,\n",
      "        [1.5988],\n",
      "        [1.8565],\n",
      "        [1.8569]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputDim=len(inputs)\n",
    "outputDim=1 \n",
    "hiddenSize=50\n",
    "learningRate=0.1\n",
    "\n",
    "model = linearRegression(inputDim, outputDim,hiddenSize)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "#Training Loop\n",
    "epochs=1000\n",
    "\n",
    "epoch_array=np.zeros(epochs)\n",
    "loss_array=np.zeros(epochs)\n",
    "for epoch in range(epochs): #Forward Pass and loss\n",
    "    for xb,yb in train_dl:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = model(xb)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, yb)\n",
    "        # print(loss)\n",
    "        # get gradients w.r.t to parameters, (backward pass)\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_array[epoch]=epoch \n",
    "        loss_array[epoch]=loss.item()\n",
    "\n",
    "        if epoch %100==0:\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    predicted = model(inputs_tensor)\n",
    "    print(predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f1f08d48a50>"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib qt\n",
    "fig=plt.figure()\n",
    "ax= plt.axes(projection='3d')\n",
    "ax.scatter3D(inputs_tensor.cpu().detach().numpy()[:,0].flatten(),inputs_tensor.cpu().detach().numpy()[:,1].flatten(),predicted.cpu().detach().numpy().flatten(),'red')\n",
    "ax.scatter3D(inputs_tensor.cpu().detach().numpy()[:,0],inputs_tensor.cpu().detach().numpy()[:,1],n.cpu().detach().numpy(),alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),n.cpu().detach().numpy(),alpha=0.2)\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),predicted.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1f181d0450>]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "plt.plot(epoch_array,np.log(loss_array))"
   ]
  }
 ]
}