{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predict Single Morphology Variable (Sersic n) Based on Multiple Star-formation Variables (M*, SFR and more)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.linear1 = torch.nn.Linear(outputSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Prep the input data to go into a DataLoader "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3638, 2])\ntorch.Size([3638, 1])\n3638\n(tensor([-1.6233,  9.4534], device='cuda:0'), tensor([0.9313], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs=[log_SFR,log_mass]\n",
    "\n",
    "def data_preparer(inputs):  \n",
    "    \"\"\"\n",
    "    Takes in a list in which each element is an input variable and then preps\n",
    "    it accordingly to return it as one combined GPU pytorch tensor. \n",
    "    \"\"\"\n",
    "    for i in range (len(inputs)):\n",
    "        inputs[i]=np.array(inputs[i],dtype=np.float32) #makes all inputs np arrays of np.float 32\n",
    "    reshape=np.column_stack(inputs)\n",
    "    out=torch.from_numpy(reshape).to('cuda:0')\n",
    "    return(out)\n",
    "\n",
    "inputs_tensor=data_preparer(inputs)\n",
    "\n",
    "print(np.shape(inputs_tensor))\n",
    "print(np.shape(n))\n",
    "print(len(n))\n",
    "\n",
    "#Create Tensor Datasets \n",
    "train_ds, test_ds, validate_ds=torch.utils.data.random_split(TensorDataset(inputs_tensor,n),[2183,727,728]) #Better way to automate these splits? \n",
    "\n",
    "#Create Data Loaders\n",
    "train_dl=DataLoader(train_ds,batch_size=64,shuffle=True)\n",
    "test_dl=DataLoader(test_ds,batch_size=64,shuffle=True)\n",
    "validate_dl=DataLoader(validate_ds,batch_size=64,shuffle=True)\n",
    "\n",
    "print(train_ds[0])\n",
    "\n"
   ]
  },
  {
   "source": [
    "# The Model (N inputs, 1 output, CUDA Enabled)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ad_fn=<MseLossBackward>)\n",
      "epoch 993, loss 3.434541702270508\n",
      "tensor(3.5450, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 993, loss 3.5449767112731934\n",
      "tensor(3.5765, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 993, loss 3.576530694961548\n",
      "tensor(3.9714, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.9713735580444336\n",
      "tensor(3.4719, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.471881151199341\n",
      "tensor(2.9453, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 2.945272922515869\n",
      "tensor(3.9143, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.9143404960632324\n",
      "tensor(3.2244, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.224400520324707\n",
      "tensor(3.4652, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.4651551246643066\n",
      "tensor(3.8150, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.814954996109009\n",
      "tensor(4.0856, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.085593223571777\n",
      "tensor(4.0325, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.032464504241943\n",
      "tensor(3.2083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.2083048820495605\n",
      "tensor(4.1762, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.176168441772461\n",
      "tensor(3.2597, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.2597250938415527\n",
      "tensor(3.6176, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.6176371574401855\n",
      "tensor(3.9334, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.933434247970581\n",
      "tensor(3.5091, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.5091030597686768\n",
      "tensor(4.8489, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.848949909210205\n",
      "tensor(3.7755, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.7754881381988525\n",
      "tensor(3.7946, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.7946410179138184\n",
      "tensor(3.7449, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.744938373565674\n",
      "tensor(4.0112, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.011176586151123\n",
      "tensor(3.1689, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.168856620788574\n",
      "tensor(3.7952, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.795224189758301\n",
      "tensor(3.7361, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.736140489578247\n",
      "tensor(3.2938, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.2938156127929688\n",
      "tensor(3.6306, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.6305747032165527\n",
      "tensor(3.8805, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.880462169647217\n",
      "tensor(3.2638, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.263751745223999\n",
      "tensor(4.3459, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.3459367752075195\n",
      "tensor(3.4711, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.4711453914642334\n",
      "tensor(3.8224, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.822368860244751\n",
      "tensor(4.1012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.101187229156494\n",
      "tensor(3.1606, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.1606431007385254\n",
      "tensor(3.9292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.9291744232177734\n",
      "tensor(3.2863, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 3.2863075733184814\n",
      "tensor(4.7357, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 994, loss 4.735682964324951\n",
      "tensor(3.8427, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.8427324295043945\n",
      "tensor(3.6654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.6653547286987305\n",
      "tensor(3.6756, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.675625801086426\n",
      "tensor(3.3833, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.3832521438598633\n",
      "tensor(3.3538, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.353756904602051\n",
      "tensor(3.9456, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.945648670196533\n",
      "tensor(3.6850, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.6850013732910156\n",
      "tensor(3.3572, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.357226848602295\n",
      "tensor(3.6166, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.616626739501953\n",
      "tensor(3.6898, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.689845085144043\n",
      "tensor(3.8625, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.8624861240386963\n",
      "tensor(3.5216, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.5215532779693604\n",
      "tensor(3.8559, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.8558969497680664\n",
      "tensor(3.2800, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.279953956604004\n",
      "tensor(3.9879, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.9879021644592285\n",
      "tensor(3.3738, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.3737504482269287\n",
      "tensor(4.2642, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 4.264211654663086\n",
      "tensor(3.2571, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.2571051120758057\n",
      "tensor(4.6288, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 4.628772258758545\n",
      "tensor(3.2495, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.24947452545166\n",
      "tensor(3.1119, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.1119370460510254\n",
      "tensor(4.2681, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 4.268128395080566\n",
      "tensor(3.7002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.7002077102661133\n",
      "tensor(3.8944, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.894402265548706\n",
      "tensor(4.1086, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 4.10861349105835\n",
      "tensor(3.4568, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.4568464756011963\n",
      "tensor(3.2746, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.2745614051818848\n",
      "tensor(3.7271, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.727149486541748\n",
      "tensor(3.8242, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.8242135047912598\n",
      "tensor(4.3129, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 4.3128767013549805\n",
      "tensor(3.8702, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.8701515197753906\n",
      "tensor(4.0012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 4.0012102127075195\n",
      "tensor(3.4411, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.4410500526428223\n",
      "tensor(3.5714, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.571397304534912\n",
      "tensor(3.4721, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 995, loss 3.472121238708496\n",
      "tensor(3.2139, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.2138783931732178\n",
      "tensor(4.0441, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.044111251831055\n",
      "tensor(3.8397, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.8396663665771484\n",
      "tensor(3.0209, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.0208778381347656\n",
      "tensor(3.1231, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.123142719268799\n",
      "tensor(4.2164, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.216424942016602\n",
      "tensor(3.8956, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.895609140396118\n",
      "tensor(3.2083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.20831036567688\n",
      "tensor(3.1776, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.1775565147399902\n",
      "tensor(4.3114, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.311395168304443\n",
      "tensor(3.9062, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.906219482421875\n",
      "tensor(4.4764, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.4763898849487305\n",
      "tensor(3.4633, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.463343858718872\n",
      "tensor(3.7887, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.7886946201324463\n",
      "tensor(4.0117, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.011735916137695\n",
      "tensor(3.4266, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.426586866378784\n",
      "tensor(3.8869, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.8869380950927734\n",
      "tensor(3.5648, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.564751625061035\n",
      "tensor(4.0224, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.022390365600586\n",
      "tensor(3.8673, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.867340564727783\n",
      "tensor(3.3839, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.383942127227783\n",
      "tensor(3.5990, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.598959445953369\n",
      "tensor(3.7057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.7057199478149414\n",
      "tensor(3.5093, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.5092639923095703\n",
      "tensor(3.5379, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.537930965423584\n",
      "tensor(4.0489, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.048923492431641\n",
      "tensor(3.7908, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.7907791137695312\n",
      "tensor(3.5751, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.575087070465088\n",
      "tensor(3.8724, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.8723645210266113\n",
      "tensor(3.8089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.8089096546173096\n",
      "tensor(3.1291, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.129126787185669\n",
      "tensor(4.0408, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 4.040809631347656\n",
      "tensor(3.5268, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.5267810821533203\n",
      "tensor(3.8785, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 3.8784966468811035\n",
      "tensor(2.4936, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 996, loss 2.493612051010132\n",
      "tensor(3.7156, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.715625286102295\n",
      "tensor(3.4021, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.402106761932373\n",
      "tensor(3.5363, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.536295175552368\n",
      "tensor(4.0679, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.067896842956543\n",
      "tensor(3.7717, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.77170467376709\n",
      "tensor(3.5693, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.5692853927612305\n",
      "tensor(4.3279, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.327949047088623\n",
      "tensor(3.4689, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.4688923358917236\n",
      "tensor(3.9839, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.98392915725708\n",
      "tensor(4.2575, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.257547855377197\n",
      "tensor(3.7141, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.714141607284546\n",
      "tensor(4.0861, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.086118698120117\n",
      "tensor(3.0762, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.076150894165039\n",
      "tensor(3.7717, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.771650791168213\n",
      "tensor(3.9725, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.9725160598754883\n",
      "tensor(3.2881, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.2881314754486084\n",
      "tensor(3.8794, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.879448652267456\n",
      "tensor(4.3461, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.346075534820557\n",
      "tensor(3.7538, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.7537546157836914\n",
      "tensor(3.1841, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.184113025665283\n",
      "tensor(3.1129, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.1129298210144043\n",
      "tensor(3.2805, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.2804758548736572\n",
      "tensor(3.8995, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.8994643688201904\n",
      "tensor(3.1396, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.1396396160125732\n",
      "tensor(3.7645, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.7644925117492676\n",
      "tensor(4.0222, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.022218704223633\n",
      "tensor(3.6634, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.663374900817871\n",
      "tensor(3.2805, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.2804958820343018\n",
      "tensor(4.1890, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.18903923034668\n",
      "tensor(3.4144, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.4143807888031006\n",
      "tensor(4.1258, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 4.125843048095703\n",
      "tensor(3.6243, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.6243057250976562\n",
      "tensor(3.3331, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.3330769538879395\n",
      "tensor(3.8191, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.8191213607788086\n",
      "tensor(3.4563, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 997, loss 3.456289768218994\n",
      "tensor(3.7713, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.77128529548645\n",
      "tensor(3.5376, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.537597179412842\n",
      "tensor(3.9488, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.9487993717193604\n",
      "tensor(3.7868, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.78684663772583\n",
      "tensor(3.0831, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.0831265449523926\n",
      "tensor(4.2366, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 4.2366461753845215\n",
      "tensor(3.7338, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.733841896057129\n",
      "tensor(3.3010, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.300987958908081\n",
      "tensor(3.2832, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.283154010772705\n",
      "tensor(3.8205, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.8204963207244873\n",
      "tensor(3.9182, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.9182474613189697\n",
      "tensor(3.5293, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.529279947280884\n",
      "tensor(4.2325, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 4.232526779174805\n",
      "tensor(3.7908, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.7907791137695312\n",
      "tensor(3.5880, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.5880472660064697\n",
      "tensor(3.2386, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.238558769226074\n",
      "tensor(3.9849, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.9848601818084717\n",
      "tensor(3.6639, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.66385555267334\n",
      "tensor(3.4946, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.494619607925415\n",
      "tensor(4.0761, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 4.076062202453613\n",
      "tensor(3.9064, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.906440496444702\n",
      "tensor(3.4261, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.4261059761047363\n",
      "tensor(3.8315, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.8314850330352783\n",
      "tensor(3.7449, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.7448930740356445\n",
      "tensor(3.3570, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.357034683227539\n",
      "tensor(3.6245, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.6245179176330566\n",
      "tensor(3.6833, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.683323860168457\n",
      "tensor(3.8382, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.8381524085998535\n",
      "tensor(3.3683, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.3682801723480225\n",
      "tensor(4.0058, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 4.005757808685303\n",
      "tensor(3.4792, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.4791932106018066\n",
      "tensor(3.4360, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.436032772064209\n",
      "tensor(3.2343, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 3.2343180179595947\n",
      "tensor(4.3473, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 4.347290992736816\n",
      "tensor(5.8200, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 998, loss 5.820032596588135\n",
      "tensor(3.6964, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.696408271789551\n",
      "tensor(3.4112, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.411238431930542\n",
      "tensor(3.6473, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.6472830772399902\n",
      "tensor(3.6482, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.6482326984405518\n",
      "tensor(3.5552, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.555185317993164\n",
      "tensor(4.1083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 4.108325958251953\n",
      "tensor(3.0903, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.090291738510132\n",
      "tensor(3.6493, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.649282455444336\n",
      "tensor(3.8206, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.820607900619507\n",
      "tensor(3.4201, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.4201455116271973\n",
      "tensor(3.5730, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.5729598999023438\n",
      "tensor(3.5068, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.5068137645721436\n",
      "tensor(3.8088, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.8087918758392334\n",
      "tensor(4.0453, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 4.045283794403076\n",
      "tensor(3.8679, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.867933988571167\n",
      "tensor(4.3897, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 4.389675140380859\n",
      "tensor(3.2140, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.213979721069336\n",
      "tensor(3.5590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.5589613914489746\n",
      "tensor(3.8524, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.852400541305542\n",
      "tensor(3.6991, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.699061632156372\n",
      "tensor(4.5523, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 4.552290916442871\n",
      "tensor(3.4760, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.4760026931762695\n",
      "tensor(3.7063, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.706329822540283\n",
      "tensor(4.3619, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 4.361855506896973\n",
      "tensor(3.0889, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.088921070098877\n",
      "tensor(3.8066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.8065783977508545\n",
      "tensor(3.7300, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.7299787998199463\n",
      "tensor(3.8857, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.885746717453003\n",
      "tensor(4.1512, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 4.151224613189697\n",
      "tensor(3.4629, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.4629411697387695\n",
      "tensor(3.3550, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.3550281524658203\n",
      "tensor(3.8254, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.8253560066223145\n",
      "tensor(3.9875, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.987492561340332\n",
      "tensor(3.3310, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 3.3309545516967773\n",
      "tensor(2.2006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 999, loss 2.200599431991577\n",
      "tensor([[3.2517],\n",
      "        [3.2517],\n",
      "        [3.2517],\n",
      "        ...,\n",
      "        [3.2517],\n",
      "        [3.2517],\n",
      "        [3.2517]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputDim=len(inputs)\n",
    "outputDim=1 \n",
    "learningRate=0.1\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "#Training Loop\n",
    "epochs=1000\n",
    "\n",
    "epoch_array=np.zeros(epochs)\n",
    "loss_array=np.zeros(epochs)\n",
    "for epoch in range(epochs): #Forward Pass and loss\n",
    "    for xb,yb in train_dl:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = model(xb)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, yb)\n",
    "        print(loss)\n",
    "        # get gradients w.r.t to parameters, (backward pass)\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_array[epoch]=epoch \n",
    "        loss_array[epoch]=loss.item()\n",
    "\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    predicted = model(inputs_tensor)\n",
    "    print(predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-13860fca5af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib qt\n",
    "fig=plt.figure()\n",
    "ax= plt.axes(projection='3d')\n",
    "ax.plot3D(inputs_tensor.cpu().detach().numpy()[:,0].flatten(),inputs_tensor.cpu().detach().numpy()[:,1].flatten(),predicted.cpu().detach().numpy().flatten(),'red')\n",
    "ax.scatter3D(inputs_tensor.cpu().detach().numpy()[:,0],inputs_tensor.cpu().detach().numpy()[:,1],n.cpu().detach().numpy(),alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),n.cpu().detach().numpy(),alpha=0.2)\n",
    "# plt.scatter(inputs_tensor.cpu().detach().numpy(),predicted.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'GeForce RTX 2070'"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": []
  }
 ]
}