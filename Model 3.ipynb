{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 3: Predict Star Formation Variables (sSFR, SFR, M*, age) Based on Visual Morphology (galaxy image)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[0;34m[INFO]: \u001b[0mNo release version set. Setting default to DR15\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import decode_image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import marvin\n",
    "from marvin.tools.maps import Maps\n",
    "from marvin.tools.image import Image\n",
    "from marvin.utils.general.images import get_images_by_list\n",
    "from marvin import config\n",
    "from marvin.tools.cube import Cube\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image as image_PIL\n",
    "\n",
    "#set config attributes and turn on global downloads of Marvin data\n",
    "config.setRelease('DR15')\n",
    "config.mode = 'local'\n",
    "config.download = True\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.linear1 = torch.nn.Linear(outputSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "\n",
    "#Problem with image associated with manga id at galaxy_list[3548], mangaid- 1-135668\n",
    "galaxy_list=np.delete(galaxy_list,3548)\n",
    "\n",
    "galaxy_list=np.unique(galaxy_list)\n",
    "\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "galaxies=galaxies.sort_values(by=['plateifu']) #Sorting galaxies by plateifu to match ImageFolder Output \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n"
   ]
  },
  {
   "source": [
    "# Importing Images from their Downloaded Locations \n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_locations=[]\n",
    "# for i in range (len(galaxy_list)):\n",
    "#     image_locations.append(Image(galaxy_list[i]).filename)\n",
    "    \n",
    "# image_locations=np.array(image_locations,dtype=str)\n",
    "# np.savetxt('Image Directories',image_locations,fmt='%s')\n",
    "\n",
    "image_locations=np.loadtxt('Image Directories',dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n",
    "\n",
    "\n",
    "\n",
    "# training_folder_name = '/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/'\n",
    "\n",
    "# # New location for the resized images\n",
    "# train_folder = '/home/juanp/Documents/Resized MaNGA Pictures'\n",
    "\n",
    "\n",
    "# # Create resized copies of all of the source images\n",
    "# size = (128,128)\n",
    "\n",
    "# # Create the output folder if it doesn't already exist\n",
    "# if os.path.exists(train_folder):\n",
    "#     shutil.rmtree(train_folder)\n",
    "\n",
    "# # Loop through each subfolder in the input folder\n",
    "# print('Transforming images...')\n",
    "# for root, folders, files in os.walk(training_folder_name):\n",
    "#     for sub_folder in folders:\n",
    "#         print('processing folder ' + sub_folder)\n",
    "#         # Create a matching subfolder in the output dir\n",
    "#         saveFolder = os.path.join(train_folder,sub_folder)\n",
    "#         if not os.path.exists(saveFolder):\n",
    "#             os.makedirs(saveFolder)\n",
    "#         # Loop through the files in the subfolder\n",
    "#         file_names = os.listdir(os.path.join(root,sub_folder))\n",
    "#         for file_name in file_names:\n",
    "#             # Open the file\n",
    "#             file_path = os.path.join(root,sub_folder, file_name)\n",
    "#             #print(\"reading \" + file_path)\n",
    "#             try:\n",
    "#                 image = image_PIL.open(file_path)\n",
    "#                  # Create a resized version and save it\n",
    "#                 resized_image = resize_image(image, size)\n",
    "#                 saveAs = os.path.join(saveFolder, file_name)\n",
    "#                 #print(\"writing \" + saveAs)\n",
    "#                 resized_image.save(saveAs)\n",
    "#             except:\n",
    "#                 print(file_path)\n",
    "           \n",
    "\n",
    "# print('Done.')"
   ]
  },
  {
   "source": [
    "# Putting the Images into DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data loaders ready to read /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n"
     ]
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "\n",
    "# image=ImageFolder('/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/') #Picks up 3590 pictures, directory list has lenght 3637 however \n",
    "\n",
    "image_directory='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3'\n",
    "classes= sorted(os.listdir(image_directory))\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        # transform=transformation\n",
    "    )\n",
    "    \n",
    "    #This loop transforms the images and assigns them the correct label \n",
    "\n",
    "    full_dataset_v2 = []   \n",
    "    for i in range(len(full_dataset)): \n",
    "        temp=transformation(resize_image(full_dataset[i][0]))\n",
    "        full_dataset_v2.append((temp,log_SFR.iloc[i])) \n",
    "    \n",
    "    full_dataset=full_dataset_v2\n",
    "\n",
    "\n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=10,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=10,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(image_directory)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset ImageFolder\n    Number of datapoints: 3589\n    Root location: /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n3637\n3589\n3589\n"
     ]
    }
   ],
   "source": [
    "print(ImageFolder(image_directory))\n",
    "print(len(image_locations))\n",
    "print(len(np.unique(image_locations)))\n",
    "print(len(np.unique(galaxy_list)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_dataset = torchvision.datasets.ImageFolder(\n",
    "#         root='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/7968',\n",
    "#         # transform=transformation\n",
    "#     ) \n",
    "\n",
    "# full_dataset_v2 = []   \n",
    "# for i in range(len(full_dataset)): \n",
    "#     full_dataset_v2.append((resize_image(full_dataset[i][0]) , i)) \n",
    "\n",
    "# print(full_dataset_v2)\n",
    "\n",
    "\n",
    "# print(full_dataset[0][1])\n",
    "# for i in range(len(full_dataset)): \n",
    "#         full_dataset[i] = (resize_image(full_dataset[i][0]) , 0) \n",
    "        # full_dataset[i][0] = resize_image(full_dataset[i][0])\n",
    "        # full_dataset[i][1] = correct_label(correct_manga_id) "
   ]
  },
  {
   "source": [
    "# Defining the Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (drop): Dropout2d(p=0.2, inplace=False)\n  (fc): Linear(in_features=24576, out_features=10, bias=True)\n  (fc1): Linear(in_features=10, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 128 x 128, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=10)\n",
    "\n",
    "        self.fc1= nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function\n",
    "        x=F.relu(x) \n",
    "        x= self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(SFR)).to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "# Creating Training Loop/Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "    \n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target.float())\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Create Test Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "batch 1960 Loss: 0.437837\n",
      "\tTraining batch 1961 Loss: 0.010212\n",
      "\tTraining batch 1962 Loss: 0.013032\n",
      "\tTraining batch 1963 Loss: 3.140362\n",
      "\tTraining batch 1964 Loss: 0.317411\n",
      "\tTraining batch 1965 Loss: 0.236204\n",
      "\tTraining batch 1966 Loss: 0.260394\n",
      "\tTraining batch 1967 Loss: 1.148960\n",
      "\tTraining batch 1968 Loss: 0.186821\n",
      "\tTraining batch 1969 Loss: 4.305521\n",
      "\tTraining batch 1970 Loss: 3.394409\n",
      "\tTraining batch 1971 Loss: 0.050964\n",
      "\tTraining batch 1972 Loss: 2.923019\n",
      "\tTraining batch 1973 Loss: 1.461470\n",
      "\tTraining batch 1974 Loss: 0.041677\n",
      "\tTraining batch 1975 Loss: 0.759667\n",
      "\tTraining batch 1976 Loss: 0.514424\n",
      "\tTraining batch 1977 Loss: 0.409678\n",
      "\tTraining batch 1978 Loss: 0.088270\n",
      "\tTraining batch 1979 Loss: 0.000788\n",
      "\tTraining batch 1980 Loss: 0.000260\n",
      "\tTraining batch 1981 Loss: 0.610880\n",
      "\tTraining batch 1982 Loss: 0.020888\n",
      "\tTraining batch 1983 Loss: 0.032918\n",
      "\tTraining batch 1984 Loss: 0.008055\n",
      "\tTraining batch 1985 Loss: 0.162163\n",
      "\tTraining batch 1986 Loss: 3.176881\n",
      "\tTraining batch 1987 Loss: 0.002966\n",
      "\tTraining batch 1988 Loss: 1.293800\n",
      "\tTraining batch 1989 Loss: 2.064396\n",
      "\tTraining batch 1990 Loss: 1.526215\n",
      "\tTraining batch 1991 Loss: 1.471056\n",
      "\tTraining batch 1992 Loss: 0.384725\n",
      "\tTraining batch 1993 Loss: 1.878041\n",
      "\tTraining batch 1994 Loss: 0.031289\n",
      "\tTraining batch 1995 Loss: 0.532331\n",
      "\tTraining batch 1996 Loss: 4.424453\n",
      "\tTraining batch 1997 Loss: 0.119622\n",
      "\tTraining batch 1998 Loss: 0.415674\n",
      "\tTraining batch 1999 Loss: 0.017199\n",
      "\tTraining batch 2000 Loss: 0.620405\n",
      "\tTraining batch 2001 Loss: 0.000000\n",
      "\tTraining batch 2002 Loss: 0.944262\n",
      "\tTraining batch 2003 Loss: 0.002114\n",
      "\tTraining batch 2004 Loss: 0.094092\n",
      "\tTraining batch 2005 Loss: 0.979597\n",
      "\tTraining batch 2006 Loss: 0.509773\n",
      "\tTraining batch 2007 Loss: 0.020409\n",
      "\tTraining batch 2008 Loss: 0.092414\n",
      "\tTraining batch 2009 Loss: 0.288689\n",
      "\tTraining batch 2010 Loss: 0.212910\n",
      "\tTraining batch 2011 Loss: 0.785956\n",
      "\tTraining batch 2012 Loss: 0.177372\n",
      "\tTraining batch 2013 Loss: 0.104574\n",
      "\tTraining batch 2014 Loss: 0.880924\n",
      "\tTraining batch 2015 Loss: 0.066114\n",
      "\tTraining batch 2016 Loss: 0.250146\n",
      "\tTraining batch 2017 Loss: 0.009389\n",
      "\tTraining batch 2018 Loss: 0.008330\n",
      "\tTraining batch 2019 Loss: 2.932632\n",
      "\tTraining batch 2020 Loss: 0.780726\n",
      "\tTraining batch 2021 Loss: 2.320759\n",
      "\tTraining batch 2022 Loss: 0.033514\n",
      "\tTraining batch 2023 Loss: 0.834489\n",
      "\tTraining batch 2024 Loss: 0.886796\n",
      "\tTraining batch 2025 Loss: 0.574017\n",
      "\tTraining batch 2026 Loss: 0.043167\n",
      "\tTraining batch 2027 Loss: 3.998805\n",
      "\tTraining batch 2028 Loss: 0.267850\n",
      "\tTraining batch 2029 Loss: 0.654272\n",
      "\tTraining batch 2030 Loss: 0.025261\n",
      "\tTraining batch 2031 Loss: 0.610416\n",
      "\tTraining batch 2032 Loss: 2.444214\n",
      "\tTraining batch 2033 Loss: 0.472004\n",
      "\tTraining batch 2034 Loss: 1.445239\n",
      "\tTraining batch 2035 Loss: 0.035492\n",
      "\tTraining batch 2036 Loss: 0.192274\n",
      "\tTraining batch 2037 Loss: 1.337554\n",
      "\tTraining batch 2038 Loss: 1.989794\n",
      "\tTraining batch 2039 Loss: 0.084751\n",
      "\tTraining batch 2040 Loss: 0.587438\n",
      "\tTraining batch 2041 Loss: 0.301070\n",
      "\tTraining batch 2042 Loss: 1.330659\n",
      "\tTraining batch 2043 Loss: 0.417561\n",
      "\tTraining batch 2044 Loss: 1.032610\n",
      "\tTraining batch 2045 Loss: 0.006644\n",
      "\tTraining batch 2046 Loss: 0.370768\n",
      "\tTraining batch 2047 Loss: 2.773797\n",
      "\tTraining batch 2048 Loss: 0.870887\n",
      "\tTraining batch 2049 Loss: 0.980013\n",
      "\tTraining batch 2050 Loss: 2.847459\n",
      "\tTraining batch 2051 Loss: 1.828620\n",
      "\tTraining batch 2052 Loss: 0.244390\n",
      "\tTraining batch 2053 Loss: 1.907226\n",
      "\tTraining batch 2054 Loss: 0.536951\n",
      "\tTraining batch 2055 Loss: 0.124261\n",
      "\tTraining batch 2056 Loss: 0.174274\n",
      "\tTraining batch 2057 Loss: 1.697915\n",
      "\tTraining batch 2058 Loss: 0.236246\n",
      "\tTraining batch 2059 Loss: 0.481623\n",
      "\tTraining batch 2060 Loss: 0.898832\n",
      "\tTraining batch 2061 Loss: 1.086074\n",
      "\tTraining batch 2062 Loss: 12.764264\n",
      "\tTraining batch 2063 Loss: 0.212290\n",
      "\tTraining batch 2064 Loss: 0.114419\n",
      "\tTraining batch 2065 Loss: 0.020936\n",
      "\tTraining batch 2066 Loss: 0.118352\n",
      "\tTraining batch 2067 Loss: 1.819178\n",
      "\tTraining batch 2068 Loss: 2.265598\n",
      "\tTraining batch 2069 Loss: 0.472413\n",
      "\tTraining batch 2070 Loss: 0.781029\n",
      "\tTraining batch 2071 Loss: 0.022441\n",
      "\tTraining batch 2072 Loss: 0.417443\n",
      "\tTraining batch 2073 Loss: 0.754947\n",
      "\tTraining batch 2074 Loss: 5.540483\n",
      "\tTraining batch 2075 Loss: 0.743746\n",
      "\tTraining batch 2076 Loss: 0.003814\n",
      "\tTraining batch 2077 Loss: 0.145914\n",
      "\tTraining batch 2078 Loss: 3.320969\n",
      "\tTraining batch 2079 Loss: 0.797887\n",
      "\tTraining batch 2080 Loss: 0.031859\n",
      "\tTraining batch 2081 Loss: 0.291285\n",
      "\tTraining batch 2082 Loss: 0.035897\n",
      "\tTraining batch 2083 Loss: 0.008928\n",
      "\tTraining batch 2084 Loss: 0.029363\n",
      "\tTraining batch 2085 Loss: 0.034175\n",
      "\tTraining batch 2086 Loss: 0.164542\n",
      "\tTraining batch 2087 Loss: 1.210204\n",
      "\tTraining batch 2088 Loss: 0.440164\n",
      "\tTraining batch 2089 Loss: 0.005200\n",
      "\tTraining batch 2090 Loss: 0.307731\n",
      "\tTraining batch 2091 Loss: 0.241622\n",
      "\tTraining batch 2092 Loss: 1.724687\n",
      "\tTraining batch 2093 Loss: 0.156339\n",
      "\tTraining batch 2094 Loss: 0.101753\n",
      "\tTraining batch 2095 Loss: 1.070420\n",
      "\tTraining batch 2096 Loss: 2.427500\n",
      "\tTraining batch 2097 Loss: 1.040946\n",
      "\tTraining batch 2098 Loss: 0.008157\n",
      "\tTraining batch 2099 Loss: 0.009315\n",
      "\tTraining batch 2100 Loss: 1.573938\n",
      "\tTraining batch 2101 Loss: 0.857053\n",
      "\tTraining batch 2102 Loss: 0.164051\n",
      "\tTraining batch 2103 Loss: 0.003711\n",
      "\tTraining batch 2104 Loss: 0.785994\n",
      "\tTraining batch 2105 Loss: 0.000117\n",
      "\tTraining batch 2106 Loss: 0.983172\n",
      "\tTraining batch 2107 Loss: 1.859820\n",
      "\tTraining batch 2108 Loss: 0.267166\n",
      "\tTraining batch 2109 Loss: 1.006432\n",
      "\tTraining batch 2110 Loss: 0.086590\n",
      "\tTraining batch 2111 Loss: 0.403584\n",
      "\tTraining batch 2112 Loss: 1.319594\n",
      "\tTraining batch 2113 Loss: 0.390454\n",
      "\tTraining batch 2114 Loss: 2.120064\n",
      "\tTraining batch 2115 Loss: 0.591590\n",
      "\tTraining batch 2116 Loss: 2.472391\n",
      "\tTraining batch 2117 Loss: 0.081418\n",
      "\tTraining batch 2118 Loss: 0.021277\n",
      "\tTraining batch 2119 Loss: 3.079348\n",
      "\tTraining batch 2120 Loss: 1.499513\n",
      "\tTraining batch 2121 Loss: 1.495885\n",
      "\tTraining batch 2122 Loss: 0.002299\n",
      "\tTraining batch 2123 Loss: 0.433559\n",
      "\tTraining batch 2124 Loss: 0.071303\n",
      "\tTraining batch 2125 Loss: 0.390231\n",
      "\tTraining batch 2126 Loss: 0.209320\n",
      "\tTraining batch 2127 Loss: 1.406050\n",
      "\tTraining batch 2128 Loss: 3.406063\n",
      "\tTraining batch 2129 Loss: 0.572413\n",
      "\tTraining batch 2130 Loss: 0.600701\n",
      "\tTraining batch 2131 Loss: 0.288642\n",
      "\tTraining batch 2132 Loss: 0.486368\n",
      "\tTraining batch 2133 Loss: 0.078347\n",
      "\tTraining batch 2134 Loss: 1.099558\n",
      "\tTraining batch 2135 Loss: 0.918580\n",
      "\tTraining batch 2136 Loss: 2.209906\n",
      "\tTraining batch 2137 Loss: 0.094158\n",
      "\tTraining batch 2138 Loss: 0.122113\n",
      "\tTraining batch 2139 Loss: 0.810040\n",
      "\tTraining batch 2140 Loss: 0.208833\n",
      "\tTraining batch 2141 Loss: 8.080981\n",
      "\tTraining batch 2142 Loss: 1.097952\n",
      "\tTraining batch 2143 Loss: 0.317021\n",
      "\tTraining batch 2144 Loss: 0.061505\n",
      "\tTraining batch 2145 Loss: 0.000506\n",
      "\tTraining batch 2146 Loss: 0.007072\n",
      "\tTraining batch 2147 Loss: 0.218118\n",
      "\tTraining batch 2148 Loss: 1.273325\n",
      "\tTraining batch 2149 Loss: 1.121770\n",
      "\tTraining batch 2150 Loss: 0.069120\n",
      "\tTraining batch 2151 Loss: 0.556040\n",
      "\tTraining batch 2152 Loss: 0.007621\n",
      "\tTraining batch 2153 Loss: 0.893878\n",
      "\tTraining batch 2154 Loss: 0.025508\n",
      "\tTraining batch 2155 Loss: 0.565589\n",
      "\tTraining batch 2156 Loss: 2.812714\n",
      "\tTraining batch 2157 Loss: 0.317919\n",
      "\tTraining batch 2158 Loss: 1.662911\n",
      "\tTraining batch 2159 Loss: 0.028796\n",
      "\tTraining batch 2160 Loss: 0.526331\n",
      "\tTraining batch 2161 Loss: 0.388264\n",
      "\tTraining batch 2162 Loss: 0.947931\n",
      "\tTraining batch 2163 Loss: 0.516082\n",
      "\tTraining batch 2164 Loss: 0.366188\n",
      "\tTraining batch 2165 Loss: 0.676293\n",
      "\tTraining batch 2166 Loss: 0.476843\n",
      "\tTraining batch 2167 Loss: 0.951225\n",
      "\tTraining batch 2168 Loss: 0.762594\n",
      "\tTraining batch 2169 Loss: 2.217702\n",
      "\tTraining batch 2170 Loss: 1.313981\n",
      "\tTraining batch 2171 Loss: 0.031052\n",
      "\tTraining batch 2172 Loss: 0.071733\n",
      "\tTraining batch 2173 Loss: 0.126497\n",
      "\tTraining batch 2174 Loss: 0.106338\n",
      "\tTraining batch 2175 Loss: 0.081312\n",
      "\tTraining batch 2176 Loss: 0.045469\n",
      "\tTraining batch 2177 Loss: 0.834923\n",
      "\tTraining batch 2178 Loss: 0.228240\n",
      "\tTraining batch 2179 Loss: 0.365500\n",
      "\tTraining batch 2180 Loss: 0.869952\n",
      "\tTraining batch 2181 Loss: 0.028867\n",
      "\tTraining batch 2182 Loss: 0.851099\n",
      "\tTraining batch 2183 Loss: 2.132398\n",
      "\tTraining batch 2184 Loss: 0.404671\n",
      "\tTraining batch 2185 Loss: 0.012359\n",
      "\tTraining batch 2186 Loss: 1.295127\n",
      "\tTraining batch 2187 Loss: 0.132239\n",
      "\tTraining batch 2188 Loss: 1.309554\n",
      "\tTraining batch 2189 Loss: 0.001982\n",
      "\tTraining batch 2190 Loss: 1.803675\n",
      "\tTraining batch 2191 Loss: 0.472242\n",
      "\tTraining batch 2192 Loss: 0.722466\n",
      "\tTraining batch 2193 Loss: 0.793929\n",
      "\tTraining batch 2194 Loss: 0.543743\n",
      "\tTraining batch 2195 Loss: 0.316204\n",
      "\tTraining batch 2196 Loss: 1.966955\n",
      "\tTraining batch 2197 Loss: 0.145693\n",
      "\tTraining batch 2198 Loss: 0.600394\n",
      "\tTraining batch 2199 Loss: 4.944246\n",
      "\tTraining batch 2200 Loss: 0.087678\n",
      "\tTraining batch 2201 Loss: 2.360772\n",
      "\tTraining batch 2202 Loss: 3.334447\n",
      "\tTraining batch 2203 Loss: 3.244522\n",
      "\tTraining batch 2204 Loss: 0.154632\n",
      "\tTraining batch 2205 Loss: 1.119146\n",
      "\tTraining batch 2206 Loss: 0.067045\n",
      "\tTraining batch 2207 Loss: 0.289181\n",
      "\tTraining batch 2208 Loss: 0.737874\n",
      "\tTraining batch 2209 Loss: 0.140145\n",
      "\tTraining batch 2210 Loss: 0.596112\n",
      "\tTraining batch 2211 Loss: 2.371492\n",
      "\tTraining batch 2212 Loss: 0.469707\n",
      "\tTraining batch 2213 Loss: 0.021949\n",
      "\tTraining batch 2214 Loss: 0.567121\n",
      "\tTraining batch 2215 Loss: 0.806926\n",
      "\tTraining batch 2216 Loss: 0.000166\n",
      "\tTraining batch 2217 Loss: 0.943433\n",
      "\tTraining batch 2218 Loss: 0.107180\n",
      "\tTraining batch 2219 Loss: 0.246904\n",
      "\tTraining batch 2220 Loss: 0.166453\n",
      "\tTraining batch 2221 Loss: 0.890978\n",
      "\tTraining batch 2222 Loss: 0.724823\n",
      "\tTraining batch 2223 Loss: 0.988296\n",
      "\tTraining batch 2224 Loss: 0.129042\n",
      "\tTraining batch 2225 Loss: 0.359434\n",
      "\tTraining batch 2226 Loss: 1.520913\n",
      "\tTraining batch 2227 Loss: 0.013855\n",
      "\tTraining batch 2228 Loss: 0.489604\n",
      "\tTraining batch 2229 Loss: 0.938881\n",
      "\tTraining batch 2230 Loss: 0.835767\n",
      "\tTraining batch 2231 Loss: 1.270433\n",
      "\tTraining batch 2232 Loss: 0.005677\n",
      "\tTraining batch 2233 Loss: 0.058181\n",
      "\tTraining batch 2234 Loss: 0.407868\n",
      "\tTraining batch 2235 Loss: 0.178982\n",
      "\tTraining batch 2236 Loss: 0.334714\n",
      "\tTraining batch 2237 Loss: 0.244740\n",
      "\tTraining batch 2238 Loss: 0.089649\n",
      "\tTraining batch 2239 Loss: 0.201232\n",
      "\tTraining batch 2240 Loss: 0.019982\n",
      "\tTraining batch 2241 Loss: 0.013010\n",
      "\tTraining batch 2242 Loss: 1.472816\n",
      "\tTraining batch 2243 Loss: 1.316502\n",
      "\tTraining batch 2244 Loss: 1.435213\n",
      "\tTraining batch 2245 Loss: 4.548279\n",
      "\tTraining batch 2246 Loss: 1.557479\n",
      "\tTraining batch 2247 Loss: 0.052558\n",
      "\tTraining batch 2248 Loss: 0.004081\n",
      "\tTraining batch 2249 Loss: 0.085062\n",
      "\tTraining batch 2250 Loss: 0.183816\n",
      "\tTraining batch 2251 Loss: 0.324814\n",
      "\tTraining batch 2252 Loss: 0.485753\n",
      "\tTraining batch 2253 Loss: 0.256094\n",
      "\tTraining batch 2254 Loss: 0.339191\n",
      "\tTraining batch 2255 Loss: 2.575112\n",
      "\tTraining batch 2256 Loss: 0.003621\n",
      "\tTraining batch 2257 Loss: 1.006597\n",
      "\tTraining batch 2258 Loss: 0.229627\n",
      "\tTraining batch 2259 Loss: 0.811352\n",
      "\tTraining batch 2260 Loss: 0.430405\n",
      "\tTraining batch 2261 Loss: 1.245812\n",
      "\tTraining batch 2262 Loss: 0.034528\n",
      "\tTraining batch 2263 Loss: 0.108082\n",
      "\tTraining batch 2264 Loss: 0.448625\n",
      "\tTraining batch 2265 Loss: 1.385288\n",
      "\tTraining batch 2266 Loss: 0.041366\n",
      "\tTraining batch 2267 Loss: 0.071922\n",
      "\tTraining batch 2268 Loss: 0.527982\n",
      "\tTraining batch 2269 Loss: 0.151469\n",
      "\tTraining batch 2270 Loss: 0.639984\n",
      "\tTraining batch 2271 Loss: 0.959333\n",
      "\tTraining batch 2272 Loss: 0.021857\n",
      "\tTraining batch 2273 Loss: 0.001136\n",
      "\tTraining batch 2274 Loss: 0.000020\n",
      "\tTraining batch 2275 Loss: 2.517319\n",
      "\tTraining batch 2276 Loss: 4.464129\n",
      "\tTraining batch 2277 Loss: 0.807015\n",
      "\tTraining batch 2278 Loss: 0.410302\n",
      "\tTraining batch 2279 Loss: 5.092689\n",
      "\tTraining batch 2280 Loss: 0.430619\n",
      "\tTraining batch 2281 Loss: 0.398886\n",
      "\tTraining batch 2282 Loss: 0.013194\n",
      "\tTraining batch 2283 Loss: 0.197454\n",
      "\tTraining batch 2284 Loss: 1.216659\n",
      "\tTraining batch 2285 Loss: 0.122103\n",
      "\tTraining batch 2286 Loss: 0.190941\n",
      "\tTraining batch 2287 Loss: 0.522109\n",
      "\tTraining batch 2288 Loss: 1.245999\n",
      "\tTraining batch 2289 Loss: 0.011764\n",
      "\tTraining batch 2290 Loss: 0.266807\n",
      "\tTraining batch 2291 Loss: 1.132661\n",
      "\tTraining batch 2292 Loss: 0.299310\n",
      "\tTraining batch 2293 Loss: 0.666738\n",
      "\tTraining batch 2294 Loss: 0.071492\n",
      "\tTraining batch 2295 Loss: 0.061494\n",
      "\tTraining batch 2296 Loss: 0.000822\n",
      "\tTraining batch 2297 Loss: 0.380719\n",
      "\tTraining batch 2298 Loss: 0.881032\n",
      "\tTraining batch 2299 Loss: 0.002725\n",
      "\tTraining batch 2300 Loss: 0.740943\n",
      "\tTraining batch 2301 Loss: 0.205846\n",
      "\tTraining batch 2302 Loss: 0.797554\n",
      "\tTraining batch 2303 Loss: 0.630492\n",
      "\tTraining batch 2304 Loss: 1.932819\n",
      "\tTraining batch 2305 Loss: 0.118834\n",
      "\tTraining batch 2306 Loss: 0.129234\n",
      "\tTraining batch 2307 Loss: 2.534914\n",
      "\tTraining batch 2308 Loss: 0.934055\n",
      "\tTraining batch 2309 Loss: 0.225030\n",
      "\tTraining batch 2310 Loss: 0.033854\n",
      "\tTraining batch 2311 Loss: 0.003831\n",
      "\tTraining batch 2312 Loss: 1.575439\n",
      "\tTraining batch 2313 Loss: 0.191556\n",
      "\tTraining batch 2314 Loss: 0.405400\n",
      "\tTraining batch 2315 Loss: 0.000006\n",
      "\tTraining batch 2316 Loss: 0.796349\n",
      "\tTraining batch 2317 Loss: 0.917095\n",
      "\tTraining batch 2318 Loss: 0.132413\n",
      "\tTraining batch 2319 Loss: 5.943694\n",
      "\tTraining batch 2320 Loss: 0.004126\n",
      "\tTraining batch 2321 Loss: 0.074731\n",
      "\tTraining batch 2322 Loss: 0.474735\n",
      "\tTraining batch 2323 Loss: 0.002861\n",
      "\tTraining batch 2324 Loss: 0.284600\n",
      "\tTraining batch 2325 Loss: 1.455776\n",
      "\tTraining batch 2326 Loss: 0.193944\n",
      "\tTraining batch 2327 Loss: 2.883702\n",
      "\tTraining batch 2328 Loss: 0.587810\n",
      "\tTraining batch 2329 Loss: 0.027248\n",
      "\tTraining batch 2330 Loss: 0.134289\n",
      "\tTraining batch 2331 Loss: 0.217157\n",
      "\tTraining batch 2332 Loss: 3.108757\n",
      "\tTraining batch 2333 Loss: 1.116746\n",
      "\tTraining batch 2334 Loss: 0.974795\n",
      "\tTraining batch 2335 Loss: 1.724411\n",
      "\tTraining batch 2336 Loss: 0.122465\n",
      "\tTraining batch 2337 Loss: 1.344593\n",
      "\tTraining batch 2338 Loss: 0.481592\n",
      "\tTraining batch 2339 Loss: 0.252150\n",
      "\tTraining batch 2340 Loss: 0.038017\n",
      "\tTraining batch 2341 Loss: 0.041825\n",
      "\tTraining batch 2342 Loss: 1.975205\n",
      "\tTraining batch 2343 Loss: 0.274839\n",
      "\tTraining batch 2344 Loss: 0.350531\n",
      "\tTraining batch 2345 Loss: 0.047256\n",
      "\tTraining batch 2346 Loss: 1.295893\n",
      "\tTraining batch 2347 Loss: 0.027542\n",
      "\tTraining batch 2348 Loss: 1.063251\n",
      "\tTraining batch 2349 Loss: 0.087466\n",
      "\tTraining batch 2350 Loss: 0.501079\n",
      "\tTraining batch 2351 Loss: 0.140482\n",
      "\tTraining batch 2352 Loss: 0.799007\n",
      "\tTraining batch 2353 Loss: 0.196081\n",
      "\tTraining batch 2354 Loss: 0.089347\n",
      "\tTraining batch 2355 Loss: 0.201523\n",
      "\tTraining batch 2356 Loss: 0.424644\n",
      "\tTraining batch 2357 Loss: 0.478761\n",
      "\tTraining batch 2358 Loss: 0.169027\n",
      "\tTraining batch 2359 Loss: 2.442029\n",
      "\tTraining batch 2360 Loss: 0.156710\n",
      "\tTraining batch 2361 Loss: 0.447239\n",
      "\tTraining batch 2362 Loss: 0.962738\n",
      "\tTraining batch 2363 Loss: 1.408037\n",
      "\tTraining batch 2364 Loss: 0.295900\n",
      "\tTraining batch 2365 Loss: 0.306032\n",
      "\tTraining batch 2366 Loss: 2.083648\n",
      "\tTraining batch 2367 Loss: 0.027361\n",
      "\tTraining batch 2368 Loss: 0.226744\n",
      "\tTraining batch 2369 Loss: 0.043143\n",
      "\tTraining batch 2370 Loss: 0.708612\n",
      "\tTraining batch 2371 Loss: 0.220047\n",
      "\tTraining batch 2372 Loss: 0.119196\n",
      "\tTraining batch 2373 Loss: 2.058880\n",
      "\tTraining batch 2374 Loss: 0.025477\n",
      "\tTraining batch 2375 Loss: 0.021211\n",
      "\tTraining batch 2376 Loss: 0.127706\n",
      "\tTraining batch 2377 Loss: 1.788052\n",
      "\tTraining batch 2378 Loss: 1.265493\n",
      "\tTraining batch 2379 Loss: 0.390011\n",
      "\tTraining batch 2380 Loss: 0.223898\n",
      "\tTraining batch 2381 Loss: 0.033505\n",
      "\tTraining batch 2382 Loss: 0.398632\n",
      "\tTraining batch 2383 Loss: 0.056657\n",
      "\tTraining batch 2384 Loss: 0.792976\n",
      "\tTraining batch 2385 Loss: 0.176094\n",
      "\tTraining batch 2386 Loss: 0.030553\n",
      "\tTraining batch 2387 Loss: 0.043286\n",
      "\tTraining batch 2388 Loss: 2.645540\n",
      "\tTraining batch 2389 Loss: 0.338647\n",
      "\tTraining batch 2390 Loss: 0.274492\n",
      "\tTraining batch 2391 Loss: 1.933652\n",
      "\tTraining batch 2392 Loss: 0.869106\n",
      "\tTraining batch 2393 Loss: 0.046105\n",
      "\tTraining batch 2394 Loss: 0.305684\n",
      "\tTraining batch 2395 Loss: 0.030276\n",
      "\tTraining batch 2396 Loss: 0.092365\n",
      "\tTraining batch 2397 Loss: 0.784166\n",
      "\tTraining batch 2398 Loss: 0.990116\n",
      "\tTraining batch 2399 Loss: 1.478812\n",
      "\tTraining batch 2400 Loss: 0.103575\n",
      "\tTraining batch 2401 Loss: 0.260915\n",
      "\tTraining batch 2402 Loss: 0.442748\n",
      "\tTraining batch 2403 Loss: 0.016085\n",
      "\tTraining batch 2404 Loss: 0.414299\n",
      "\tTraining batch 2405 Loss: 0.032253\n",
      "\tTraining batch 2406 Loss: 2.483922\n",
      "\tTraining batch 2407 Loss: 3.804232\n",
      "\tTraining batch 2408 Loss: 0.530100\n",
      "\tTraining batch 2409 Loss: 7.193934\n",
      "\tTraining batch 2410 Loss: 0.242167\n",
      "\tTraining batch 2411 Loss: 1.625640\n",
      "\tTraining batch 2412 Loss: 0.195860\n",
      "\tTraining batch 2413 Loss: 1.171703\n",
      "\tTraining batch 2414 Loss: 0.819651\n",
      "\tTraining batch 2415 Loss: 0.196813\n",
      "\tTraining batch 2416 Loss: 0.073114\n",
      "\tTraining batch 2417 Loss: 0.007540\n",
      "\tTraining batch 2418 Loss: 0.224358\n",
      "\tTraining batch 2419 Loss: 2.257592\n",
      "\tTraining batch 2420 Loss: 1.437439\n",
      "\tTraining batch 2421 Loss: 0.491415\n",
      "\tTraining batch 2422 Loss: 0.004263\n",
      "\tTraining batch 2423 Loss: 1.289066\n",
      "\tTraining batch 2424 Loss: 4.148919\n",
      "\tTraining batch 2425 Loss: 0.221496\n",
      "\tTraining batch 2426 Loss: 0.524535\n",
      "\tTraining batch 2427 Loss: 0.091007\n",
      "\tTraining batch 2428 Loss: 2.417270\n",
      "\tTraining batch 2429 Loss: 0.202369\n",
      "\tTraining batch 2430 Loss: 0.904834\n",
      "\tTraining batch 2431 Loss: 0.153954\n",
      "\tTraining batch 2432 Loss: 0.000330\n",
      "\tTraining batch 2433 Loss: 1.034770\n",
      "\tTraining batch 2434 Loss: 0.013079\n",
      "\tTraining batch 2435 Loss: 0.024107\n",
      "\tTraining batch 2436 Loss: 0.051202\n",
      "\tTraining batch 2437 Loss: 1.415380\n",
      "\tTraining batch 2438 Loss: 3.388843\n",
      "\tTraining batch 2439 Loss: 0.119885\n",
      "\tTraining batch 2440 Loss: 0.155544\n",
      "\tTraining batch 2441 Loss: 0.070710\n",
      "\tTraining batch 2442 Loss: 0.517535\n",
      "\tTraining batch 2443 Loss: 0.210210\n",
      "\tTraining batch 2444 Loss: 0.006165\n",
      "\tTraining batch 2445 Loss: 0.150130\n",
      "\tTraining batch 2446 Loss: 0.360629\n",
      "\tTraining batch 2447 Loss: 0.299088\n",
      "\tTraining batch 2448 Loss: 3.702297\n",
      "\tTraining batch 2449 Loss: 5.175188\n",
      "\tTraining batch 2450 Loss: 0.039560\n",
      "\tTraining batch 2451 Loss: 0.036764\n",
      "\tTraining batch 2452 Loss: 4.613178\n",
      "\tTraining batch 2453 Loss: 0.207291\n",
      "\tTraining batch 2454 Loss: 1.977742\n",
      "\tTraining batch 2455 Loss: 0.154181\n",
      "\tTraining batch 2456 Loss: 0.682240\n",
      "\tTraining batch 2457 Loss: 2.011683\n",
      "\tTraining batch 2458 Loss: 2.513165\n",
      "\tTraining batch 2459 Loss: 0.611199\n",
      "\tTraining batch 2460 Loss: 7.389826\n",
      "\tTraining batch 2461 Loss: 0.003501\n",
      "\tTraining batch 2462 Loss: 0.186664\n",
      "\tTraining batch 2463 Loss: 0.044640\n",
      "\tTraining batch 2464 Loss: 0.203279\n",
      "\tTraining batch 2465 Loss: 1.657794\n",
      "\tTraining batch 2466 Loss: 0.095792\n",
      "\tTraining batch 2467 Loss: 0.641126\n",
      "\tTraining batch 2468 Loss: 0.429742\n",
      "\tTraining batch 2469 Loss: 0.005372\n",
      "\tTraining batch 2470 Loss: 2.042954\n",
      "\tTraining batch 2471 Loss: 1.177355\n",
      "\tTraining batch 2472 Loss: 0.002115\n",
      "\tTraining batch 2473 Loss: 0.032832\n",
      "\tTraining batch 2474 Loss: 0.207598\n",
      "\tTraining batch 2475 Loss: 0.538564\n",
      "\tTraining batch 2476 Loss: 0.040256\n",
      "\tTraining batch 2477 Loss: 0.510290\n",
      "\tTraining batch 2478 Loss: 0.511335\n",
      "\tTraining batch 2479 Loss: 0.202052\n",
      "\tTraining batch 2480 Loss: 2.124738\n",
      "\tTraining batch 2481 Loss: 0.175421\n",
      "\tTraining batch 2482 Loss: 0.016186\n",
      "\tTraining batch 2483 Loss: 0.186804\n",
      "\tTraining batch 2484 Loss: 0.442991\n",
      "\tTraining batch 2485 Loss: 0.011722\n",
      "\tTraining batch 2486 Loss: 0.209719\n",
      "\tTraining batch 2487 Loss: 0.115594\n",
      "\tTraining batch 2488 Loss: 6.768310\n",
      "\tTraining batch 2489 Loss: 0.410125\n",
      "\tTraining batch 2490 Loss: 0.288222\n",
      "\tTraining batch 2491 Loss: 0.060454\n",
      "\tTraining batch 2492 Loss: 0.195456\n",
      "\tTraining batch 2493 Loss: 0.730568\n",
      "\tTraining batch 2494 Loss: 0.019782\n",
      "\tTraining batch 2495 Loss: 0.028080\n",
      "\tTraining batch 2496 Loss: 0.125011\n",
      "\tTraining batch 2497 Loss: 0.004176\n",
      "\tTraining batch 2498 Loss: 1.778219\n",
      "\tTraining batch 2499 Loss: 0.361409\n",
      "\tTraining batch 2500 Loss: 0.238221\n",
      "\tTraining batch 2501 Loss: 0.598056\n",
      "\tTraining batch 2502 Loss: 0.284695\n",
      "\tTraining batch 2503 Loss: 0.135887\n",
      "\tTraining batch 2504 Loss: 4.989164\n",
      "\tTraining batch 2505 Loss: 0.123583\n",
      "\tTraining batch 2506 Loss: 1.104940\n",
      "\tTraining batch 2507 Loss: 0.170618\n",
      "\tTraining batch 2508 Loss: 0.030741\n",
      "\tTraining batch 2509 Loss: 5.274861\n",
      "\tTraining batch 2510 Loss: 0.564937\n",
      "\tTraining batch 2511 Loss: 1.052026\n",
      "\tTraining batch 2512 Loss: 10.899019\n",
      "Training set: Average loss: 0.741046\n",
      "Validation set: Average loss: 0.779055, Accuracy: 0/1077 (0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.MSELoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 100\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mMore than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\u001b[0m \u001b[0;36m(RuntimeWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib qt\n",
    "#Loop that pulls a different image each time and cycles through the images in the dataloader \n",
    "for (picture,label) in test_loader:\n",
    "    plt.figure()\n",
    "    plt.imshow(picture[0][0,0:,0:])\n",
    "    plt.show()\n",
    "\n",
    "#Loop we used in the meeting that generates the same image over and over, I think because its pulling the same data loader entry\n",
    "# counter=0\n",
    "# for i in test_loader:\n",
    "#     while counter < 10:\n",
    "#         plt.figure()\n",
    "#         plt.imshow(i[0][0][0,0:,0:])\n",
    "#         plt.show()\n",
    "#         counter=counter+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0308], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9945], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8043], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.4506], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9870], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.7998], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9754], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.1924], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.6249], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8022], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.3517], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8685], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6642], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.5204], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([0.0922], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.3623], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.1782], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.2494], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8030], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.3615], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5324], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.8502], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9017], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4377], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.2910], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.5624], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6224], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.6386], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.1868], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.1734], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0194], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.3926], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6995], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4994], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4482], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.0188], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.3861], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.3593], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7588], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([0.0751], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6086], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.2313], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.6985], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.6936], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.9052], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9993], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.1167], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.8604], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6252], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.2327], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.8165], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9865], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.3859], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.9534], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7989], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4857], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6121], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4801], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.7416], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.4393], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.1914], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5986], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5104], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0093], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4101], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.2157], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6522], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4422], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.0825], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.3485], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4798], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7879], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.0018], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.3108], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.8986], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5412], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.5778], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6706], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.3652], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.2339], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.2580], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.0727], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.1349], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.3360], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.5669], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4092], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.3158], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9677], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.4875], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.1435], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.2244], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.3861], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.2389], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.6969], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.3963], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.2012], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9659], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.4441], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7564], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6745], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5556], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6373], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0707], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.3639], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.3528], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.3712], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5288], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5994], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9239], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7789], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6268], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.1936], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8889], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.9125], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.0761], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.2051], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4241], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.6174], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.8786], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.3709], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4237], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.5444], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4058], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8868], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.4679], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9299], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4722], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.8415], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.5639], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6521], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.5068], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.7320], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.1770], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.1589], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.2870], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4715], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.7466], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.6204], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4491], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.7401], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9782], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.1918], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.2919], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4559], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.1941], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7655], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.4630], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.6221], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9103], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.8093], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6558], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9642], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7126], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9267], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9992], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6378], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6526], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4662], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.9112], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.5199], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.6180], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.4139], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9346], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.5750], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6736], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0943], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0660], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.7361], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.1712], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.2268], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.2492], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.2732], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0204], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6459], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.2080], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8428], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([0.1627], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4590], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8472], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.3216], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.1012], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.8078], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.2799], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.6201], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.4616], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.7477], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9051], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.0901], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.8294], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.0835], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.7854], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-3.4981], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.4309], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-2.7386], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.4401], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.8378], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-1.1949], dtype=torch.float64)\n",
      "tensor([[-1.4597]], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.9156], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in test_loader:\n",
    "    # print(i[1].shape)\n",
    "    print(model(i[0].to(device))[0:10],i[1][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(epoch_nums, np.log10(training_loss))\n",
    "plt.plot(epoch_nums, np.log10(validation_loss))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Log of loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Building Confusion matrix, Not relevant in a linear regression model?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Getting predictions from test set...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7437220421c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Plot the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruelabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtick_marks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "# Defining Labels and Predictions\n",
    "truelabels = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "model.cuda\n",
    "print(\"Getting predictions from test set...\")\n",
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    for label in target.cpu().detach().numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model(data).cpu().detach().numpy().argmax(1):\n",
    "        predictions.append(prediction) \n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(truelabels, predictions)\n",
    "tick_marks = np.arange(len(classes))\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "plt.figure(figsize = (7,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "plt.xlabel(\"Predicted Shape\", fontsize = 20)\n",
    "plt.ylabel(\"True Shape\", fontsize = 20)\n",
    "plt.show()"
   ]
  }
 ]
}