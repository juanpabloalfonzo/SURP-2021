{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 3: Predict Star Formation Variables (sSFR, SFR, M*, age) Based on Visual Morphology (galaxy image)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[0;34m[INFO]: \u001b[0mNo release version set. Setting default to DR15\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import decode_image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import marvin\n",
    "from marvin.tools.maps import Maps\n",
    "from marvin.tools.image import Image\n",
    "from marvin.utils.general.images import get_images_by_list\n",
    "from marvin import config\n",
    "from marvin.tools.cube import Cube\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image as image_PIL\n",
    "import time \n",
    "\n",
    "#set config attributes and turn on global downloads of Marvin data\n",
    "config.setRelease('DR15')\n",
    "config.mode = 'local'\n",
    "config.download = True\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.linear1 = torch.nn.Linear(outputSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "\n",
    "#Problem with image associated with manga id at galaxy_list[3548], mangaid- 1-135668\n",
    "galaxy_list=np.delete(galaxy_list,3548)\n",
    "\n",
    "galaxy_list=np.unique(galaxy_list)\n",
    "\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "galaxies=galaxies.sort_values(by=['plateifu']) #Sorting galaxies by plateifu to match ImageFolder Output \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n"
   ]
  },
  {
   "source": [
    "# Importing Images from their Downloaded Locations \n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_locations=[]\n",
    "# for i in range (len(galaxy_list)):\n",
    "#     image_locations.append(Image(galaxy_list[i]).filename)\n",
    "    \n",
    "# image_locations=np.array(image_locations,dtype=str)\n",
    "# np.savetxt('Image Directories',image_locations,fmt='%s')\n",
    "\n",
    "image_locations=np.loadtxt('Image Directories',dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n"
   ]
  },
  {
   "source": [
    "# Putting the Images into DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "46657\nData loaders ready to read /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n"
     ]
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "\n",
    "# image=ImageFolder('/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/') #Picks up 3590 pictures, directory list has lenght 3637 however \n",
    "\n",
    "image_directory='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3'\n",
    "classes= sorted(os.listdir(image_directory))\n",
    "\n",
    "batch_size=50\n",
    "image_copies=13\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        #Rotates the image by some angle\n",
    "        # transforms.RandomRotation(0,360),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        # transform=transformation\n",
    "    )\n",
    "    \n",
    "    #This loop transforms the images and assigns them the correct label \n",
    "\n",
    "    full_dataset_v2 = []   \n",
    "    for i in range(len(full_dataset)): \n",
    "        for j in range(image_copies): #This loops makes it so we have 5 copies of each galaxy image with different orientations \n",
    "            temp=transformation(resize_image(full_dataset[i][0]))\n",
    "            full_dataset_v2.append((temp,log_SFR.iloc[i])) \n",
    "    \n",
    "    full_dataset=full_dataset_v2\n",
    "\n",
    "    print(len(full_dataset))\n",
    "\n",
    "\n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(image_directory)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset ImageFolder\n    Number of datapoints: 3589\n    Root location: /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n3637\n3589\n3589\n"
     ]
    }
   ],
   "source": [
    "print(ImageFolder(image_directory))\n",
    "print(len(image_locations))\n",
    "print(len(np.unique(image_locations)))\n",
    "print(len(np.unique(galaxy_list)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_dataset = torchvision.datasets.ImageFolder(\n",
    "#         root='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/7968',\n",
    "#         # transform=transformation\n",
    "#     ) \n",
    "\n",
    "# full_dataset_v2 = []   \n",
    "# for i in range(len(full_dataset)): \n",
    "#     full_dataset_v2.append((resize_image(full_dataset[i][0]) , i)) \n",
    "\n",
    "# print(full_dataset_v2)\n",
    "\n",
    "\n",
    "# print(full_dataset[0][1])\n",
    "# for i in range(len(full_dataset)): \n",
    "#         full_dataset[i] = (resize_image(full_dataset[i][0]) , 0) \n",
    "        # full_dataset[i][0] = resize_image(full_dataset[i][0])\n",
    "        # full_dataset[i][1] = correct_label(correct_manga_id) "
   ]
  },
  {
   "source": [
    "# Defining the Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (drop): Dropout2d(p=0.2, inplace=False)\n  (fc): Linear(in_features=24576, out_features=10, bias=True)\n  (fc1): Linear(in_features=10, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 128 x 128, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=10)\n",
    "\n",
    "        self.fc1= nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x+1\n",
    "        \n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # print(x)\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x=self.conv1(x)\n",
    "        \n",
    "        x=self.pool(x)\n",
    "       \n",
    "\n",
    "        x=F.relu(x)\n",
    "\n",
    "        \n",
    "      \n",
    " \n",
    "     \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x))) \n",
    "        \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return class probabilities via a log_softmax function\n",
    "        x=F.relu(x) \n",
    "        \n",
    "        x= self.fc1(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(SFR)).to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "# Creating Training Loop/Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target=target.float()\n",
    "        # print(target.shape)\n",
    "        target=target.reshape(-1,1)\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "    \n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        if batch_idx==0:\n",
    "            print(output,target,output-target,loss)\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Create Test Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            target=target.float()\n",
    "            target=target.reshape(-1,1)\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3588],\n",
      "        [-1.7237],\n",
      "        [-3.3298],\n",
      "        [-0.9214],\n",
      "        [-0.7878],\n",
      "        [-2.0445],\n",
      "        [-1.4896],\n",
      "        [-1.1809],\n",
      "        [-1.5734],\n",
      "        [-1.7931],\n",
      "        [-1.8785],\n",
      "        [-0.3588],\n",
      "        [-0.5374],\n",
      "        [-0.9281],\n",
      "        [-1.7062],\n",
      "        [-1.9798],\n",
      "        [-0.5469],\n",
      "        [-1.3866],\n",
      "        [-1.6930],\n",
      "        [-1.0606],\n",
      "        [-1.7489],\n",
      "        [-1.2676],\n",
      "        [-1.0479],\n",
      "        [-1.4253],\n",
      "        [-1.8860],\n",
      "        [-1.3189],\n",
      "        [-3.4482],\n",
      "        [-3.1321],\n",
      "        [-0.8178],\n",
      "        [-2.3272],\n",
      "        [-2.3168],\n",
      "        [-0.5035],\n",
      "        [-0.8856],\n",
      "        [-1.7133],\n",
      "        [-1.0069],\n",
      "        [-1.1096],\n",
      "        [-1.6150],\n",
      "        [-1.7117],\n",
      "        [-1.6519],\n",
      "        [-0.4446],\n",
      "        [-0.5563]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.9253e+00],\n",
      "        [-5.4957e-01],\n",
      "        [-7.6544e-01],\n",
      "        [-2.7156e+00],\n",
      "        [-1.2404e+00],\n",
      "        [-1.5328e+00],\n",
      "        [-1.7665e+00],\n",
      "        [-2.1508e+00],\n",
      "        [-2.4072e+00],\n",
      "        [-2.7987e-01],\n",
      "        [-2.1664e+00],\n",
      "        [-3.0442e+00],\n",
      "        [-7.8789e-01],\n",
      "        [-9.4436e-01],\n",
      "        [-1.9418e+00],\n",
      "        [-1.9967e+00],\n",
      "        [-1.3823e+00],\n",
      "        [-1.3693e+00],\n",
      "        [-1.6609e+00],\n",
      "        [-2.2928e+00],\n",
      "        [-2.7158e-03],\n",
      "        [ 1.5932e-01],\n",
      "        [-1.4757e+00],\n",
      "        [-1.4320e+00],\n",
      "        [-1.9527e+00],\n",
      "        [-5.7889e-01],\n",
      "        [-1.7921e+00],\n",
      "        [-9.9002e-01],\n",
      "        [-5.9124e-01],\n",
      "        [-2.3145e+00],\n",
      "        [-1.3627e+00],\n",
      "        [-1.2464e+00],\n",
      "        [-1.8986e+00],\n",
      "        [-2.0206e+00],\n",
      "        [-7.1869e-01],\n",
      "        [-3.0307e+00],\n",
      "        [-3.1045e+00],\n",
      "        [-8.3443e-01],\n",
      "        [-1.8406e+00],\n",
      "        [-3.2271e+00],\n",
      "        [-8.8401e-01],\n",
      "        [-9.8845e-01],\n",
      "        [-1.7820e+00],\n",
      "        [-2.6396e+00],\n",
      "        [-9.4529e-01],\n",
      "        [-1.5803e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-2.5232e+00],\n",
      "        [-2.3616e-01],\n",
      "        [-6.2145e-01]], device='cuda:0') tensor([[ 0.6138],\n",
      "        [-0.1319],\n",
      "        [-0.3650],\n",
      "        [-0.1395],\n",
      "        [-0.2480],\n",
      "        [-0.0832],\n",
      "        [ 0.0401],\n",
      "        [-0.0083],\n",
      "        [ 0.8150],\n",
      "        [-0.0789],\n",
      "        [ 0.4427],\n",
      "        [-0.2856],\n",
      "        [-0.1335],\n",
      "        [ 0.1565],\n",
      "        [-0.1027],\n",
      "        [ 0.5071],\n",
      "        [ 0.2015],\n",
      "        [-0.2040],\n",
      "        [-0.1322],\n",
      "        [ 0.4144],\n",
      "        [-0.3561],\n",
      "        [-0.6967],\n",
      "        [ 0.5477],\n",
      "        [-0.2742],\n",
      "        [-0.0271],\n",
      "        [ 0.0320],\n",
      "        [ 0.4055],\n",
      "        [-0.7030],\n",
      "        [-0.4694],\n",
      "        [ 0.5656],\n",
      "        [ 0.0951],\n",
      "        [ 0.1985],\n",
      "        [ 0.4733],\n",
      "        [ 0.1346],\n",
      "        [-0.6002],\n",
      "        [-0.4175],\n",
      "        [-0.0276],\n",
      "        [ 0.0166],\n",
      "        [-0.4866],\n",
      "        [ 0.9103],\n",
      "        [ 0.3805],\n",
      "        [ 0.1028],\n",
      "        [ 0.0687],\n",
      "        [ 1.6328],\n",
      "        [-0.1644],\n",
      "        [-0.0348],\n",
      "        [ 0.2256],\n",
      "        [ 0.8713],\n",
      "        [-0.2085],\n",
      "        [ 0.0651]], device='cuda:0', grad_fn=<SubBackward0>) tensor(9.9380, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 10.212251\n",
      "Validation set: Average loss: 10.159542, Accuracy: 0/13998 (0%)\n",
      "\n",
      "996\n",
      "Epoch: 996\n",
      "tensor([[-1.5409],\n",
      "        [-0.9107],\n",
      "        [-1.1650],\n",
      "        [-2.5841],\n",
      "        [-1.6026],\n",
      "        [-1.8918],\n",
      "        [-1.4287],\n",
      "        [-1.6714],\n",
      "        [-1.9909],\n",
      "        [-0.4884],\n",
      "        [-1.8147],\n",
      "        [-2.7210],\n",
      "        [-0.9607],\n",
      "        [-1.3636],\n",
      "        [-1.9206],\n",
      "        [-1.7600],\n",
      "        [-1.5812],\n",
      "        [-1.7899],\n",
      "        [-1.6174],\n",
      "        [-2.0293],\n",
      "        [-0.3587],\n",
      "        [-0.7475],\n",
      "        [-1.0080],\n",
      "        [-1.6088],\n",
      "        [-2.2250],\n",
      "        [-0.7663],\n",
      "        [-1.0338],\n",
      "        [-0.9692],\n",
      "        [-0.3587],\n",
      "        [-2.0665],\n",
      "        [-1.3175],\n",
      "        [-0.9144],\n",
      "        [-1.9702],\n",
      "        [-2.2585],\n",
      "        [-1.3548],\n",
      "        [-3.5842],\n",
      "        [-3.7418],\n",
      "        [-0.7407],\n",
      "        [-1.5920],\n",
      "        [-2.8998],\n",
      "        [-1.0123],\n",
      "        [-1.2360],\n",
      "        [-1.4819],\n",
      "        [-1.5681],\n",
      "        [-1.1251],\n",
      "        [-1.6687],\n",
      "        [-1.3422],\n",
      "        [-1.8663],\n",
      "        [-0.6031],\n",
      "        [-0.9157]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.9253e+00],\n",
      "        [-5.4957e-01],\n",
      "        [-7.6544e-01],\n",
      "        [-2.7156e+00],\n",
      "        [-1.2404e+00],\n",
      "        [-1.5328e+00],\n",
      "        [-1.7665e+00],\n",
      "        [-2.1508e+00],\n",
      "        [-2.4072e+00],\n",
      "        [-2.7987e-01],\n",
      "        [-2.1664e+00],\n",
      "        [-3.0442e+00],\n",
      "        [-7.8789e-01],\n",
      "        [-9.4436e-01],\n",
      "        [-1.9418e+00],\n",
      "        [-1.9967e+00],\n",
      "        [-1.3823e+00],\n",
      "        [-1.3693e+00],\n",
      "        [-1.6609e+00],\n",
      "        [-2.2928e+00],\n",
      "        [-2.7158e-03],\n",
      "        [ 1.5932e-01],\n",
      "        [-1.4757e+00],\n",
      "        [-1.4320e+00],\n",
      "        [-1.9527e+00],\n",
      "        [-5.7889e-01],\n",
      "        [-1.7921e+00],\n",
      "        [-9.9002e-01],\n",
      "        [-5.9124e-01],\n",
      "        [-2.3145e+00],\n",
      "        [-1.3627e+00],\n",
      "        [-1.2464e+00],\n",
      "        [-1.8986e+00],\n",
      "        [-2.0206e+00],\n",
      "        [-7.1869e-01],\n",
      "        [-3.0307e+00],\n",
      "        [-3.1045e+00],\n",
      "        [-8.3443e-01],\n",
      "        [-1.8406e+00],\n",
      "        [-3.2271e+00],\n",
      "        [-8.8401e-01],\n",
      "        [-9.8845e-01],\n",
      "        [-1.7820e+00],\n",
      "        [-2.6396e+00],\n",
      "        [-9.4529e-01],\n",
      "        [-1.5803e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-2.5232e+00],\n",
      "        [-2.3616e-01],\n",
      "        [-6.2145e-01]], device='cuda:0') tensor([[ 0.3844],\n",
      "        [-0.3611],\n",
      "        [-0.3996],\n",
      "        [ 0.1316],\n",
      "        [-0.3621],\n",
      "        [-0.3590],\n",
      "        [ 0.3378],\n",
      "        [ 0.4794],\n",
      "        [ 0.4163],\n",
      "        [-0.2085],\n",
      "        [ 0.3517],\n",
      "        [ 0.3232],\n",
      "        [-0.1728],\n",
      "        [-0.4192],\n",
      "        [ 0.0212],\n",
      "        [ 0.2367],\n",
      "        [-0.1989],\n",
      "        [-0.4206],\n",
      "        [ 0.0435],\n",
      "        [ 0.2635],\n",
      "        [-0.3560],\n",
      "        [-0.9068],\n",
      "        [ 0.4677],\n",
      "        [-0.1768],\n",
      "        [-0.2722],\n",
      "        [-0.1874],\n",
      "        [ 0.7583],\n",
      "        [ 0.0208],\n",
      "        [ 0.2325],\n",
      "        [ 0.2480],\n",
      "        [ 0.0452],\n",
      "        [ 0.3320],\n",
      "        [-0.0716],\n",
      "        [-0.2379],\n",
      "        [-0.6361],\n",
      "        [-0.5535],\n",
      "        [-0.6373],\n",
      "        [ 0.0938],\n",
      "        [ 0.2486],\n",
      "        [ 0.3272],\n",
      "        [-0.1283],\n",
      "        [-0.2476],\n",
      "        [ 0.3001],\n",
      "        [ 1.0715],\n",
      "        [-0.1798],\n",
      "        [-0.0884],\n",
      "        [ 0.5951],\n",
      "        [ 0.6569],\n",
      "        [-0.3670],\n",
      "        [-0.2942]], device='cuda:0', grad_fn=<SubBackward0>) tensor(7.8851, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 10.410924\n",
      "Validation set: Average loss: 8.984848, Accuracy: 0/13998 (0%)\n",
      "\n",
      "997\n",
      "Epoch: 997\n",
      "tensor([[-1.4924],\n",
      "        [-1.1122],\n",
      "        [-0.9775],\n",
      "        [-2.1439],\n",
      "        [-1.7734],\n",
      "        [-1.4275],\n",
      "        [-0.8785],\n",
      "        [-1.9046],\n",
      "        [-1.7790],\n",
      "        [-0.7152],\n",
      "        [-2.0448],\n",
      "        [-3.0189],\n",
      "        [-1.5870],\n",
      "        [-1.1531],\n",
      "        [-2.6573],\n",
      "        [-1.2921],\n",
      "        [-1.2617],\n",
      "        [-1.2994],\n",
      "        [-1.5144],\n",
      "        [-2.0394],\n",
      "        [-0.3640],\n",
      "        [-0.5774],\n",
      "        [-1.1703],\n",
      "        [-1.2718],\n",
      "        [-1.7227],\n",
      "        [-0.3640],\n",
      "        [-1.6201],\n",
      "        [-1.3397],\n",
      "        [-0.3640],\n",
      "        [-2.8226],\n",
      "        [-1.5883],\n",
      "        [-1.2873],\n",
      "        [-1.4147],\n",
      "        [-2.3073],\n",
      "        [-0.9751],\n",
      "        [-3.5095],\n",
      "        [-4.6807],\n",
      "        [-0.5236],\n",
      "        [-2.0069],\n",
      "        [-2.5572],\n",
      "        [-1.0263],\n",
      "        [-0.8379],\n",
      "        [-1.4126],\n",
      "        [-1.8129],\n",
      "        [-0.9067],\n",
      "        [-1.6994],\n",
      "        [-1.2489],\n",
      "        [-1.8917],\n",
      "        [-1.2610],\n",
      "        [-0.6833]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.9253e+00],\n",
      "        [-5.4957e-01],\n",
      "        [-7.6544e-01],\n",
      "        [-2.7156e+00],\n",
      "        [-1.2404e+00],\n",
      "        [-1.5328e+00],\n",
      "        [-1.7665e+00],\n",
      "        [-2.1508e+00],\n",
      "        [-2.4072e+00],\n",
      "        [-2.7987e-01],\n",
      "        [-2.1664e+00],\n",
      "        [-3.0442e+00],\n",
      "        [-7.8789e-01],\n",
      "        [-9.4436e-01],\n",
      "        [-1.9418e+00],\n",
      "        [-1.9967e+00],\n",
      "        [-1.3823e+00],\n",
      "        [-1.3693e+00],\n",
      "        [-1.6609e+00],\n",
      "        [-2.2928e+00],\n",
      "        [-2.7158e-03],\n",
      "        [ 1.5932e-01],\n",
      "        [-1.4757e+00],\n",
      "        [-1.4320e+00],\n",
      "        [-1.9527e+00],\n",
      "        [-5.7889e-01],\n",
      "        [-1.7921e+00],\n",
      "        [-9.9002e-01],\n",
      "        [-5.9124e-01],\n",
      "        [-2.3145e+00],\n",
      "        [-1.3627e+00],\n",
      "        [-1.2464e+00],\n",
      "        [-1.8986e+00],\n",
      "        [-2.0206e+00],\n",
      "        [-7.1869e-01],\n",
      "        [-3.0307e+00],\n",
      "        [-3.1045e+00],\n",
      "        [-8.3443e-01],\n",
      "        [-1.8406e+00],\n",
      "        [-3.2271e+00],\n",
      "        [-8.8401e-01],\n",
      "        [-9.8845e-01],\n",
      "        [-1.7820e+00],\n",
      "        [-2.6396e+00],\n",
      "        [-9.4529e-01],\n",
      "        [-1.5803e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-2.5232e+00],\n",
      "        [-2.3616e-01],\n",
      "        [-6.2145e-01]], device='cuda:0') tensor([[ 0.4328],\n",
      "        [-0.5627],\n",
      "        [-0.2121],\n",
      "        [ 0.5718],\n",
      "        [-0.5330],\n",
      "        [ 0.1053],\n",
      "        [ 0.8880],\n",
      "        [ 0.2462],\n",
      "        [ 0.6282],\n",
      "        [-0.4353],\n",
      "        [ 0.1216],\n",
      "        [ 0.0253],\n",
      "        [-0.7991],\n",
      "        [-0.2087],\n",
      "        [-0.7155],\n",
      "        [ 0.7047],\n",
      "        [ 0.1206],\n",
      "        [ 0.0700],\n",
      "        [ 0.1465],\n",
      "        [ 0.2534],\n",
      "        [-0.3613],\n",
      "        [-0.7368],\n",
      "        [ 0.3054],\n",
      "        [ 0.1602],\n",
      "        [ 0.2300],\n",
      "        [ 0.2149],\n",
      "        [ 0.1720],\n",
      "        [-0.3497],\n",
      "        [ 0.2272],\n",
      "        [-0.5081],\n",
      "        [-0.2257],\n",
      "        [-0.0409],\n",
      "        [ 0.4839],\n",
      "        [-0.2867],\n",
      "        [-0.2564],\n",
      "        [-0.4788],\n",
      "        [-1.5762],\n",
      "        [ 0.3108],\n",
      "        [-0.1663],\n",
      "        [ 0.6699],\n",
      "        [-0.1423],\n",
      "        [ 0.1505],\n",
      "        [ 0.3693],\n",
      "        [ 0.8267],\n",
      "        [ 0.0386],\n",
      "        [-0.1191],\n",
      "        [ 0.6884],\n",
      "        [ 0.6316],\n",
      "        [-1.0249],\n",
      "        [-0.0619]], device='cuda:0', grad_fn=<SubBackward0>) tensor(12.2955, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 10.328379\n",
      "Validation set: Average loss: 9.253222, Accuracy: 0/13998 (0%)\n",
      "\n",
      "998\n",
      "Epoch: 998\n",
      "tensor([[-1.8058],\n",
      "        [-0.7127],\n",
      "        [-1.9292],\n",
      "        [-2.0758],\n",
      "        [-1.2857],\n",
      "        [-1.7006],\n",
      "        [-1.8522],\n",
      "        [-2.2067],\n",
      "        [-1.9489],\n",
      "        [-0.3651],\n",
      "        [-1.7811],\n",
      "        [-3.1118],\n",
      "        [-1.3358],\n",
      "        [-1.5465],\n",
      "        [-1.8957],\n",
      "        [-0.8598],\n",
      "        [-1.2251],\n",
      "        [-1.6123],\n",
      "        [-1.2248],\n",
      "        [-2.1702],\n",
      "        [-0.3651],\n",
      "        [-0.3651],\n",
      "        [-1.5915],\n",
      "        [-1.2475],\n",
      "        [-2.2085],\n",
      "        [-0.3651],\n",
      "        [-1.2277],\n",
      "        [-1.4863],\n",
      "        [-0.3651],\n",
      "        [-2.1532],\n",
      "        [-1.1806],\n",
      "        [-1.1004],\n",
      "        [-1.6389],\n",
      "        [-1.7986],\n",
      "        [-1.4170],\n",
      "        [-2.6497],\n",
      "        [-2.3578],\n",
      "        [-0.8825],\n",
      "        [-1.3186],\n",
      "        [-3.0268],\n",
      "        [-0.6832],\n",
      "        [-0.4403],\n",
      "        [-1.4694],\n",
      "        [-3.5908],\n",
      "        [-0.8524],\n",
      "        [-1.0883],\n",
      "        [-1.0111],\n",
      "        [-2.0603],\n",
      "        [-1.2387],\n",
      "        [-0.6213]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.9253e+00],\n",
      "        [-5.4957e-01],\n",
      "        [-7.6544e-01],\n",
      "        [-2.7156e+00],\n",
      "        [-1.2404e+00],\n",
      "        [-1.5328e+00],\n",
      "        [-1.7665e+00],\n",
      "        [-2.1508e+00],\n",
      "        [-2.4072e+00],\n",
      "        [-2.7987e-01],\n",
      "        [-2.1664e+00],\n",
      "        [-3.0442e+00],\n",
      "        [-7.8789e-01],\n",
      "        [-9.4436e-01],\n",
      "        [-1.9418e+00],\n",
      "        [-1.9967e+00],\n",
      "        [-1.3823e+00],\n",
      "        [-1.3693e+00],\n",
      "        [-1.6609e+00],\n",
      "        [-2.2928e+00],\n",
      "        [-2.7158e-03],\n",
      "        [ 1.5932e-01],\n",
      "        [-1.4757e+00],\n",
      "        [-1.4320e+00],\n",
      "        [-1.9527e+00],\n",
      "        [-5.7889e-01],\n",
      "        [-1.7921e+00],\n",
      "        [-9.9002e-01],\n",
      "        [-5.9124e-01],\n",
      "        [-2.3145e+00],\n",
      "        [-1.3627e+00],\n",
      "        [-1.2464e+00],\n",
      "        [-1.8986e+00],\n",
      "        [-2.0206e+00],\n",
      "        [-7.1869e-01],\n",
      "        [-3.0307e+00],\n",
      "        [-3.1045e+00],\n",
      "        [-8.3443e-01],\n",
      "        [-1.8406e+00],\n",
      "        [-3.2271e+00],\n",
      "        [-8.8401e-01],\n",
      "        [-9.8845e-01],\n",
      "        [-1.7820e+00],\n",
      "        [-2.6396e+00],\n",
      "        [-9.4529e-01],\n",
      "        [-1.5803e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-2.5232e+00],\n",
      "        [-2.3616e-01],\n",
      "        [-6.2145e-01]], device='cuda:0') tensor([[ 1.1942e-01],\n",
      "        [-1.6309e-01],\n",
      "        [-1.1638e+00],\n",
      "        [ 6.3983e-01],\n",
      "        [-4.5221e-02],\n",
      "        [-1.6778e-01],\n",
      "        [-8.5735e-02],\n",
      "        [-5.5916e-02],\n",
      "        [ 4.5834e-01],\n",
      "        [-8.5210e-02],\n",
      "        [ 3.8532e-01],\n",
      "        [-6.7580e-02],\n",
      "        [-5.4786e-01],\n",
      "        [-6.0217e-01],\n",
      "        [ 4.6177e-02],\n",
      "        [ 1.1369e+00],\n",
      "        [ 1.5721e-01],\n",
      "        [-2.4296e-01],\n",
      "        [ 4.3606e-01],\n",
      "        [ 1.2261e-01],\n",
      "        [-3.6236e-01],\n",
      "        [-5.2440e-01],\n",
      "        [-1.1574e-01],\n",
      "        [ 1.8445e-01],\n",
      "        [-2.5581e-01],\n",
      "        [ 2.1381e-01],\n",
      "        [ 5.6436e-01],\n",
      "        [-4.9629e-01],\n",
      "        [ 2.2616e-01],\n",
      "        [ 1.6126e-01],\n",
      "        [ 1.8207e-01],\n",
      "        [ 1.4604e-01],\n",
      "        [ 2.5968e-01],\n",
      "        [ 2.2202e-01],\n",
      "        [-6.9831e-01],\n",
      "        [ 3.8097e-01],\n",
      "        [ 7.4670e-01],\n",
      "        [-4.8065e-02],\n",
      "        [ 5.2196e-01],\n",
      "        [ 2.0025e-01],\n",
      "        [ 2.0081e-01],\n",
      "        [ 5.4810e-01],\n",
      "        [ 3.1255e-01],\n",
      "        [-9.5118e-01],\n",
      "        [ 9.2858e-02],\n",
      "        [ 4.9192e-01],\n",
      "        [ 9.2623e-01],\n",
      "        [ 4.6292e-01],\n",
      "        [-1.0025e+00],\n",
      "        [ 1.8150e-04]], device='cuda:0', grad_fn=<SubBackward0>) tensor(11.0163, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 10.188139\n",
      "Validation set: Average loss: 9.823438, Accuracy: 0/13998 (0%)\n",
      "\n",
      "999\n",
      "Epoch: 999\n",
      "tensor([[-1.8972],\n",
      "        [-0.6590],\n",
      "        [-1.6978],\n",
      "        [-2.7625],\n",
      "        [-0.7417],\n",
      "        [-1.4796],\n",
      "        [-1.7717],\n",
      "        [-1.1743],\n",
      "        [-2.1866],\n",
      "        [-1.0676],\n",
      "        [-2.4930],\n",
      "        [-2.5414],\n",
      "        [-1.3717],\n",
      "        [-1.0960],\n",
      "        [-1.7819],\n",
      "        [-1.4469],\n",
      "        [-1.3006],\n",
      "        [-0.9437],\n",
      "        [-1.4846],\n",
      "        [-1.5477],\n",
      "        [-0.3689],\n",
      "        [-0.6072],\n",
      "        [-1.3676],\n",
      "        [-0.5924],\n",
      "        [-2.2417],\n",
      "        [-0.8196],\n",
      "        [-1.1981],\n",
      "        [-1.3821],\n",
      "        [-0.3689],\n",
      "        [-1.9813],\n",
      "        [-1.0169],\n",
      "        [-1.6218],\n",
      "        [-1.6667],\n",
      "        [-1.7983],\n",
      "        [-1.0401],\n",
      "        [-2.9025],\n",
      "        [-1.3479],\n",
      "        [-0.5883],\n",
      "        [-1.4838],\n",
      "        [-3.0734],\n",
      "        [-0.8703],\n",
      "        [-0.9463],\n",
      "        [-2.2494],\n",
      "        [-1.7551],\n",
      "        [-0.9602],\n",
      "        [-1.6023],\n",
      "        [-0.9757],\n",
      "        [-2.0186],\n",
      "        [-0.4986],\n",
      "        [-1.5643]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.9253e+00],\n",
      "        [-5.4957e-01],\n",
      "        [-7.6544e-01],\n",
      "        [-2.7156e+00],\n",
      "        [-1.2404e+00],\n",
      "        [-1.5328e+00],\n",
      "        [-1.7665e+00],\n",
      "        [-2.1508e+00],\n",
      "        [-2.4072e+00],\n",
      "        [-2.7987e-01],\n",
      "        [-2.1664e+00],\n",
      "        [-3.0442e+00],\n",
      "        [-7.8789e-01],\n",
      "        [-9.4436e-01],\n",
      "        [-1.9418e+00],\n",
      "        [-1.9967e+00],\n",
      "        [-1.3823e+00],\n",
      "        [-1.3693e+00],\n",
      "        [-1.6609e+00],\n",
      "        [-2.2928e+00],\n",
      "        [-2.7158e-03],\n",
      "        [ 1.5932e-01],\n",
      "        [-1.4757e+00],\n",
      "        [-1.4320e+00],\n",
      "        [-1.9527e+00],\n",
      "        [-5.7889e-01],\n",
      "        [-1.7921e+00],\n",
      "        [-9.9002e-01],\n",
      "        [-5.9124e-01],\n",
      "        [-2.3145e+00],\n",
      "        [-1.3627e+00],\n",
      "        [-1.2464e+00],\n",
      "        [-1.8986e+00],\n",
      "        [-2.0206e+00],\n",
      "        [-7.1869e-01],\n",
      "        [-3.0307e+00],\n",
      "        [-3.1045e+00],\n",
      "        [-8.3443e-01],\n",
      "        [-1.8406e+00],\n",
      "        [-3.2271e+00],\n",
      "        [-8.8401e-01],\n",
      "        [-9.8845e-01],\n",
      "        [-1.7820e+00],\n",
      "        [-2.6396e+00],\n",
      "        [-9.4529e-01],\n",
      "        [-1.5803e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-2.5232e+00],\n",
      "        [-2.3616e-01],\n",
      "        [-6.2145e-01]], device='cuda:0') tensor([[ 0.0281],\n",
      "        [-0.1094],\n",
      "        [-0.9324],\n",
      "        [-0.0469],\n",
      "        [ 0.4987],\n",
      "        [ 0.0532],\n",
      "        [-0.0052],\n",
      "        [ 0.9765],\n",
      "        [ 0.2207],\n",
      "        [-0.7877],\n",
      "        [-0.3266],\n",
      "        [ 0.5028],\n",
      "        [-0.5838],\n",
      "        [-0.1516],\n",
      "        [ 0.1600],\n",
      "        [ 0.5498],\n",
      "        [ 0.0818],\n",
      "        [ 0.4257],\n",
      "        [ 0.1763],\n",
      "        [ 0.7451],\n",
      "        [-0.3662],\n",
      "        [-0.7665],\n",
      "        [ 0.1082],\n",
      "        [ 0.8396],\n",
      "        [-0.2890],\n",
      "        [-0.2407],\n",
      "        [ 0.5940],\n",
      "        [-0.3921],\n",
      "        [ 0.2223],\n",
      "        [ 0.3331],\n",
      "        [ 0.3458],\n",
      "        [-0.3754],\n",
      "        [ 0.2319],\n",
      "        [ 0.2223],\n",
      "        [-0.3214],\n",
      "        [ 0.1282],\n",
      "        [ 1.7566],\n",
      "        [ 0.2461],\n",
      "        [ 0.3568],\n",
      "        [ 0.1536],\n",
      "        [ 0.0137],\n",
      "        [ 0.0422],\n",
      "        [-0.4674],\n",
      "        [ 0.8845],\n",
      "        [-0.0149],\n",
      "        [-0.0220],\n",
      "        [ 0.9616],\n",
      "        [ 0.5046],\n",
      "        [-0.2624],\n",
      "        [-0.9428]], device='cuda:0', grad_fn=<SubBackward0>) tensor(13.7516, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 10.274499\n",
      "Validation set: Average loss: 8.805415, Accuracy: 0/13998 (0%)\n",
      "\n",
      "1000\n",
      "Epoch: 1000\n",
      "tensor([[-2.1107],\n",
      "        [-0.6394],\n",
      "        [-1.1703],\n",
      "        [-2.4763],\n",
      "        [-1.1634],\n",
      "        [-1.6778],\n",
      "        [-2.4886],\n",
      "        [-1.5943],\n",
      "        [-1.7811],\n",
      "        [-0.4072],\n",
      "        [-1.8224],\n",
      "        [-2.9133],\n",
      "        [-1.1080],\n",
      "        [-1.0445],\n",
      "        [-1.5791],\n",
      "        [-1.4401],\n",
      "        [-1.1789],\n",
      "        [-1.9117],\n",
      "        [-1.5477],\n",
      "        [-1.9508],\n",
      "        [-0.6252],\n",
      "        [-0.9201],\n",
      "        [-1.1968],\n",
      "        [-1.7475],\n",
      "        [-2.2649],\n",
      "        [-0.6421],\n",
      "        [-1.2173],\n",
      "        [-1.3709],\n",
      "        [-0.3664],\n",
      "        [-1.4373],\n",
      "        [-1.0481],\n",
      "        [-1.0572],\n",
      "        [-1.9787],\n",
      "        [-1.4720],\n",
      "        [-1.0432],\n",
      "        [-3.0741],\n",
      "        [-2.3764],\n",
      "        [-0.6155],\n",
      "        [-1.4801],\n",
      "        [-2.7296],\n",
      "        [-0.8510],\n",
      "        [-0.8583],\n",
      "        [-1.1494],\n",
      "        [-1.5736],\n",
      "        [-0.9176],\n",
      "        [-1.5735],\n",
      "        [-1.8277],\n",
      "        [-2.0513],\n",
      "        [-0.7212],\n",
      "        [-1.2035]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.9253e+00],\n",
      "        [-5.4957e-01],\n",
      "        [-7.6544e-01],\n",
      "        [-2.7156e+00],\n",
      "        [-1.2404e+00],\n",
      "        [-1.5328e+00],\n",
      "        [-1.7665e+00],\n",
      "        [-2.1508e+00],\n",
      "        [-2.4072e+00],\n",
      "        [-2.7987e-01],\n",
      "        [-2.1664e+00],\n",
      "        [-3.0442e+00],\n",
      "        [-7.8789e-01],\n",
      "        [-9.4436e-01],\n",
      "        [-1.9418e+00],\n",
      "        [-1.9967e+00],\n",
      "        [-1.3823e+00],\n",
      "        [-1.3693e+00],\n",
      "        [-1.6609e+00],\n",
      "        [-2.2928e+00],\n",
      "        [-2.7158e-03],\n",
      "        [ 1.5932e-01],\n",
      "        [-1.4757e+00],\n",
      "        [-1.4320e+00],\n",
      "        [-1.9527e+00],\n",
      "        [-5.7889e-01],\n",
      "        [-1.7921e+00],\n",
      "        [-9.9002e-01],\n",
      "        [-5.9124e-01],\n",
      "        [-2.3145e+00],\n",
      "        [-1.3627e+00],\n",
      "        [-1.2464e+00],\n",
      "        [-1.8986e+00],\n",
      "        [-2.0206e+00],\n",
      "        [-7.1869e-01],\n",
      "        [-3.0307e+00],\n",
      "        [-3.1045e+00],\n",
      "        [-8.3443e-01],\n",
      "        [-1.8406e+00],\n",
      "        [-3.2271e+00],\n",
      "        [-8.8401e-01],\n",
      "        [-9.8845e-01],\n",
      "        [-1.7820e+00],\n",
      "        [-2.6396e+00],\n",
      "        [-9.4529e-01],\n",
      "        [-1.5803e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-2.5232e+00],\n",
      "        [-2.3616e-01],\n",
      "        [-6.2145e-01]], device='cuda:0') tensor([[-0.1854],\n",
      "        [-0.0898],\n",
      "        [-0.4048],\n",
      "        [ 0.2393],\n",
      "        [ 0.0770],\n",
      "        [-0.1450],\n",
      "        [-0.7221],\n",
      "        [ 0.5564],\n",
      "        [ 0.6261],\n",
      "        [-0.1273],\n",
      "        [ 0.3440],\n",
      "        [ 0.1309],\n",
      "        [-0.3201],\n",
      "        [-0.1002],\n",
      "        [ 0.3627],\n",
      "        [ 0.5566],\n",
      "        [ 0.2035],\n",
      "        [-0.5424],\n",
      "        [ 0.1132],\n",
      "        [ 0.3420],\n",
      "        [-0.6225],\n",
      "        [-1.0794],\n",
      "        [ 0.2789],\n",
      "        [-0.3155],\n",
      "        [-0.3121],\n",
      "        [-0.0632],\n",
      "        [ 0.5748],\n",
      "        [-0.3809],\n",
      "        [ 0.2248],\n",
      "        [ 0.8771],\n",
      "        [ 0.3146],\n",
      "        [ 0.1892],\n",
      "        [-0.0802],\n",
      "        [ 0.5487],\n",
      "        [-0.3245],\n",
      "        [-0.0434],\n",
      "        [ 0.7281],\n",
      "        [ 0.2190],\n",
      "        [ 0.3605],\n",
      "        [ 0.4975],\n",
      "        [ 0.0330],\n",
      "        [ 0.1301],\n",
      "        [ 0.6325],\n",
      "        [ 1.0660],\n",
      "        [ 0.0277],\n",
      "        [ 0.0067],\n",
      "        [ 0.1096],\n",
      "        [ 0.4719],\n",
      "        [-0.4851],\n",
      "        [-0.5820]], device='cuda:0', grad_fn=<SubBackward0>) tensor(9.6804, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 10.103092\n",
      "Validation set: Average loss: 9.482179, Accuracy: 0/13998 (0%)\n",
      "\n",
      "106.5281120578448\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 1000\n",
    "print('Training on', device)\n",
    "time_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "time_end = time.time()\n",
    "total_time = (time_end-time_start)/60 \n",
    "print(total_time)"
   ]
  },
  {
   "source": [
    "# Checking How Well the Model is Preforming "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truths_train = [] \n",
    "all_preds_train = [] \n",
    "for (data,target) in train_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_train.append(target.cpu().detach().numpy())\n",
    "    all_preds_train.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_train=len(all_truths_train)-1\n",
    "\n",
    "remainder_train=len(all_truths_train[incomplete_batch_id_train])\n",
    "\n",
    "total_values_train=(len(all_truths_train)*batch_size)-(batch_size-remainder_train)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_train_array=np.zeros(total_values_train)\n",
    "all_preds_train_array=np.zeros(total_values_train)\n",
    "k=0\n",
    "while k < total_values_train:\n",
    "    for i in range(len(all_truths_train)):\n",
    "        if i<incomplete_batch_id_train:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_train\n",
    "            for j in range(remainder_train):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# all_truths_train=all_truths_train_array\n",
    "# all_preds_train=all_preds_train_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.8297485552040986\n[-1.666185]\n-1.8297485552040986\n-1.6661850214004517\n"
     ]
    }
   ],
   "source": [
    "all_truths_test = [] \n",
    "all_preds_test = [] \n",
    "for (data,target) in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_test.append(target.cpu().detach().numpy())\n",
    "    all_preds_test.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_test=len(all_truths_test)-1\n",
    "\n",
    "remainder_test=len(all_truths_test[incomplete_batch_id_test])\n",
    "\n",
    "\n",
    "total_values_test=(len(all_truths_test)*batch_size)-(batch_size-remainder_test)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_test_array=np.zeros(total_values_test)\n",
    "all_preds_test_array=np.zeros(total_values_test)\n",
    "k=0\n",
    "while k < total_values_test:\n",
    "    for i in range(len(all_truths_test)):\n",
    "        if i<incomplete_batch_id_test:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_test\n",
    "            for j in range(remainder_test):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "print(all_truths_test[3][43])\n",
    "print(all_preds_test[3][43])\n",
    "print(all_truths_test_array[193])\n",
    "print(all_preds_test_array[193])\n",
    "# all_truths_test=all_truths_test_array\n",
    "# all_preds_test=all_preds_test_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train=r2_score(all_truths_train_array,all_preds_train_array)\n",
    "r2_test=r2_score(all_truths_test_array,all_preds_test_array)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.subplot(1,2,1) \n",
    "plt.title('Test Data Set')\n",
    "plt.scatter(all_truths_test_array, all_preds_test_array,color = 'b', alpha = 0.1*7/3)\n",
    "plt.xlabel('Test True Values')\n",
    "plt.ylabel('Test Predicted Value')\n",
    "plt.text(0,-5,'R$^2$='+ str(round(r2_test,4)))\n",
    "plt.plot([-5,1],[-5,1],'r--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Train Data Set')\n",
    "plt.scatter(all_truths_train_array, all_preds_train_array,color = 'k', alpha = 0.1)\n",
    "plt.xlabel('Train True Values')\n",
    "plt.ylabel('Train Predicted Value')\n",
    "plt.plot([-5,1],[-5,1],'r--') \n",
    "plt.text(0,-5.5,'R$^2$='+str(round(r2_train,3)))\n",
    "plt.show()\n",
    "plt.savefig('/home/juanp/Documents/SURP-2021/Plots/Model 3/13 Copies of Images.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/juanp/Documents/SURP-2021/Models/SFR_Model_13_Copies_Images')"
   ]
  },
  {
   "source": [
    "# Loss as a Function of Epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.plot(epoch_nums, np.log10(training_loss))\n",
    "# plt.plot(epoch_nums, np.log10(validation_loss))\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('Log of loss')\n",
    "# plt.legend(['training', 'validation'], loc='upper right')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Plotting Galaxy Images and their True SFR vs CNN Predicted SFR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(picture[0][0,0:,0:])\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(picture[1][0,0:,0:])\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(picture[2][0,0:,0:])\n",
    "#     plt.show()"
   ]
  }
 ]
}