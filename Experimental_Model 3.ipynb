{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 3: Predict Star Formation Variables (sSFR, SFR, M*, age) Based on Visual Morphology (galaxy image)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import decode_image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import marvin\n",
    "from marvin.tools.maps import Maps\n",
    "from marvin.tools.image import Image\n",
    "from marvin.utils.general.images import get_images_by_list\n",
    "from marvin import config\n",
    "from marvin.tools.cube import Cube\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image as image_PIL\n",
    "import time \n",
    "\n",
    "#set config attributes and turn on global downloads of Marvin data\n",
    "config.setRelease('DR15')\n",
    "config.mode = 'local'\n",
    "config.download = True\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.linear1 = torch.nn.Linear(outputSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "\n",
    "#Problem with image associated with manga id at galaxy_list[3548], mangaid- 1-135668\n",
    "galaxy_list=np.delete(galaxy_list,3548)\n",
    "\n",
    "galaxy_list=np.unique(galaxy_list)\n",
    "\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "galaxies=galaxies.sort_values(by=['plateifu']) #Sorting galaxies by plateifu to match ImageFolder Output \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n"
   ]
  },
  {
   "source": [
    "# Importing Images from their Downloaded Locations \n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_locations=[]\n",
    "# for i in range (len(galaxy_list)):\n",
    "#     image_locations.append(Image(galaxy_list[i]).filename)\n",
    "    \n",
    "# image_locations=np.array(image_locations,dtype=str)\n",
    "# np.savetxt('Image Directories',image_locations,fmt='%s')\n",
    "\n",
    "image_locations=np.loadtxt('Image Directories',dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n"
   ]
  },
  {
   "source": [
    "# Putting the Images into DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "89725\n",
      "Data loaders ready to read /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n"
     ]
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "\n",
    "# image=ImageFolder('/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/') #Picks up 3590 pictures, directory list has lenght 3637 however \n",
    "\n",
    "image_directory='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3'\n",
    "classes= sorted(os.listdir(image_directory))\n",
    "\n",
    "batch_size=50\n",
    "image_copies=30\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        #Rotates the image by some angle\n",
    "        # transforms.RandomRotation(0,360),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        # transform=transformation\n",
    "    )\n",
    "    \n",
    "    #This loop transforms the images and assigns them the correct label \n",
    "\n",
    "    full_dataset_v2 = []   \n",
    "    for i in range(len(full_dataset)): \n",
    "        for j in range(image_copies): #This loops makes it so we have 5 copies of each galaxy image with different orientations \n",
    "            temp=transformation(resize_image(full_dataset[i][0]))\n",
    "            full_dataset_v2.append((temp,log_SFR.iloc[i])) \n",
    "    \n",
    "    full_dataset=full_dataset_v2\n",
    "\n",
    "    print(len(full_dataset))\n",
    "\n",
    "\n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(image_directory)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset ImageFolder\n    Number of datapoints: 3589\n    Root location: /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n3637\n3589\n3589\n"
     ]
    }
   ],
   "source": [
    "print(ImageFolder(image_directory))\n",
    "print(len(image_locations))\n",
    "print(len(np.unique(image_locations)))\n",
    "print(len(np.unique(galaxy_list)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_dataset = torchvision.datasets.ImageFolder(\n",
    "#         root='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/7968',\n",
    "#         # transform=transformation\n",
    "#     ) \n",
    "\n",
    "# full_dataset_v2 = []   \n",
    "# for i in range(len(full_dataset)): \n",
    "#     full_dataset_v2.append((resize_image(full_dataset[i][0]) , i)) \n",
    "\n",
    "# print(full_dataset_v2)\n",
    "\n",
    "\n",
    "# print(full_dataset[0][1])\n",
    "# for i in range(len(full_dataset)): \n",
    "#         full_dataset[i] = (resize_image(full_dataset[i][0]) , 0) \n",
    "        # full_dataset[i][0] = resize_image(full_dataset[i][0])\n",
    "        # full_dataset[i][1] = correct_label(correct_manga_id) "
   ]
  },
  {
   "source": [
    "# Defining the Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (drop): Dropout2d(p=0.2, inplace=False)\n  (fc): Linear(in_features=24576, out_features=10, bias=True)\n  (fc1): Linear(in_features=10, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 128 x 128, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=10)\n",
    "\n",
    "        self.fc1= nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x+1\n",
    "        \n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # print(x)\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x=self.conv1(x)\n",
    "        \n",
    "        x=self.pool(x)\n",
    "       \n",
    "\n",
    "        x=F.relu(x)\n",
    "\n",
    "        \n",
    "      \n",
    " \n",
    "     \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x))) \n",
    "        \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return class probabilities via a log_softmax function\n",
    "        x=F.relu(x) \n",
    "        \n",
    "        x= self.fc1(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(SFR)).to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "# Creating Training Loop/Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target=target.float()\n",
    "        # print(target.shape)\n",
    "        target=target.reshape(-1,1)\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "    \n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        if batch_idx==0:\n",
    "            print(output,target,output-target,loss)\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Create Test Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            target=target.float()\n",
    "            target=target.reshape(-1,1)\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "],\n",
      "        [ 0.1095],\n",
      "        [ 0.0254]], device='cuda:0') tensor([[ 0.5045],\n",
      "        [-0.1642],\n",
      "        [-0.3575],\n",
      "        [ 0.7383],\n",
      "        [ 0.3190],\n",
      "        [-0.1487],\n",
      "        [-0.1908],\n",
      "        [-0.3204],\n",
      "        [ 0.5308],\n",
      "        [-0.7021],\n",
      "        [-0.3097],\n",
      "        [ 0.7572],\n",
      "        [ 0.2726],\n",
      "        [ 0.0287],\n",
      "        [ 0.1982],\n",
      "        [ 0.3025],\n",
      "        [ 0.2231],\n",
      "        [-0.3559],\n",
      "        [ 0.4273],\n",
      "        [ 0.3230],\n",
      "        [-0.5748],\n",
      "        [ 0.1478],\n",
      "        [ 0.1924],\n",
      "        [ 0.1751],\n",
      "        [-0.3105],\n",
      "        [-0.0062],\n",
      "        [-0.2061],\n",
      "        [ 0.2849],\n",
      "        [-0.3880],\n",
      "        [-0.0635],\n",
      "        [ 0.3023],\n",
      "        [ 0.2758],\n",
      "        [-0.0853],\n",
      "        [-0.6468],\n",
      "        [ 0.1672],\n",
      "        [ 0.1555],\n",
      "        [ 0.0328],\n",
      "        [-0.2489],\n",
      "        [ 0.0554],\n",
      "        [-0.0989],\n",
      "        [ 0.3417],\n",
      "        [-0.1485],\n",
      "        [-0.1076],\n",
      "        [-0.1136],\n",
      "        [ 0.2678],\n",
      "        [-0.0429],\n",
      "        [-0.5968],\n",
      "        [-0.2396],\n",
      "        [-0.3691],\n",
      "        [-0.3094]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.7286, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.982386\n",
      "Validation set: Average loss: 2.990823, Accuracy: 0/26918 (0%)\n",
      "\n",
      "995\n",
      "Epoch: 995\n",
      "tensor([[-2.5236],\n",
      "        [-2.9411],\n",
      "        [-1.7586],\n",
      "        [-1.8528],\n",
      "        [-2.7704],\n",
      "        [-1.3269],\n",
      "        [-2.0580],\n",
      "        [-1.0192],\n",
      "        [-1.6632],\n",
      "        [-2.5013],\n",
      "        [-1.0776],\n",
      "        [-2.8125],\n",
      "        [-2.9161],\n",
      "        [-1.1543],\n",
      "        [-1.9190],\n",
      "        [-1.8613],\n",
      "        [-1.7453],\n",
      "        [-1.1338],\n",
      "        [-0.8011],\n",
      "        [-1.8425],\n",
      "        [-1.1479],\n",
      "        [-2.4155],\n",
      "        [-1.1572],\n",
      "        [-1.4403],\n",
      "        [-1.1731],\n",
      "        [-0.9818],\n",
      "        [-0.9181],\n",
      "        [-0.8339],\n",
      "        [-1.0441],\n",
      "        [-1.3004],\n",
      "        [-3.2214],\n",
      "        [-0.3129],\n",
      "        [-3.0551],\n",
      "        [-1.4527],\n",
      "        [-2.3047],\n",
      "        [-1.3119],\n",
      "        [-1.0162],\n",
      "        [-0.7082],\n",
      "        [-1.6209],\n",
      "        [-1.3958],\n",
      "        [-1.2829],\n",
      "        [-1.0672],\n",
      "        [-1.1785],\n",
      "        [-0.5022],\n",
      "        [-1.6602],\n",
      "        [-0.9663],\n",
      "        [-1.1445],\n",
      "        [-1.4932],\n",
      "        [-0.1780],\n",
      "        [-0.0094]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.7846],\n",
      "        [-3.1074],\n",
      "        [-2.2029],\n",
      "        [-2.8539],\n",
      "        [-2.2303],\n",
      "        [-1.3859],\n",
      "        [-2.3475],\n",
      "        [-0.8304],\n",
      "        [-1.9239],\n",
      "        [-2.5190],\n",
      "        [-0.9444],\n",
      "        [-2.9536],\n",
      "        [-2.3707],\n",
      "        [-1.4242],\n",
      "        [-1.8178],\n",
      "        [-1.8720],\n",
      "        [-0.9926],\n",
      "        [-1.1143],\n",
      "        [-1.2063],\n",
      "        [-2.8130],\n",
      "        [-0.9279],\n",
      "        [-2.5624],\n",
      "        [-1.4155],\n",
      "        [-1.0044],\n",
      "        [-1.1255],\n",
      "        [-0.8840],\n",
      "        [-1.1321],\n",
      "        [-1.0339],\n",
      "        [-1.0240],\n",
      "        [-1.0735],\n",
      "        [-3.2359],\n",
      "        [-0.8875],\n",
      "        [-2.7982],\n",
      "        [-1.2461],\n",
      "        [-2.7090],\n",
      "        [-1.1547],\n",
      "        [-1.1439],\n",
      "        [-1.1777],\n",
      "        [-1.5478],\n",
      "        [-0.4903],\n",
      "        [-1.9299],\n",
      "        [-1.1503],\n",
      "        [-1.0735],\n",
      "        [ 0.0658],\n",
      "        [-1.8406],\n",
      "        [-0.8046],\n",
      "        [-1.0124],\n",
      "        [-1.5583],\n",
      "        [ 0.1095],\n",
      "        [ 0.0254]], device='cuda:0') tensor([[ 0.2610],\n",
      "        [ 0.1663],\n",
      "        [ 0.4444],\n",
      "        [ 1.0011],\n",
      "        [-0.5401],\n",
      "        [ 0.0590],\n",
      "        [ 0.2894],\n",
      "        [-0.1888],\n",
      "        [ 0.2607],\n",
      "        [ 0.0177],\n",
      "        [-0.1333],\n",
      "        [ 0.1411],\n",
      "        [-0.5454],\n",
      "        [ 0.2699],\n",
      "        [-0.1012],\n",
      "        [ 0.0106],\n",
      "        [-0.7527],\n",
      "        [-0.0196],\n",
      "        [ 0.4053],\n",
      "        [ 0.9706],\n",
      "        [-0.2200],\n",
      "        [ 0.1470],\n",
      "        [ 0.2582],\n",
      "        [-0.4359],\n",
      "        [-0.0476],\n",
      "        [-0.0978],\n",
      "        [ 0.2141],\n",
      "        [ 0.2000],\n",
      "        [-0.0201],\n",
      "        [-0.2269],\n",
      "        [ 0.0145],\n",
      "        [ 0.5746],\n",
      "        [-0.2569],\n",
      "        [-0.2066],\n",
      "        [ 0.4043],\n",
      "        [-0.1572],\n",
      "        [ 0.1277],\n",
      "        [ 0.4695],\n",
      "        [-0.0731],\n",
      "        [-0.9054],\n",
      "        [ 0.6471],\n",
      "        [ 0.0831],\n",
      "        [-0.1050],\n",
      "        [-0.5680],\n",
      "        [ 0.1804],\n",
      "        [-0.1617],\n",
      "        [-0.1321],\n",
      "        [ 0.0651],\n",
      "        [-0.2875],\n",
      "        [-0.0348]], device='cuda:0', grad_fn=<SubBackward0>) tensor(6.9586, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.976720\n",
      "Validation set: Average loss: 2.723225, Accuracy: 0/26918 (0%)\n",
      "\n",
      "996\n",
      "Epoch: 996\n",
      "tensor([[-3.2315],\n",
      "        [-2.3220],\n",
      "        [-2.9520],\n",
      "        [-2.6831],\n",
      "        [-2.2819],\n",
      "        [-1.4481],\n",
      "        [-2.3980],\n",
      "        [-0.9443],\n",
      "        [-1.8365],\n",
      "        [-2.4403],\n",
      "        [-0.5410],\n",
      "        [-2.6016],\n",
      "        [-1.6625],\n",
      "        [-1.3950],\n",
      "        [-1.7176],\n",
      "        [-1.6892],\n",
      "        [-0.8763],\n",
      "        [-1.3596],\n",
      "        [-1.0169],\n",
      "        [-2.9952],\n",
      "        [-1.5863],\n",
      "        [-2.0105],\n",
      "        [-1.1381],\n",
      "        [-1.0150],\n",
      "        [-1.1939],\n",
      "        [-0.7179],\n",
      "        [-1.6929],\n",
      "        [-1.6277],\n",
      "        [-1.1158],\n",
      "        [-1.2579],\n",
      "        [-3.7076],\n",
      "        [-0.5889],\n",
      "        [-2.5107],\n",
      "        [-1.3023],\n",
      "        [-3.1135],\n",
      "        [-1.5512],\n",
      "        [-1.2122],\n",
      "        [-1.7111],\n",
      "        [-1.5780],\n",
      "        [-1.2360],\n",
      "        [-1.6822],\n",
      "        [-1.0646],\n",
      "        [-1.2544],\n",
      "        [-0.1941],\n",
      "        [-1.6883],\n",
      "        [-0.8609],\n",
      "        [-1.2217],\n",
      "        [-1.6243],\n",
      "        [-0.1763],\n",
      "        [-0.1828]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.7846],\n",
      "        [-3.1074],\n",
      "        [-2.2029],\n",
      "        [-2.8539],\n",
      "        [-2.2303],\n",
      "        [-1.3859],\n",
      "        [-2.3475],\n",
      "        [-0.8304],\n",
      "        [-1.9239],\n",
      "        [-2.5190],\n",
      "        [-0.9444],\n",
      "        [-2.9536],\n",
      "        [-2.3707],\n",
      "        [-1.4242],\n",
      "        [-1.8178],\n",
      "        [-1.8720],\n",
      "        [-0.9926],\n",
      "        [-1.1143],\n",
      "        [-1.2063],\n",
      "        [-2.8130],\n",
      "        [-0.9279],\n",
      "        [-2.5624],\n",
      "        [-1.4155],\n",
      "        [-1.0044],\n",
      "        [-1.1255],\n",
      "        [-0.8840],\n",
      "        [-1.1321],\n",
      "        [-1.0339],\n",
      "        [-1.0240],\n",
      "        [-1.0735],\n",
      "        [-3.2359],\n",
      "        [-0.8875],\n",
      "        [-2.7982],\n",
      "        [-1.2461],\n",
      "        [-2.7090],\n",
      "        [-1.1547],\n",
      "        [-1.1439],\n",
      "        [-1.1777],\n",
      "        [-1.5478],\n",
      "        [-0.4903],\n",
      "        [-1.9299],\n",
      "        [-1.1503],\n",
      "        [-1.0735],\n",
      "        [ 0.0658],\n",
      "        [-1.8406],\n",
      "        [-0.8046],\n",
      "        [-1.0124],\n",
      "        [-1.5583],\n",
      "        [ 0.1095],\n",
      "        [ 0.0254]], device='cuda:0') tensor([[-0.4469],\n",
      "        [ 0.7854],\n",
      "        [-0.7491],\n",
      "        [ 0.1708],\n",
      "        [-0.0516],\n",
      "        [-0.0621],\n",
      "        [-0.0505],\n",
      "        [-0.1139],\n",
      "        [ 0.0874],\n",
      "        [ 0.0787],\n",
      "        [ 0.4034],\n",
      "        [ 0.3520],\n",
      "        [ 0.7082],\n",
      "        [ 0.0292],\n",
      "        [ 0.1002],\n",
      "        [ 0.1828],\n",
      "        [ 0.1163],\n",
      "        [-0.2454],\n",
      "        [ 0.1894],\n",
      "        [-0.1822],\n",
      "        [-0.6584],\n",
      "        [ 0.5519],\n",
      "        [ 0.2774],\n",
      "        [-0.0106],\n",
      "        [-0.0684],\n",
      "        [ 0.1661],\n",
      "        [-0.5607],\n",
      "        [-0.5939],\n",
      "        [-0.0917],\n",
      "        [-0.1844],\n",
      "        [-0.4717],\n",
      "        [ 0.2986],\n",
      "        [ 0.2875],\n",
      "        [-0.0562],\n",
      "        [-0.4045],\n",
      "        [-0.3965],\n",
      "        [-0.0683],\n",
      "        [-0.5334],\n",
      "        [-0.0303],\n",
      "        [-0.7457],\n",
      "        [ 0.2477],\n",
      "        [ 0.0857],\n",
      "        [-0.1809],\n",
      "        [-0.2599],\n",
      "        [ 0.1523],\n",
      "        [-0.0564],\n",
      "        [-0.2093],\n",
      "        [-0.0660],\n",
      "        [-0.2857],\n",
      "        [-0.2082]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.9079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.901349\n",
      "Validation set: Average loss: 3.049801, Accuracy: 0/26918 (0%)\n",
      "\n",
      "997\n",
      "Epoch: 997\n",
      "tensor([[-1.8860],\n",
      "        [-2.6710],\n",
      "        [-1.7121],\n",
      "        [-2.5024],\n",
      "        [-1.9643],\n",
      "        [-1.1582],\n",
      "        [-2.3350],\n",
      "        [-1.0656],\n",
      "        [-2.3276],\n",
      "        [-2.4912],\n",
      "        [-0.9064],\n",
      "        [-3.3545],\n",
      "        [-1.6792],\n",
      "        [-2.0281],\n",
      "        [-0.6655],\n",
      "        [-2.1578],\n",
      "        [-1.3833],\n",
      "        [-0.9433],\n",
      "        [-0.9423],\n",
      "        [-2.9447],\n",
      "        [-1.3532],\n",
      "        [-2.9689],\n",
      "        [-1.3138],\n",
      "        [-0.9361],\n",
      "        [-1.0955],\n",
      "        [-0.9342],\n",
      "        [-1.1632],\n",
      "        [-1.2738],\n",
      "        [-1.2296],\n",
      "        [-1.3115],\n",
      "        [-2.7933],\n",
      "        [-0.9110],\n",
      "        [-2.6879],\n",
      "        [-1.6605],\n",
      "        [-2.5251],\n",
      "        [-1.2338],\n",
      "        [-1.1911],\n",
      "        [-0.9463],\n",
      "        [-1.1390],\n",
      "        [-0.7682],\n",
      "        [-1.5559],\n",
      "        [-0.8407],\n",
      "        [-1.4361],\n",
      "        [-0.3004],\n",
      "        [-1.9268],\n",
      "        [-0.7736],\n",
      "        [-1.2724],\n",
      "        [-1.4564],\n",
      "        [-0.1770],\n",
      "        [ 0.1027]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.7846],\n",
      "        [-3.1074],\n",
      "        [-2.2029],\n",
      "        [-2.8539],\n",
      "        [-2.2303],\n",
      "        [-1.3859],\n",
      "        [-2.3475],\n",
      "        [-0.8304],\n",
      "        [-1.9239],\n",
      "        [-2.5190],\n",
      "        [-0.9444],\n",
      "        [-2.9536],\n",
      "        [-2.3707],\n",
      "        [-1.4242],\n",
      "        [-1.8178],\n",
      "        [-1.8720],\n",
      "        [-0.9926],\n",
      "        [-1.1143],\n",
      "        [-1.2063],\n",
      "        [-2.8130],\n",
      "        [-0.9279],\n",
      "        [-2.5624],\n",
      "        [-1.4155],\n",
      "        [-1.0044],\n",
      "        [-1.1255],\n",
      "        [-0.8840],\n",
      "        [-1.1321],\n",
      "        [-1.0339],\n",
      "        [-1.0240],\n",
      "        [-1.0735],\n",
      "        [-3.2359],\n",
      "        [-0.8875],\n",
      "        [-2.7982],\n",
      "        [-1.2461],\n",
      "        [-2.7090],\n",
      "        [-1.1547],\n",
      "        [-1.1439],\n",
      "        [-1.1777],\n",
      "        [-1.5478],\n",
      "        [-0.4903],\n",
      "        [-1.9299],\n",
      "        [-1.1503],\n",
      "        [-1.0735],\n",
      "        [ 0.0658],\n",
      "        [-1.8406],\n",
      "        [-0.8046],\n",
      "        [-1.0124],\n",
      "        [-1.5583],\n",
      "        [ 0.1095],\n",
      "        [ 0.0254]], device='cuda:0') tensor([[ 0.8987],\n",
      "        [ 0.4364],\n",
      "        [ 0.4908],\n",
      "        [ 0.3515],\n",
      "        [ 0.2659],\n",
      "        [ 0.2277],\n",
      "        [ 0.0124],\n",
      "        [-0.2352],\n",
      "        [-0.4037],\n",
      "        [ 0.0278],\n",
      "        [ 0.0379],\n",
      "        [-0.4009],\n",
      "        [ 0.6915],\n",
      "        [-0.6039],\n",
      "        [ 1.1524],\n",
      "        [-0.2859],\n",
      "        [-0.3907],\n",
      "        [ 0.1710],\n",
      "        [ 0.2640],\n",
      "        [-0.1317],\n",
      "        [-0.4253],\n",
      "        [-0.4065],\n",
      "        [ 0.1016],\n",
      "        [ 0.0683],\n",
      "        [ 0.0300],\n",
      "        [-0.0502],\n",
      "        [-0.0310],\n",
      "        [-0.2399],\n",
      "        [-0.2055],\n",
      "        [-0.2380],\n",
      "        [ 0.4426],\n",
      "        [-0.0235],\n",
      "        [ 0.1103],\n",
      "        [-0.4145],\n",
      "        [ 0.1839],\n",
      "        [-0.0791],\n",
      "        [-0.0472],\n",
      "        [ 0.2314],\n",
      "        [ 0.4088],\n",
      "        [-0.2779],\n",
      "        [ 0.3740],\n",
      "        [ 0.3096],\n",
      "        [-0.3627],\n",
      "        [-0.3661],\n",
      "        [-0.0862],\n",
      "        [ 0.0310],\n",
      "        [-0.2600],\n",
      "        [ 0.1020],\n",
      "        [-0.2865],\n",
      "        [ 0.0774]], device='cuda:0', grad_fn=<SubBackward0>) tensor(6.3061, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.975251\n",
      "Validation set: Average loss: 2.886901, Accuracy: 0/26918 (0%)\n",
      "\n",
      "998\n",
      "Epoch: 998\n",
      "tensor([[-3.1563],\n",
      "        [-2.6709],\n",
      "        [-2.3638],\n",
      "        [-2.9090],\n",
      "        [-2.2236],\n",
      "        [-1.3878],\n",
      "        [-2.1619],\n",
      "        [-0.7762],\n",
      "        [-2.4154],\n",
      "        [-1.9258],\n",
      "        [-1.0121],\n",
      "        [-2.7335],\n",
      "        [-2.8237],\n",
      "        [-1.6726],\n",
      "        [-1.7671],\n",
      "        [-2.1559],\n",
      "        [-1.5960],\n",
      "        [-1.1715],\n",
      "        [-0.7103],\n",
      "        [-3.0801],\n",
      "        [-0.9000],\n",
      "        [-2.0619],\n",
      "        [-1.2956],\n",
      "        [-0.7400],\n",
      "        [-1.1443],\n",
      "        [-0.6946],\n",
      "        [-1.0902],\n",
      "        [-1.2644],\n",
      "        [-0.7430],\n",
      "        [-1.3390],\n",
      "        [-2.8405],\n",
      "        [-0.6235],\n",
      "        [-2.7105],\n",
      "        [-1.1262],\n",
      "        [-2.6156],\n",
      "        [-1.3545],\n",
      "        [-1.0993],\n",
      "        [-1.0722],\n",
      "        [-1.1493],\n",
      "        [-0.4724],\n",
      "        [-2.4316],\n",
      "        [-1.3522],\n",
      "        [-1.6295],\n",
      "        [-0.1136],\n",
      "        [-1.5108],\n",
      "        [-0.7452],\n",
      "        [-1.2655],\n",
      "        [-1.1423],\n",
      "        [-0.1822],\n",
      "        [-0.3116]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.7846],\n",
      "        [-3.1074],\n",
      "        [-2.2029],\n",
      "        [-2.8539],\n",
      "        [-2.2303],\n",
      "        [-1.3859],\n",
      "        [-2.3475],\n",
      "        [-0.8304],\n",
      "        [-1.9239],\n",
      "        [-2.5190],\n",
      "        [-0.9444],\n",
      "        [-2.9536],\n",
      "        [-2.3707],\n",
      "        [-1.4242],\n",
      "        [-1.8178],\n",
      "        [-1.8720],\n",
      "        [-0.9926],\n",
      "        [-1.1143],\n",
      "        [-1.2063],\n",
      "        [-2.8130],\n",
      "        [-0.9279],\n",
      "        [-2.5624],\n",
      "        [-1.4155],\n",
      "        [-1.0044],\n",
      "        [-1.1255],\n",
      "        [-0.8840],\n",
      "        [-1.1321],\n",
      "        [-1.0339],\n",
      "        [-1.0240],\n",
      "        [-1.0735],\n",
      "        [-3.2359],\n",
      "        [-0.8875],\n",
      "        [-2.7982],\n",
      "        [-1.2461],\n",
      "        [-2.7090],\n",
      "        [-1.1547],\n",
      "        [-1.1439],\n",
      "        [-1.1777],\n",
      "        [-1.5478],\n",
      "        [-0.4903],\n",
      "        [-1.9299],\n",
      "        [-1.1503],\n",
      "        [-1.0735],\n",
      "        [ 0.0658],\n",
      "        [-1.8406],\n",
      "        [-0.8046],\n",
      "        [-1.0124],\n",
      "        [-1.5583],\n",
      "        [ 0.1095],\n",
      "        [ 0.0254]], device='cuda:0') tensor([[-0.3716],\n",
      "        [ 0.4365],\n",
      "        [-0.1609],\n",
      "        [-0.0551],\n",
      "        [ 0.0066],\n",
      "        [-0.0019],\n",
      "        [ 0.1856],\n",
      "        [ 0.0542],\n",
      "        [-0.4915],\n",
      "        [ 0.5932],\n",
      "        [-0.0677],\n",
      "        [ 0.2201],\n",
      "        [-0.4530],\n",
      "        [-0.2484],\n",
      "        [ 0.0507],\n",
      "        [-0.2839],\n",
      "        [-0.6034],\n",
      "        [-0.0572],\n",
      "        [ 0.4960],\n",
      "        [-0.2671],\n",
      "        [ 0.0279],\n",
      "        [ 0.5006],\n",
      "        [ 0.1199],\n",
      "        [ 0.2644],\n",
      "        [-0.0188],\n",
      "        [ 0.1894],\n",
      "        [ 0.0420],\n",
      "        [-0.2305],\n",
      "        [ 0.2811],\n",
      "        [-0.2655],\n",
      "        [ 0.3954],\n",
      "        [ 0.2640],\n",
      "        [ 0.0877],\n",
      "        [ 0.1198],\n",
      "        [ 0.0934],\n",
      "        [-0.1998],\n",
      "        [ 0.0446],\n",
      "        [ 0.1055],\n",
      "        [ 0.3984],\n",
      "        [ 0.0180],\n",
      "        [-0.5016],\n",
      "        [-0.2019],\n",
      "        [-0.5560],\n",
      "        [-0.1793],\n",
      "        [ 0.3298],\n",
      "        [ 0.0594],\n",
      "        [-0.2531],\n",
      "        [ 0.4160],\n",
      "        [-0.2917],\n",
      "        [-0.3370]], device='cuda:0', grad_fn=<SubBackward0>) tensor(4.3032, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.883278\n",
      "Validation set: Average loss: 2.911604, Accuracy: 0/26918 (0%)\n",
      "\n",
      "999\n",
      "Epoch: 999\n",
      "tensor([[-2.3263],\n",
      "        [-2.6174],\n",
      "        [-2.6302],\n",
      "        [-3.0553],\n",
      "        [-2.1175],\n",
      "        [-1.0797],\n",
      "        [-1.6775],\n",
      "        [-0.6092],\n",
      "        [-1.8796],\n",
      "        [-2.1054],\n",
      "        [-0.4715],\n",
      "        [-2.9932],\n",
      "        [-2.5263],\n",
      "        [-2.1696],\n",
      "        [-1.6603],\n",
      "        [-2.3818],\n",
      "        [-2.0530],\n",
      "        [-1.0326],\n",
      "        [-1.4163],\n",
      "        [-2.3953],\n",
      "        [-1.6181],\n",
      "        [-2.0193],\n",
      "        [-1.3387],\n",
      "        [-1.0056],\n",
      "        [-1.3126],\n",
      "        [-0.5527],\n",
      "        [-1.0457],\n",
      "        [-0.8887],\n",
      "        [-1.3498],\n",
      "        [-1.2067],\n",
      "        [-3.2046],\n",
      "        [-0.8709],\n",
      "        [-2.6063],\n",
      "        [-1.7037],\n",
      "        [-2.1997],\n",
      "        [-1.3598],\n",
      "        [-1.0919],\n",
      "        [-1.2610],\n",
      "        [-1.1923],\n",
      "        [-0.7239],\n",
      "        [-1.3722],\n",
      "        [-1.0184],\n",
      "        [-1.4268],\n",
      "        [-0.2036],\n",
      "        [-1.5899],\n",
      "        [-1.1601],\n",
      "        [-1.4793],\n",
      "        [-1.4654],\n",
      "        [-0.5493],\n",
      "        [ 0.2003]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.7846],\n",
      "        [-3.1074],\n",
      "        [-2.2029],\n",
      "        [-2.8539],\n",
      "        [-2.2303],\n",
      "        [-1.3859],\n",
      "        [-2.3475],\n",
      "        [-0.8304],\n",
      "        [-1.9239],\n",
      "        [-2.5190],\n",
      "        [-0.9444],\n",
      "        [-2.9536],\n",
      "        [-2.3707],\n",
      "        [-1.4242],\n",
      "        [-1.8178],\n",
      "        [-1.8720],\n",
      "        [-0.9926],\n",
      "        [-1.1143],\n",
      "        [-1.2063],\n",
      "        [-2.8130],\n",
      "        [-0.9279],\n",
      "        [-2.5624],\n",
      "        [-1.4155],\n",
      "        [-1.0044],\n",
      "        [-1.1255],\n",
      "        [-0.8840],\n",
      "        [-1.1321],\n",
      "        [-1.0339],\n",
      "        [-1.0240],\n",
      "        [-1.0735],\n",
      "        [-3.2359],\n",
      "        [-0.8875],\n",
      "        [-2.7982],\n",
      "        [-1.2461],\n",
      "        [-2.7090],\n",
      "        [-1.1547],\n",
      "        [-1.1439],\n",
      "        [-1.1777],\n",
      "        [-1.5478],\n",
      "        [-0.4903],\n",
      "        [-1.9299],\n",
      "        [-1.1503],\n",
      "        [-1.0735],\n",
      "        [ 0.0658],\n",
      "        [-1.8406],\n",
      "        [-0.8046],\n",
      "        [-1.0124],\n",
      "        [-1.5583],\n",
      "        [ 0.1095],\n",
      "        [ 0.0254]], device='cuda:0') tensor([[ 0.4584],\n",
      "        [ 0.4900],\n",
      "        [-0.4273],\n",
      "        [-0.2014],\n",
      "        [ 0.1128],\n",
      "        [ 0.3063],\n",
      "        [ 0.6700],\n",
      "        [ 0.2212],\n",
      "        [ 0.0443],\n",
      "        [ 0.4136],\n",
      "        [ 0.4729],\n",
      "        [-0.0396],\n",
      "        [-0.1556],\n",
      "        [-0.7454],\n",
      "        [ 0.1575],\n",
      "        [-0.5098],\n",
      "        [-1.0605],\n",
      "        [ 0.0817],\n",
      "        [-0.2100],\n",
      "        [ 0.4178],\n",
      "        [-0.6902],\n",
      "        [ 0.5431],\n",
      "        [ 0.0767],\n",
      "        [-0.0012],\n",
      "        [-0.1871],\n",
      "        [ 0.3313],\n",
      "        [ 0.0864],\n",
      "        [ 0.1452],\n",
      "        [-0.3258],\n",
      "        [-0.1332],\n",
      "        [ 0.0313],\n",
      "        [ 0.0166],\n",
      "        [ 0.1919],\n",
      "        [-0.4577],\n",
      "        [ 0.5092],\n",
      "        [-0.2051],\n",
      "        [ 0.0520],\n",
      "        [-0.0833],\n",
      "        [ 0.3554],\n",
      "        [-0.2336],\n",
      "        [ 0.5577],\n",
      "        [ 0.1319],\n",
      "        [-0.3533],\n",
      "        [-0.2694],\n",
      "        [ 0.2507],\n",
      "        [-0.3555],\n",
      "        [-0.4668],\n",
      "        [ 0.0929],\n",
      "        [-0.6588],\n",
      "        [ 0.1749]], device='cuda:0', grad_fn=<SubBackward0>) tensor(7.1107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.915912\n",
      "Validation set: Average loss: 3.210736, Accuracy: 0/26918 (0%)\n",
      "\n",
      "1000\n",
      "Epoch: 1000\n",
      "tensor([[-2.0898],\n",
      "        [-1.9821],\n",
      "        [-1.7221],\n",
      "        [-2.0060],\n",
      "        [-2.4055],\n",
      "        [-1.4570],\n",
      "        [-2.2045],\n",
      "        [-0.7463],\n",
      "        [-2.0765],\n",
      "        [-1.8149],\n",
      "        [-1.2486],\n",
      "        [-1.9917],\n",
      "        [-2.4334],\n",
      "        [-1.1474],\n",
      "        [-2.0268],\n",
      "        [-1.9644],\n",
      "        [-0.5663],\n",
      "        [-1.5374],\n",
      "        [-1.0717],\n",
      "        [-2.5083],\n",
      "        [-0.9072],\n",
      "        [-2.8830],\n",
      "        [-1.5195],\n",
      "        [-1.0417],\n",
      "        [-1.2208],\n",
      "        [-1.1284],\n",
      "        [-1.1771],\n",
      "        [-1.4116],\n",
      "        [-1.8062],\n",
      "        [-1.0421],\n",
      "        [-3.0706],\n",
      "        [-0.7365],\n",
      "        [-3.2879],\n",
      "        [-1.7020],\n",
      "        [-2.9547],\n",
      "        [-1.9820],\n",
      "        [-1.0782],\n",
      "        [-1.2983],\n",
      "        [-1.4010],\n",
      "        [-0.9388],\n",
      "        [-1.8752],\n",
      "        [-1.1272],\n",
      "        [-1.0319],\n",
      "        [-0.1595],\n",
      "        [-1.9177],\n",
      "        [-1.5008],\n",
      "        [-1.1610],\n",
      "        [-1.1486],\n",
      "        [-0.5248],\n",
      "        [ 0.1651]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.7846],\n",
      "        [-3.1074],\n",
      "        [-2.2029],\n",
      "        [-2.8539],\n",
      "        [-2.2303],\n",
      "        [-1.3859],\n",
      "        [-2.3475],\n",
      "        [-0.8304],\n",
      "        [-1.9239],\n",
      "        [-2.5190],\n",
      "        [-0.9444],\n",
      "        [-2.9536],\n",
      "        [-2.3707],\n",
      "        [-1.4242],\n",
      "        [-1.8178],\n",
      "        [-1.8720],\n",
      "        [-0.9926],\n",
      "        [-1.1143],\n",
      "        [-1.2063],\n",
      "        [-2.8130],\n",
      "        [-0.9279],\n",
      "        [-2.5624],\n",
      "        [-1.4155],\n",
      "        [-1.0044],\n",
      "        [-1.1255],\n",
      "        [-0.8840],\n",
      "        [-1.1321],\n",
      "        [-1.0339],\n",
      "        [-1.0240],\n",
      "        [-1.0735],\n",
      "        [-3.2359],\n",
      "        [-0.8875],\n",
      "        [-2.7982],\n",
      "        [-1.2461],\n",
      "        [-2.7090],\n",
      "        [-1.1547],\n",
      "        [-1.1439],\n",
      "        [-1.1777],\n",
      "        [-1.5478],\n",
      "        [-0.4903],\n",
      "        [-1.9299],\n",
      "        [-1.1503],\n",
      "        [-1.0735],\n",
      "        [ 0.0658],\n",
      "        [-1.8406],\n",
      "        [-0.8046],\n",
      "        [-1.0124],\n",
      "        [-1.5583],\n",
      "        [ 0.1095],\n",
      "        [ 0.0254]], device='cuda:0') tensor([[ 0.6948],\n",
      "        [ 1.1253],\n",
      "        [ 0.4809],\n",
      "        [ 0.8479],\n",
      "        [-0.1753],\n",
      "        [-0.0711],\n",
      "        [ 0.1429],\n",
      "        [ 0.0841],\n",
      "        [-0.1526],\n",
      "        [ 0.7042],\n",
      "        [-0.3042],\n",
      "        [ 0.9619],\n",
      "        [-0.0627],\n",
      "        [ 0.2768],\n",
      "        [-0.2089],\n",
      "        [-0.0924],\n",
      "        [ 0.4263],\n",
      "        [-0.4231],\n",
      "        [ 0.1346],\n",
      "        [ 0.3047],\n",
      "        [ 0.0207],\n",
      "        [-0.3206],\n",
      "        [-0.1040],\n",
      "        [-0.0373],\n",
      "        [-0.0953],\n",
      "        [-0.2444],\n",
      "        [-0.0449],\n",
      "        [-0.3777],\n",
      "        [-0.7821],\n",
      "        [ 0.0313],\n",
      "        [ 0.1653],\n",
      "        [ 0.1510],\n",
      "        [-0.4897],\n",
      "        [-0.4560],\n",
      "        [-0.2457],\n",
      "        [-0.8274],\n",
      "        [ 0.0657],\n",
      "        [-0.1206],\n",
      "        [ 0.1468],\n",
      "        [-0.4484],\n",
      "        [ 0.0548],\n",
      "        [ 0.0231],\n",
      "        [ 0.0415],\n",
      "        [-0.2252],\n",
      "        [-0.0771],\n",
      "        [-0.6963],\n",
      "        [-0.1485],\n",
      "        [ 0.4097],\n",
      "        [-0.6343],\n",
      "        [ 0.1397]], device='cuda:0', grad_fn=<SubBackward0>) tensor(8.4884, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.921963\n",
      "Validation set: Average loss: 3.072080, Accuracy: 0/26918 (0%)\n",
      "\n",
      "204.30766931772231\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 1000\n",
    "print('Training on', device)\n",
    "time_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "time_end = time.time()\n",
    "total_time = (time_end-time_start)/60 \n",
    "print(total_time)"
   ]
  },
  {
   "source": [
    "# Checking How Well the Model is Preforming "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truths_train = [] \n",
    "all_preds_train = [] \n",
    "for (data,target) in train_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_train.append(target.cpu().detach().numpy())\n",
    "    all_preds_train.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_train=len(all_truths_train)-1\n",
    "\n",
    "remainder_train=len(all_truths_train[incomplete_batch_id_train])\n",
    "\n",
    "total_values_train=(len(all_truths_train)*batch_size)-(batch_size-remainder_train)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_train_array=np.zeros(total_values_train)\n",
    "all_preds_train_array=np.zeros(total_values_train)\n",
    "k=0\n",
    "while k < total_values_train:\n",
    "    for i in range(len(all_truths_train)):\n",
    "        if i<incomplete_batch_id_train:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_train\n",
    "            for j in range(remainder_train):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# all_truths_train=all_truths_train_array\n",
    "# all_preds_train=all_preds_train_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-2.057188971976344\n[-1.9566252]\n-2.057188971976344\n-1.95662522315979\n"
     ]
    }
   ],
   "source": [
    "all_truths_test = [] \n",
    "all_preds_test = [] \n",
    "for (data,target) in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_test.append(target.cpu().detach().numpy())\n",
    "    all_preds_test.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_test=len(all_truths_test)-1\n",
    "\n",
    "remainder_test=len(all_truths_test[incomplete_batch_id_test])\n",
    "\n",
    "\n",
    "total_values_test=(len(all_truths_test)*batch_size)-(batch_size-remainder_test)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_test_array=np.zeros(total_values_test)\n",
    "all_preds_test_array=np.zeros(total_values_test)\n",
    "k=0\n",
    "while k < total_values_test:\n",
    "    for i in range(len(all_truths_test)):\n",
    "        if i<incomplete_batch_id_test:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_test\n",
    "            for j in range(remainder_test):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "print(all_truths_test[3][43])\n",
    "print(all_preds_test[3][43])\n",
    "print(all_truths_test_array[193])\n",
    "print(all_preds_test_array[193])\n",
    "# all_truths_test=all_truths_test_array\n",
    "# all_preds_test=all_preds_test_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train=r2_score(all_truths_train_array,all_preds_train_array)\n",
    "r2_test=r2_score(all_truths_test_array,all_preds_test_array)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.subplot(1,2,1) \n",
    "plt.title('Test Data Set')\n",
    "plt.scatter(all_truths_test_array, all_preds_test_array,color = 'b', alpha = 0.1*7/3)\n",
    "plt.xlabel('Test True Values')\n",
    "plt.ylabel('Test Predicted Value')\n",
    "plt.text(0,-5,'R$^2$='+ str(round(r2_test,4)))\n",
    "plt.plot([-5,1],[-5,1],'r--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Train Data Set')\n",
    "plt.scatter(all_truths_train_array, all_preds_train_array,color = 'k', alpha = 0.1)\n",
    "plt.xlabel('Train True Values')\n",
    "plt.ylabel('Train Predicted Value')\n",
    "plt.plot([-5,1],[-5,1],'r--') \n",
    "plt.text(0,-5.5,'R$^2$='+str(round(r2_train,3)))\n",
    "plt.show()\n",
    "plt.savefig('/home/juanp/Documents/SURP-2021/Plots/Model 3/30 Copies of Images.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/juanp/Documents/SURP-2021/Models/SFR_Model_30_Copies_Images')"
   ]
  },
  {
   "source": [
    "# Loss as a Function of Epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.plot(epoch_nums, np.log10(training_loss))\n",
    "# plt.plot(epoch_nums, np.log10(validation_loss))\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('Log of loss')\n",
    "# plt.legend(['training', 'validation'], loc='upper right')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Plotting Galaxy Images and their True SFR vs CNN Predicted SFR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(picture[0][0,0:,0:])\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(picture[1][0,0:,0:])\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(picture[2][0,0:,0:])\n",
    "#     plt.show()"
   ]
  }
 ]
}