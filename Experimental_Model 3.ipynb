{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 3: Predict Star Formation Variables (sSFR, SFR, M*, age) Based on Visual Morphology (galaxy image)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import decode_image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import marvin\n",
    "from marvin.tools.maps import Maps\n",
    "from marvin.tools.image import Image\n",
    "from marvin.utils.general.images import get_images_by_list\n",
    "from marvin import config\n",
    "from marvin.tools.cube import Cube\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image as image_PIL\n",
    "import time \n",
    "\n",
    "#set config attributes and turn on global downloads of Marvin data\n",
    "config.setRelease('DR15')\n",
    "config.mode = 'local'\n",
    "config.download = True\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.linear1 = torch.nn.Linear(outputSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "\n",
    "#Problem with image associated with manga id at galaxy_list[3548], mangaid- 1-135668\n",
    "galaxy_list=np.delete(galaxy_list,3548)\n",
    "\n",
    "galaxy_list=np.unique(galaxy_list)\n",
    "\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "galaxies=galaxies.sort_values(by=['plateifu']) #Sorting galaxies by plateifu to match ImageFolder Output \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n"
   ]
  },
  {
   "source": [
    "# Importing Images from their Downloaded Locations \n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_locations=[]\n",
    "# for i in range (len(galaxy_list)):\n",
    "#     image_locations.append(Image(galaxy_list[i]).filename)\n",
    "    \n",
    "# image_locations=np.array(image_locations,dtype=str)\n",
    "# np.savetxt('Image Directories',image_locations,fmt='%s')\n",
    "\n",
    "image_locations=np.loadtxt('Image Directories',dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n"
   ]
  },
  {
   "source": [
    "# Putting the Images into DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32301\nData loaders ready to read /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n"
     ]
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "\n",
    "# image=ImageFolder('/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/') #Picks up 3590 pictures, directory list has lenght 3637 however \n",
    "\n",
    "image_directory='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3'\n",
    "classes= sorted(os.listdir(image_directory))\n",
    "\n",
    "batch_size=50\n",
    "image_copies=9\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        #Rotates the image by some angle\n",
    "        # transforms.RandomRotation(0,360),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        # transform=transformation\n",
    "    )\n",
    "    \n",
    "    #This loop transforms the images and assigns them the correct label \n",
    "\n",
    "    full_dataset_v2 = []   \n",
    "    for i in range(len(full_dataset)): \n",
    "        for j in range(image_copies): #This loops makes it so we have 5 copies of each galaxy image with different orientations \n",
    "            temp=transformation(resize_image(full_dataset[i][0]))\n",
    "            full_dataset_v2.append((temp,log_SFR.iloc[i])) \n",
    "    \n",
    "    full_dataset=full_dataset_v2\n",
    "\n",
    "    print(len(full_dataset))\n",
    "\n",
    "\n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(image_directory)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset ImageFolder\n    Number of datapoints: 3589\n    Root location: /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n3637\n3589\n3589\n"
     ]
    }
   ],
   "source": [
    "print(ImageFolder(image_directory))\n",
    "print(len(image_locations))\n",
    "print(len(np.unique(image_locations)))\n",
    "print(len(np.unique(galaxy_list)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_dataset = torchvision.datasets.ImageFolder(\n",
    "#         root='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/7968',\n",
    "#         # transform=transformation\n",
    "#     ) \n",
    "\n",
    "# full_dataset_v2 = []   \n",
    "# for i in range(len(full_dataset)): \n",
    "#     full_dataset_v2.append((resize_image(full_dataset[i][0]) , i)) \n",
    "\n",
    "# print(full_dataset_v2)\n",
    "\n",
    "\n",
    "# print(full_dataset[0][1])\n",
    "# for i in range(len(full_dataset)): \n",
    "#         full_dataset[i] = (resize_image(full_dataset[i][0]) , 0) \n",
    "        # full_dataset[i][0] = resize_image(full_dataset[i][0])\n",
    "        # full_dataset[i][1] = correct_label(correct_manga_id) "
   ]
  },
  {
   "source": [
    "# Defining the Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (drop): Dropout2d(p=0.2, inplace=False)\n  (fc): Linear(in_features=24576, out_features=10, bias=True)\n  (fc1): Linear(in_features=10, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 128 x 128, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=10)\n",
    "\n",
    "        self.fc1= nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x+1\n",
    "        \n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # print(x)\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x=self.conv1(x)\n",
    "        \n",
    "        x=self.pool(x)\n",
    "       \n",
    "\n",
    "        x=F.relu(x)\n",
    "\n",
    "        \n",
    "      \n",
    " \n",
    "     \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x))) \n",
    "        \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return class probabilities via a log_softmax function\n",
    "        x=F.relu(x) \n",
    "        \n",
    "        x= self.fc1(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(SFR)).to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "# Creating Training Loop/Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target=target.float()\n",
    "        # print(target.shape)\n",
    "        target=target.reshape(-1,1)\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "    \n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        if batch_idx==0:\n",
    "            print(output,target,output-target,loss)\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Create Test Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            target=target.float()\n",
    "            target=target.reshape(-1,1)\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-1.8359],\n",
      "        [-1.8268],\n",
      "        [-1.1604]], device='cuda:0') tensor([[ 0.0539],\n",
      "        [-0.1501],\n",
      "        [-0.0234],\n",
      "        [-0.4876],\n",
      "        [ 0.7378],\n",
      "        [ 0.1706],\n",
      "        [ 0.3126],\n",
      "        [ 0.5212],\n",
      "        [ 0.1624],\n",
      "        [ 0.1997],\n",
      "        [-0.2910],\n",
      "        [ 0.1014],\n",
      "        [ 0.5699],\n",
      "        [-0.2947],\n",
      "        [-0.3234],\n",
      "        [-0.0096],\n",
      "        [ 0.1254],\n",
      "        [ 0.2286],\n",
      "        [-0.5130],\n",
      "        [ 0.3162],\n",
      "        [-0.3801],\n",
      "        [-0.0923],\n",
      "        [-0.5194],\n",
      "        [ 0.8948],\n",
      "        [-0.0786],\n",
      "        [ 0.4330],\n",
      "        [-0.5777],\n",
      "        [ 0.0010],\n",
      "        [-0.0582],\n",
      "        [-0.1651],\n",
      "        [-0.2103],\n",
      "        [-0.0162],\n",
      "        [-0.7025],\n",
      "        [-0.0805],\n",
      "        [-0.2364],\n",
      "        [-0.2338],\n",
      "        [-0.3248],\n",
      "        [-0.2123],\n",
      "        [-0.1003],\n",
      "        [ 0.2379],\n",
      "        [ 0.0327],\n",
      "        [-0.2999],\n",
      "        [ 0.6747],\n",
      "        [ 0.4048],\n",
      "        [-0.6749],\n",
      "        [-0.3985],\n",
      "        [-0.1758],\n",
      "        [ 0.4662],\n",
      "        [ 0.5286],\n",
      "        [ 0.0232]], device='cuda:0', grad_fn=<SubBackward0>) tensor(6.8216, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.361994\n",
      "Validation set: Average loss: 9.518700, Accuracy: 0/9691 (0%)\n",
      "\n",
      "995\n",
      "Epoch: 995\n",
      "tensor([[-1.3617],\n",
      "        [-1.0588],\n",
      "        [-3.6430],\n",
      "        [-1.3908],\n",
      "        [-3.4123],\n",
      "        [-1.0800],\n",
      "        [-1.4791],\n",
      "        [-1.6173],\n",
      "        [-1.3338],\n",
      "        [-1.5037],\n",
      "        [-1.6085],\n",
      "        [-0.6049],\n",
      "        [-1.8496],\n",
      "        [-3.1153],\n",
      "        [ 0.0469],\n",
      "        [-0.6982],\n",
      "        [-0.8857],\n",
      "        [-1.8910],\n",
      "        [-0.5052],\n",
      "        [-0.8708],\n",
      "        [-1.8656],\n",
      "        [-1.1623],\n",
      "        [-1.0474],\n",
      "        [-3.2062],\n",
      "        [-0.3988],\n",
      "        [-1.7394],\n",
      "        [-1.2548],\n",
      "        [-0.3306],\n",
      "        [-0.7508],\n",
      "        [-1.9721],\n",
      "        [-0.2172],\n",
      "        [-1.9111],\n",
      "        [-3.6236],\n",
      "        [-1.2564],\n",
      "        [-0.7175],\n",
      "        [-1.6189],\n",
      "        [-0.6300],\n",
      "        [-0.6409],\n",
      "        [-0.6554],\n",
      "        [-1.5619],\n",
      "        [-0.7700],\n",
      "        [-0.7060],\n",
      "        [-1.6069],\n",
      "        [-0.7980],\n",
      "        [-3.0255],\n",
      "        [-0.8753],\n",
      "        [-1.3906],\n",
      "        [-2.1147],\n",
      "        [-2.4082],\n",
      "        [-1.8097]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.3084],\n",
      "        [-0.8556],\n",
      "        [-3.5055],\n",
      "        [ 0.0132],\n",
      "        [-3.4981],\n",
      "        [-1.3361],\n",
      "        [-1.7756],\n",
      "        [-1.7811],\n",
      "        [-1.4087],\n",
      "        [-1.9343],\n",
      "        [-1.2012],\n",
      "        [-0.2754],\n",
      "        [-2.1261],\n",
      "        [-2.9673],\n",
      "        [ 0.1764],\n",
      "        [-0.8383],\n",
      "        [-1.0749],\n",
      "        [-1.7956],\n",
      "        [-0.1893],\n",
      "        [-0.8067],\n",
      "        [-2.1149],\n",
      "        [-0.5300],\n",
      "        [-0.7723],\n",
      "        [-3.1130],\n",
      "        [-0.6397],\n",
      "        [-1.8272],\n",
      "        [-1.7822],\n",
      "        [-0.4559],\n",
      "        [-0.7187],\n",
      "        [-2.0522],\n",
      "        [-0.7304],\n",
      "        [-2.0840],\n",
      "        [-2.7713],\n",
      "        [-1.0382],\n",
      "        [-0.5858],\n",
      "        [-1.7614],\n",
      "        [ 0.0885],\n",
      "        [-0.6206],\n",
      "        [-0.6092],\n",
      "        [-2.1231],\n",
      "        [-0.9659],\n",
      "        [-0.4335],\n",
      "        [-1.6519],\n",
      "        [-1.3654],\n",
      "        [-2.2389],\n",
      "        [-0.3387],\n",
      "        [-1.6701],\n",
      "        [-1.8359],\n",
      "        [-1.8268],\n",
      "        [-1.1604]], device='cuda:0') tensor([[-0.0533],\n",
      "        [-0.2032],\n",
      "        [-0.1376],\n",
      "        [-1.4040],\n",
      "        [ 0.0858],\n",
      "        [ 0.2560],\n",
      "        [ 0.2965],\n",
      "        [ 0.1638],\n",
      "        [ 0.0750],\n",
      "        [ 0.4306],\n",
      "        [-0.4073],\n",
      "        [-0.3295],\n",
      "        [ 0.2765],\n",
      "        [-0.1479],\n",
      "        [-0.1294],\n",
      "        [ 0.1401],\n",
      "        [ 0.1891],\n",
      "        [-0.0954],\n",
      "        [-0.3158],\n",
      "        [-0.0641],\n",
      "        [ 0.2492],\n",
      "        [-0.6322],\n",
      "        [-0.2751],\n",
      "        [-0.0932],\n",
      "        [ 0.2409],\n",
      "        [ 0.0878],\n",
      "        [ 0.5274],\n",
      "        [ 0.1253],\n",
      "        [-0.0322],\n",
      "        [ 0.0802],\n",
      "        [ 0.5132],\n",
      "        [ 0.1729],\n",
      "        [-0.8524],\n",
      "        [-0.2182],\n",
      "        [-0.1317],\n",
      "        [ 0.1425],\n",
      "        [-0.7185],\n",
      "        [-0.0203],\n",
      "        [-0.0461],\n",
      "        [ 0.5613],\n",
      "        [ 0.1960],\n",
      "        [-0.2725],\n",
      "        [ 0.0450],\n",
      "        [ 0.5674],\n",
      "        [-0.7866],\n",
      "        [-0.5366],\n",
      "        [ 0.2795],\n",
      "        [-0.2789],\n",
      "        [-0.5814],\n",
      "        [-0.6493]], device='cuda:0', grad_fn=<SubBackward0>) tensor(8.0816, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.216561\n",
      "Validation set: Average loss: 9.632289, Accuracy: 0/9691 (0%)\n",
      "\n",
      "996\n",
      "Epoch: 996\n",
      "tensor([[-1.9586],\n",
      "        [-0.9776],\n",
      "        [-3.2285],\n",
      "        [-0.6050],\n",
      "        [-2.8300],\n",
      "        [-1.2919],\n",
      "        [-1.4663],\n",
      "        [-1.5283],\n",
      "        [-0.9857],\n",
      "        [-2.2245],\n",
      "        [-0.9545],\n",
      "        [-0.3908],\n",
      "        [-1.8327],\n",
      "        [-2.2631],\n",
      "        [-0.2868],\n",
      "        [-0.9286],\n",
      "        [-0.9398],\n",
      "        [-1.9452],\n",
      "        [-0.5690],\n",
      "        [-0.7640],\n",
      "        [-2.2885],\n",
      "        [-0.6606],\n",
      "        [-0.9092],\n",
      "        [-3.1013],\n",
      "        [-0.1606],\n",
      "        [-1.6928],\n",
      "        [-1.6762],\n",
      "        [-0.1976],\n",
      "        [-0.6908],\n",
      "        [-2.2586],\n",
      "        [-0.8651],\n",
      "        [-2.3310],\n",
      "        [-2.2846],\n",
      "        [-1.0171],\n",
      "        [-0.7122],\n",
      "        [-1.8770],\n",
      "        [-0.0870],\n",
      "        [-0.8138],\n",
      "        [-0.8311],\n",
      "        [-1.7037],\n",
      "        [-0.6846],\n",
      "        [-0.5021],\n",
      "        [-1.8284],\n",
      "        [-1.2877],\n",
      "        [-1.7990],\n",
      "        [-0.9477],\n",
      "        [-1.1330],\n",
      "        [-1.5864],\n",
      "        [-1.7735],\n",
      "        [-0.8142]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.3084],\n",
      "        [-0.8556],\n",
      "        [-3.5055],\n",
      "        [ 0.0132],\n",
      "        [-3.4981],\n",
      "        [-1.3361],\n",
      "        [-1.7756],\n",
      "        [-1.7811],\n",
      "        [-1.4087],\n",
      "        [-1.9343],\n",
      "        [-1.2012],\n",
      "        [-0.2754],\n",
      "        [-2.1261],\n",
      "        [-2.9673],\n",
      "        [ 0.1764],\n",
      "        [-0.8383],\n",
      "        [-1.0749],\n",
      "        [-1.7956],\n",
      "        [-0.1893],\n",
      "        [-0.8067],\n",
      "        [-2.1149],\n",
      "        [-0.5300],\n",
      "        [-0.7723],\n",
      "        [-3.1130],\n",
      "        [-0.6397],\n",
      "        [-1.8272],\n",
      "        [-1.7822],\n",
      "        [-0.4559],\n",
      "        [-0.7187],\n",
      "        [-2.0522],\n",
      "        [-0.7304],\n",
      "        [-2.0840],\n",
      "        [-2.7713],\n",
      "        [-1.0382],\n",
      "        [-0.5858],\n",
      "        [-1.7614],\n",
      "        [ 0.0885],\n",
      "        [-0.6206],\n",
      "        [-0.6092],\n",
      "        [-2.1231],\n",
      "        [-0.9659],\n",
      "        [-0.4335],\n",
      "        [-1.6519],\n",
      "        [-1.3654],\n",
      "        [-2.2389],\n",
      "        [-0.3387],\n",
      "        [-1.6701],\n",
      "        [-1.8359],\n",
      "        [-1.8268],\n",
      "        [-1.1604]], device='cuda:0') tensor([[-0.6502],\n",
      "        [-0.1220],\n",
      "        [ 0.2770],\n",
      "        [-0.6182],\n",
      "        [ 0.6681],\n",
      "        [ 0.0441],\n",
      "        [ 0.3093],\n",
      "        [ 0.2529],\n",
      "        [ 0.4230],\n",
      "        [-0.2902],\n",
      "        [ 0.2467],\n",
      "        [-0.1153],\n",
      "        [ 0.2934],\n",
      "        [ 0.7043],\n",
      "        [-0.4632],\n",
      "        [-0.0903],\n",
      "        [ 0.1351],\n",
      "        [-0.1496],\n",
      "        [-0.3797],\n",
      "        [ 0.0427],\n",
      "        [-0.1736],\n",
      "        [-0.1305],\n",
      "        [-0.1369],\n",
      "        [ 0.0117],\n",
      "        [ 0.4791],\n",
      "        [ 0.1344],\n",
      "        [ 0.1060],\n",
      "        [ 0.2583],\n",
      "        [ 0.0279],\n",
      "        [-0.2064],\n",
      "        [-0.1347],\n",
      "        [-0.2469],\n",
      "        [ 0.4867],\n",
      "        [ 0.0211],\n",
      "        [-0.1264],\n",
      "        [-0.1156],\n",
      "        [-0.1755],\n",
      "        [-0.1932],\n",
      "        [-0.2218],\n",
      "        [ 0.4194],\n",
      "        [ 0.2813],\n",
      "        [-0.0686],\n",
      "        [-0.1765],\n",
      "        [ 0.0777],\n",
      "        [ 0.4400],\n",
      "        [-0.6091],\n",
      "        [ 0.5371],\n",
      "        [ 0.2495],\n",
      "        [ 0.0533],\n",
      "        [ 0.3462]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.0661, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.264258\n",
      "Validation set: Average loss: 9.620951, Accuracy: 0/9691 (0%)\n",
      "\n",
      "997\n",
      "Epoch: 997\n",
      "tensor([[-1.2324],\n",
      "        [-1.1085],\n",
      "        [-4.1971],\n",
      "        [-0.2920],\n",
      "        [-3.4217],\n",
      "        [-1.3494],\n",
      "        [-1.2867],\n",
      "        [-1.2937],\n",
      "        [-1.1545],\n",
      "        [-1.1274],\n",
      "        [-1.2187],\n",
      "        [-0.2189],\n",
      "        [-1.5952],\n",
      "        [-3.9369],\n",
      "        [-0.6319],\n",
      "        [-0.9967],\n",
      "        [-0.9244],\n",
      "        [-1.6282],\n",
      "        [-0.3387],\n",
      "        [-0.9996],\n",
      "        [-2.0361],\n",
      "        [-1.5781],\n",
      "        [-0.7884],\n",
      "        [-3.7842],\n",
      "        [-0.9371],\n",
      "        [-2.1821],\n",
      "        [-1.9084],\n",
      "        [-0.6327],\n",
      "        [-0.5733],\n",
      "        [-1.5990],\n",
      "        [-0.9089],\n",
      "        [-2.0894],\n",
      "        [-3.2329],\n",
      "        [-1.1638],\n",
      "        [-0.2499],\n",
      "        [-1.5044],\n",
      "        [-0.6521],\n",
      "        [-0.7440],\n",
      "        [-0.9041],\n",
      "        [-1.7066],\n",
      "        [-1.9836],\n",
      "        [-0.5824],\n",
      "        [-1.4756],\n",
      "        [-1.2743],\n",
      "        [-1.8863],\n",
      "        [-0.7396],\n",
      "        [-1.3495],\n",
      "        [-1.5176],\n",
      "        [-1.5870],\n",
      "        [-1.1353]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.3084],\n",
      "        [-0.8556],\n",
      "        [-3.5055],\n",
      "        [ 0.0132],\n",
      "        [-3.4981],\n",
      "        [-1.3361],\n",
      "        [-1.7756],\n",
      "        [-1.7811],\n",
      "        [-1.4087],\n",
      "        [-1.9343],\n",
      "        [-1.2012],\n",
      "        [-0.2754],\n",
      "        [-2.1261],\n",
      "        [-2.9673],\n",
      "        [ 0.1764],\n",
      "        [-0.8383],\n",
      "        [-1.0749],\n",
      "        [-1.7956],\n",
      "        [-0.1893],\n",
      "        [-0.8067],\n",
      "        [-2.1149],\n",
      "        [-0.5300],\n",
      "        [-0.7723],\n",
      "        [-3.1130],\n",
      "        [-0.6397],\n",
      "        [-1.8272],\n",
      "        [-1.7822],\n",
      "        [-0.4559],\n",
      "        [-0.7187],\n",
      "        [-2.0522],\n",
      "        [-0.7304],\n",
      "        [-2.0840],\n",
      "        [-2.7713],\n",
      "        [-1.0382],\n",
      "        [-0.5858],\n",
      "        [-1.7614],\n",
      "        [ 0.0885],\n",
      "        [-0.6206],\n",
      "        [-0.6092],\n",
      "        [-2.1231],\n",
      "        [-0.9659],\n",
      "        [-0.4335],\n",
      "        [-1.6519],\n",
      "        [-1.3654],\n",
      "        [-2.2389],\n",
      "        [-0.3387],\n",
      "        [-1.6701],\n",
      "        [-1.8359],\n",
      "        [-1.8268],\n",
      "        [-1.1604]], device='cuda:0') tensor([[ 0.0760],\n",
      "        [-0.2528],\n",
      "        [-0.6916],\n",
      "        [-0.3051],\n",
      "        [ 0.0763],\n",
      "        [-0.0133],\n",
      "        [ 0.4889],\n",
      "        [ 0.4875],\n",
      "        [ 0.2542],\n",
      "        [ 0.8069],\n",
      "        [-0.0175],\n",
      "        [ 0.0566],\n",
      "        [ 0.5309],\n",
      "        [-0.9696],\n",
      "        [-0.8083],\n",
      "        [-0.1583],\n",
      "        [ 0.1505],\n",
      "        [ 0.1674],\n",
      "        [-0.1494],\n",
      "        [-0.1928],\n",
      "        [ 0.0787],\n",
      "        [-1.0480],\n",
      "        [-0.0161],\n",
      "        [-0.6712],\n",
      "        [-0.2974],\n",
      "        [-0.3549],\n",
      "        [-0.1262],\n",
      "        [-0.1768],\n",
      "        [ 0.1454],\n",
      "        [ 0.4532],\n",
      "        [-0.1785],\n",
      "        [-0.0053],\n",
      "        [-0.4616],\n",
      "        [-0.1256],\n",
      "        [ 0.3359],\n",
      "        [ 0.2570],\n",
      "        [-0.7406],\n",
      "        [-0.1235],\n",
      "        [-0.2948],\n",
      "        [ 0.4166],\n",
      "        [-1.0177],\n",
      "        [-0.1489],\n",
      "        [ 0.1763],\n",
      "        [ 0.0911],\n",
      "        [ 0.3527],\n",
      "        [-0.4009],\n",
      "        [ 0.3206],\n",
      "        [ 0.3182],\n",
      "        [ 0.2399],\n",
      "        [ 0.0251]], device='cuda:0', grad_fn=<SubBackward0>) tensor(8.8055, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.223300\n",
      "Validation set: Average loss: 9.786593, Accuracy: 0/9691 (0%)\n",
      "\n",
      "998\n",
      "Epoch: 998\n",
      "tensor([[-1.7560],\n",
      "        [-0.8867],\n",
      "        [-3.6670],\n",
      "        [-0.6668],\n",
      "        [-2.9332],\n",
      "        [-1.3509],\n",
      "        [-1.8007],\n",
      "        [-1.6363],\n",
      "        [-1.3101],\n",
      "        [-2.2820],\n",
      "        [-0.9282],\n",
      "        [-0.0371],\n",
      "        [-1.8788],\n",
      "        [-2.5331],\n",
      "        [-0.5218],\n",
      "        [-1.4993],\n",
      "        [-1.0181],\n",
      "        [-1.5750],\n",
      "        [-0.4621],\n",
      "        [-0.4715],\n",
      "        [-2.1990],\n",
      "        [-0.5530],\n",
      "        [-0.8393],\n",
      "        [-3.6211],\n",
      "        [-0.4568],\n",
      "        [-1.7375],\n",
      "        [-1.6898],\n",
      "        [-0.6592],\n",
      "        [-1.0539],\n",
      "        [-2.0749],\n",
      "        [-0.7829],\n",
      "        [-2.1007],\n",
      "        [-3.1257],\n",
      "        [-1.3082],\n",
      "        [-0.6451],\n",
      "        [-1.5696],\n",
      "        [-0.4332],\n",
      "        [-0.7507],\n",
      "        [-1.1168],\n",
      "        [-2.0991],\n",
      "        [-1.5446],\n",
      "        [-0.6546],\n",
      "        [-1.3815],\n",
      "        [-1.7236],\n",
      "        [-2.2692],\n",
      "        [-0.5830],\n",
      "        [-1.3350],\n",
      "        [-1.9995],\n",
      "        [-1.7243],\n",
      "        [-1.3552]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.3084],\n",
      "        [-0.8556],\n",
      "        [-3.5055],\n",
      "        [ 0.0132],\n",
      "        [-3.4981],\n",
      "        [-1.3361],\n",
      "        [-1.7756],\n",
      "        [-1.7811],\n",
      "        [-1.4087],\n",
      "        [-1.9343],\n",
      "        [-1.2012],\n",
      "        [-0.2754],\n",
      "        [-2.1261],\n",
      "        [-2.9673],\n",
      "        [ 0.1764],\n",
      "        [-0.8383],\n",
      "        [-1.0749],\n",
      "        [-1.7956],\n",
      "        [-0.1893],\n",
      "        [-0.8067],\n",
      "        [-2.1149],\n",
      "        [-0.5300],\n",
      "        [-0.7723],\n",
      "        [-3.1130],\n",
      "        [-0.6397],\n",
      "        [-1.8272],\n",
      "        [-1.7822],\n",
      "        [-0.4559],\n",
      "        [-0.7187],\n",
      "        [-2.0522],\n",
      "        [-0.7304],\n",
      "        [-2.0840],\n",
      "        [-2.7713],\n",
      "        [-1.0382],\n",
      "        [-0.5858],\n",
      "        [-1.7614],\n",
      "        [ 0.0885],\n",
      "        [-0.6206],\n",
      "        [-0.6092],\n",
      "        [-2.1231],\n",
      "        [-0.9659],\n",
      "        [-0.4335],\n",
      "        [-1.6519],\n",
      "        [-1.3654],\n",
      "        [-2.2389],\n",
      "        [-0.3387],\n",
      "        [-1.6701],\n",
      "        [-1.8359],\n",
      "        [-1.8268],\n",
      "        [-1.1604]], device='cuda:0') tensor([[-0.4476],\n",
      "        [-0.0311],\n",
      "        [-0.1615],\n",
      "        [-0.6800],\n",
      "        [ 0.5649],\n",
      "        [-0.0148],\n",
      "        [-0.0252],\n",
      "        [ 0.1448],\n",
      "        [ 0.0986],\n",
      "        [-0.3477],\n",
      "        [ 0.2730],\n",
      "        [ 0.2384],\n",
      "        [ 0.2473],\n",
      "        [ 0.4343],\n",
      "        [-0.6981],\n",
      "        [-0.6610],\n",
      "        [ 0.0568],\n",
      "        [ 0.2206],\n",
      "        [-0.2727],\n",
      "        [ 0.3352],\n",
      "        [-0.0841],\n",
      "        [-0.0229],\n",
      "        [-0.0670],\n",
      "        [-0.5081],\n",
      "        [ 0.1828],\n",
      "        [ 0.0897],\n",
      "        [ 0.0924],\n",
      "        [-0.2034],\n",
      "        [-0.3352],\n",
      "        [-0.0226],\n",
      "        [-0.0525],\n",
      "        [-0.0167],\n",
      "        [-0.3544],\n",
      "        [-0.2700],\n",
      "        [-0.0592],\n",
      "        [ 0.1918],\n",
      "        [-0.5217],\n",
      "        [-0.1301],\n",
      "        [-0.5076],\n",
      "        [ 0.0240],\n",
      "        [-0.5787],\n",
      "        [-0.2211],\n",
      "        [ 0.2704],\n",
      "        [-0.3582],\n",
      "        [-0.0302],\n",
      "        [-0.2443],\n",
      "        [ 0.3351],\n",
      "        [-0.1637],\n",
      "        [ 0.1026],\n",
      "        [-0.1948]], device='cuda:0', grad_fn=<SubBackward0>) tensor(4.8024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.280033\n",
      "Validation set: Average loss: 9.483249, Accuracy: 0/9691 (0%)\n",
      "\n",
      "999\n",
      "Epoch: 999\n",
      "tensor([[-1.5242],\n",
      "        [-1.0036],\n",
      "        [-3.1021],\n",
      "        [-1.0736],\n",
      "        [-3.3605],\n",
      "        [-1.4676],\n",
      "        [-1.8362],\n",
      "        [-1.9520],\n",
      "        [-1.4535],\n",
      "        [-1.9648],\n",
      "        [-1.2452],\n",
      "        [-0.2823],\n",
      "        [-2.0548],\n",
      "        [-3.3315],\n",
      "        [-0.0135],\n",
      "        [-1.0857],\n",
      "        [-1.0513],\n",
      "        [-1.5444],\n",
      "        [-0.8326],\n",
      "        [-1.1599],\n",
      "        [-1.9020],\n",
      "        [-1.1143],\n",
      "        [-1.1764],\n",
      "        [-2.3626],\n",
      "        [-0.8705],\n",
      "        [-1.5970],\n",
      "        [-1.6301],\n",
      "        [-0.6506],\n",
      "        [-0.7986],\n",
      "        [-2.1850],\n",
      "        [-0.6236],\n",
      "        [-1.9253],\n",
      "        [-2.3592],\n",
      "        [-1.1448],\n",
      "        [-0.5339],\n",
      "        [-1.5624],\n",
      "        [ 0.4125],\n",
      "        [-0.5840],\n",
      "        [-0.7885],\n",
      "        [-1.7977],\n",
      "        [-1.3875],\n",
      "        [-0.8466],\n",
      "        [-1.5507],\n",
      "        [-1.4263],\n",
      "        [-1.4044],\n",
      "        [-0.4380],\n",
      "        [-1.1229],\n",
      "        [-2.1047],\n",
      "        [-2.2854],\n",
      "        [-0.9514]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.3084],\n",
      "        [-0.8556],\n",
      "        [-3.5055],\n",
      "        [ 0.0132],\n",
      "        [-3.4981],\n",
      "        [-1.3361],\n",
      "        [-1.7756],\n",
      "        [-1.7811],\n",
      "        [-1.4087],\n",
      "        [-1.9343],\n",
      "        [-1.2012],\n",
      "        [-0.2754],\n",
      "        [-2.1261],\n",
      "        [-2.9673],\n",
      "        [ 0.1764],\n",
      "        [-0.8383],\n",
      "        [-1.0749],\n",
      "        [-1.7956],\n",
      "        [-0.1893],\n",
      "        [-0.8067],\n",
      "        [-2.1149],\n",
      "        [-0.5300],\n",
      "        [-0.7723],\n",
      "        [-3.1130],\n",
      "        [-0.6397],\n",
      "        [-1.8272],\n",
      "        [-1.7822],\n",
      "        [-0.4559],\n",
      "        [-0.7187],\n",
      "        [-2.0522],\n",
      "        [-0.7304],\n",
      "        [-2.0840],\n",
      "        [-2.7713],\n",
      "        [-1.0382],\n",
      "        [-0.5858],\n",
      "        [-1.7614],\n",
      "        [ 0.0885],\n",
      "        [-0.6206],\n",
      "        [-0.6092],\n",
      "        [-2.1231],\n",
      "        [-0.9659],\n",
      "        [-0.4335],\n",
      "        [-1.6519],\n",
      "        [-1.3654],\n",
      "        [-2.2389],\n",
      "        [-0.3387],\n",
      "        [-1.6701],\n",
      "        [-1.8359],\n",
      "        [-1.8268],\n",
      "        [-1.1604]], device='cuda:0') tensor([[-0.2158],\n",
      "        [-0.1480],\n",
      "        [ 0.4033],\n",
      "        [-1.0867],\n",
      "        [ 0.1375],\n",
      "        [-0.1315],\n",
      "        [-0.0606],\n",
      "        [-0.1708],\n",
      "        [-0.0447],\n",
      "        [-0.0305],\n",
      "        [-0.0440],\n",
      "        [-0.0069],\n",
      "        [ 0.0713],\n",
      "        [-0.3641],\n",
      "        [-0.1898],\n",
      "        [-0.2474],\n",
      "        [ 0.0236],\n",
      "        [ 0.2512],\n",
      "        [-0.6432],\n",
      "        [-0.3531],\n",
      "        [ 0.2128],\n",
      "        [-0.5842],\n",
      "        [-0.4041],\n",
      "        [ 0.7504],\n",
      "        [-0.2308],\n",
      "        [ 0.2302],\n",
      "        [ 0.1521],\n",
      "        [-0.1947],\n",
      "        [-0.0799],\n",
      "        [-0.1327],\n",
      "        [ 0.1068],\n",
      "        [ 0.1587],\n",
      "        [ 0.4121],\n",
      "        [-0.1066],\n",
      "        [ 0.0520],\n",
      "        [ 0.1990],\n",
      "        [ 0.3240],\n",
      "        [ 0.0366],\n",
      "        [-0.1792],\n",
      "        [ 0.3254],\n",
      "        [-0.4216],\n",
      "        [-0.4131],\n",
      "        [ 0.1012],\n",
      "        [-0.0609],\n",
      "        [ 0.8346],\n",
      "        [-0.0994],\n",
      "        [ 0.5473],\n",
      "        [-0.2689],\n",
      "        [-0.4586],\n",
      "        [ 0.2090]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.8262, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.180141\n",
      "Validation set: Average loss: 9.211880, Accuracy: 0/9691 (0%)\n",
      "\n",
      "1000\n",
      "Epoch: 1000\n",
      "tensor([[-1.1784],\n",
      "        [-0.8690],\n",
      "        [-3.8252],\n",
      "        [-0.7904],\n",
      "        [-3.3380],\n",
      "        [-1.4797],\n",
      "        [-1.6873],\n",
      "        [-1.8191],\n",
      "        [-1.2909],\n",
      "        [-1.6339],\n",
      "        [-1.2789],\n",
      "        [-0.6415],\n",
      "        [-1.8996],\n",
      "        [-2.5916],\n",
      "        [ 0.6415],\n",
      "        [-1.5189],\n",
      "        [-1.3198],\n",
      "        [-1.6135],\n",
      "        [-0.7470],\n",
      "        [-1.4501],\n",
      "        [-2.4226],\n",
      "        [-0.9357],\n",
      "        [-0.9383],\n",
      "        [-3.2551],\n",
      "        [-0.5772],\n",
      "        [-1.7289],\n",
      "        [-1.5283],\n",
      "        [ 0.1554],\n",
      "        [-0.9914],\n",
      "        [-1.9435],\n",
      "        [-0.9179],\n",
      "        [-1.7485],\n",
      "        [-3.0102],\n",
      "        [-1.1303],\n",
      "        [-0.3075],\n",
      "        [-2.2071],\n",
      "        [-0.1664],\n",
      "        [-0.5487],\n",
      "        [-0.4548],\n",
      "        [-1.6424],\n",
      "        [-0.7789],\n",
      "        [-0.9245],\n",
      "        [-1.6118],\n",
      "        [-2.0287],\n",
      "        [-2.2187],\n",
      "        [-0.6078],\n",
      "        [-1.9282],\n",
      "        [-1.3075],\n",
      "        [-1.4225],\n",
      "        [-1.0558]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.3084],\n",
      "        [-0.8556],\n",
      "        [-3.5055],\n",
      "        [ 0.0132],\n",
      "        [-3.4981],\n",
      "        [-1.3361],\n",
      "        [-1.7756],\n",
      "        [-1.7811],\n",
      "        [-1.4087],\n",
      "        [-1.9343],\n",
      "        [-1.2012],\n",
      "        [-0.2754],\n",
      "        [-2.1261],\n",
      "        [-2.9673],\n",
      "        [ 0.1764],\n",
      "        [-0.8383],\n",
      "        [-1.0749],\n",
      "        [-1.7956],\n",
      "        [-0.1893],\n",
      "        [-0.8067],\n",
      "        [-2.1149],\n",
      "        [-0.5300],\n",
      "        [-0.7723],\n",
      "        [-3.1130],\n",
      "        [-0.6397],\n",
      "        [-1.8272],\n",
      "        [-1.7822],\n",
      "        [-0.4559],\n",
      "        [-0.7187],\n",
      "        [-2.0522],\n",
      "        [-0.7304],\n",
      "        [-2.0840],\n",
      "        [-2.7713],\n",
      "        [-1.0382],\n",
      "        [-0.5858],\n",
      "        [-1.7614],\n",
      "        [ 0.0885],\n",
      "        [-0.6206],\n",
      "        [-0.6092],\n",
      "        [-2.1231],\n",
      "        [-0.9659],\n",
      "        [-0.4335],\n",
      "        [-1.6519],\n",
      "        [-1.3654],\n",
      "        [-2.2389],\n",
      "        [-0.3387],\n",
      "        [-1.6701],\n",
      "        [-1.8359],\n",
      "        [-1.8268],\n",
      "        [-1.1604]], device='cuda:0') tensor([[ 0.1300],\n",
      "        [-0.0134],\n",
      "        [-0.3198],\n",
      "        [-0.8036],\n",
      "        [ 0.1601],\n",
      "        [-0.1436],\n",
      "        [ 0.0882],\n",
      "        [-0.0379],\n",
      "        [ 0.1178],\n",
      "        [ 0.3004],\n",
      "        [-0.0777],\n",
      "        [-0.3660],\n",
      "        [ 0.2265],\n",
      "        [ 0.3757],\n",
      "        [ 0.4652],\n",
      "        [-0.6805],\n",
      "        [-0.2449],\n",
      "        [ 0.1821],\n",
      "        [-0.5577],\n",
      "        [-0.6434],\n",
      "        [-0.3077],\n",
      "        [-0.4057],\n",
      "        [-0.1660],\n",
      "        [-0.1421],\n",
      "        [ 0.0625],\n",
      "        [ 0.0983],\n",
      "        [ 0.2538],\n",
      "        [ 0.6113],\n",
      "        [-0.2727],\n",
      "        [ 0.1087],\n",
      "        [-0.1875],\n",
      "        [ 0.3355],\n",
      "        [-0.2389],\n",
      "        [-0.0921],\n",
      "        [ 0.2784],\n",
      "        [-0.4457],\n",
      "        [-0.2549],\n",
      "        [ 0.0719],\n",
      "        [ 0.1544],\n",
      "        [ 0.4807],\n",
      "        [ 0.1871],\n",
      "        [-0.4910],\n",
      "        [ 0.0401],\n",
      "        [-0.6633],\n",
      "        [ 0.0202],\n",
      "        [-0.2691],\n",
      "        [-0.2581],\n",
      "        [ 0.5283],\n",
      "        [ 0.4043],\n",
      "        [ 0.1046]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.7247, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 5.417436\n",
      "Validation set: Average loss: 9.248420, Accuracy: 0/9691 (0%)\n",
      "\n",
      "74.89467730522156\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 1000\n",
    "print('Training on', device)\n",
    "time_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "time_end = time.time()\n",
    "total_time = (time_end-time_start)/60 \n",
    "print(total_time)"
   ]
  },
  {
   "source": [
    "# Checking How Well the Model is Preforming "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truths_train = [] \n",
    "all_preds_train = [] \n",
    "for (data,target) in train_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_train.append(target.cpu().detach().numpy())\n",
    "    all_preds_train.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_train=len(all_truths_train)-1\n",
    "\n",
    "remainder_train=len(all_truths_train[incomplete_batch_id_train])\n",
    "\n",
    "total_values_train=(len(all_truths_train)*batch_size)-(batch_size-remainder_train)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_train_array=np.zeros(total_values_train)\n",
    "all_preds_train_array=np.zeros(total_values_train)\n",
    "k=0\n",
    "while k < total_values_train:\n",
    "    for i in range(len(all_truths_train)):\n",
    "        if i<incomplete_batch_id_train:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_train\n",
    "            for j in range(remainder_train):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# all_truths_train=all_truths_train_array\n",
    "# all_preds_train=all_preds_train_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.8159067745639459\n[-0.7073945]\n-0.8159067745639459\n-0.7073944807052612\n"
     ]
    }
   ],
   "source": [
    "all_truths_test = [] \n",
    "all_preds_test = [] \n",
    "for (data,target) in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_test.append(target.cpu().detach().numpy())\n",
    "    all_preds_test.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_test=len(all_truths_test)-1\n",
    "\n",
    "remainder_test=len(all_truths_test[incomplete_batch_id_test])\n",
    "\n",
    "\n",
    "total_values_test=(len(all_truths_test)*batch_size)-(batch_size-remainder_test)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_test_array=np.zeros(total_values_test)\n",
    "all_preds_test_array=np.zeros(total_values_test)\n",
    "k=0\n",
    "while k < total_values_test:\n",
    "    for i in range(len(all_truths_test)):\n",
    "        if i<incomplete_batch_id_test:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_test\n",
    "            for j in range(remainder_test):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "print(all_truths_test[3][43])\n",
    "print(all_preds_test[3][43])\n",
    "print(all_truths_test_array[193])\n",
    "print(all_preds_test_array[193])\n",
    "# all_truths_test=all_truths_test_array\n",
    "# all_preds_test=all_preds_test_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train=r2_score(all_truths_train_array,all_preds_train_array)\n",
    "r2_test=r2_score(all_truths_test_array,all_preds_test_array)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.subplot(1,2,1) \n",
    "plt.title('Test Data Set')\n",
    "plt.scatter(all_truths_test_array, all_preds_test_array,color = 'b', alpha = 0.1*7/3)\n",
    "plt.xlabel('Test True Values')\n",
    "plt.ylabel('Test Predicted Value')\n",
    "plt.text(0,-5,'R$^2$='+ str(round(r2_test,4)))\n",
    "plt.plot([-5,1],[-5,1],'r--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Train Data Set')\n",
    "plt.scatter(all_truths_train_array, all_preds_train_array,color = 'k', alpha = 0.1)\n",
    "plt.xlabel('Train True Values')\n",
    "plt.ylabel('Train Predicted Value')\n",
    "plt.plot([-5,1],[-5,1],'r--') \n",
    "plt.text(0,-5.5,'R$^2$='+str(round(r2_train,3)))\n",
    "plt.show()\n",
    "plt.savefig('/home/juanp/Documents/SURP-2021/Plots/Model 3/9 Copies of Images.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),'/home/juanp/Documents/SURP-2021/Models/SFR_Model_9_Copies_Images')"
   ]
  },
  {
   "source": [
    "# Loss as a Function of Epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.plot(epoch_nums, np.log10(training_loss))\n",
    "# plt.plot(epoch_nums, np.log10(validation_loss))\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('Log of loss')\n",
    "# plt.legend(['training', 'validation'], loc='upper right')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Plotting Galaxy Images and their True SFR vs CNN Predicted SFR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(picture[0][0,0:,0:])\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(picture[1][0,0:,0:])\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(picture[2][0,0:,0:])\n",
    "#     plt.show()"
   ]
  }
 ]
}