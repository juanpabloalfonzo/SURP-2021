{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Predict Star Formation Variables (sSFR, SFR, M*, age) Based on Visual Morphology (galaxy image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import decode_image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import marvin\n",
    "from marvin.tools.maps import Maps\n",
    "from marvin.tools.image import Image\n",
    "from marvin.utils.general.images import get_images_by_list\n",
    "from marvin import config\n",
    "from marvin.tools.cube import Cube\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image as image_PIL\n",
    "import time \n",
    "\n",
    "#set config attributes and turn on global downloads of Marvin data\n",
    "config.setRelease('DR15')\n",
    "config.mode = 'local'\n",
    "config.download = True\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data from Schema Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "\n",
    "#Problem with image associated with manga id at galaxy_list[3548], mangaid- 1-135668\n",
    "galaxy_list=np.delete(galaxy_list,3548)\n",
    "\n",
    "galaxy_list=np.unique(galaxy_list)\n",
    "\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "galaxies=galaxies.sort_values(by=['plateifu']) #Sorting galaxies by plateifu to match ImageFolder Output \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing Images from their Downloaded Locations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_locations=[]\n",
    "# for i in range (len(galaxy_list)):\n",
    "#     image_locations.append(Image(galaxy_list[i]).filename)\n",
    "    \n",
    "# image_locations=np.array(image_locations,dtype=str)\n",
    "# np.savetxt('Image Directories',image_locations,fmt='%s')\n",
    "\n",
    "image_locations=np.loadtxt('Image Directories',dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the Images into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107670\n",
      "Data loaders ready to read /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n"
     ]
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "\n",
    "# image=ImageFolder('/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/') #Picks up 3590 pictures, directory list has lenght 3637 however \n",
    "\n",
    "image_directory='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3'\n",
    "classes= sorted(os.listdir(image_directory))\n",
    "\n",
    "batch_size=50\n",
    "image_copies=30\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        #Rotates the image by some angle\n",
    "        transforms.RandomRotation(0,360),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        # transform=transformation\n",
    "    )\n",
    "    \n",
    "    #This loop transforms the images and assigns them the correct label \n",
    "\n",
    "    full_dataset_v2 = []   \n",
    "    for i in range(len(full_dataset)): \n",
    "        for j in range(image_copies): #This loops makes it so we have 5 copies of each galaxy image with different orientations \n",
    "            temp=transformation(resize_image(full_dataset[i][0]))\n",
    "            full_dataset_v2.append((temp,log_SFR.iloc[i])) \n",
    "    \n",
    "    full_dataset=full_dataset_v2\n",
    "\n",
    "    print(len(full_dataset))\n",
    "\n",
    "\n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(image_directory)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 3589\n",
      "    Root location: /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n",
      "3637\n",
      "3589\n",
      "3589\n"
     ]
    }
   ],
   "source": [
    "print(ImageFolder(image_directory))\n",
    "print(len(image_locations))\n",
    "print(len(np.unique(image_locations)))\n",
    "print(len(np.unique(galaxy_list)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_dataset = torchvision.datasets.ImageFolder(\n",
    "#         root='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/7968',\n",
    "#         # transform=transformation\n",
    "#     ) \n",
    "\n",
    "# full_dataset_v2 = []   \n",
    "# for i in range(len(full_dataset)): \n",
    "#     full_dataset_v2.append((resize_image(full_dataset[i][0]) , i)) \n",
    "\n",
    "# print(full_dataset_v2)\n",
    "\n",
    "\n",
    "# print(full_dataset[0][1])\n",
    "# for i in range(len(full_dataset)): \n",
    "#         full_dataset[i] = (resize_image(full_dataset[i][0]) , 0) \n",
    "        # full_dataset[i][0] = resize_image(full_dataset[i][0])\n",
    "        # full_dataset[i][1] = correct_label(correct_manga_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=24576, out_features=10, bias=True)\n",
      "  (fc1): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 128 x 128, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=10)\n",
    "\n",
    "        self.fc1= nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x+1\n",
    "        \n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # print(x)\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x=self.conv1(x)\n",
    "        \n",
    "        x=self.pool(x)\n",
    "       \n",
    "\n",
    "        x=F.relu(x)\n",
    "\n",
    "        \n",
    "      \n",
    " \n",
    "     \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x))) \n",
    "        \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return class probabilities via a log_softmax function\n",
    "        x=F.relu(x) \n",
    "        \n",
    "        x= self.fc1(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(SFR)).to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Loop/Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target=target.float()\n",
    "        # print(target.shape)\n",
    "        target=target.reshape(-1,1)\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "    \n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        if batch_idx==0:\n",
    "            print(output,target,output-target,loss)\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            target=target.float()\n",
    "            target=target.reshape(-1,1)\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-0.3613],\n",
      "        [ 0.0553],\n",
      "        [-0.4980],\n",
      "        [ 0.1164],\n",
      "        [ 0.2100],\n",
      "        [ 0.0040],\n",
      "        [ 0.0481],\n",
      "        [ 0.3241],\n",
      "        [ 0.2020],\n",
      "        [ 0.7370],\n",
      "        [ 0.3017],\n",
      "        [ 0.3843],\n",
      "        [ 0.0287],\n",
      "        [ 0.3683],\n",
      "        [-0.2993],\n",
      "        [ 0.4699],\n",
      "        [ 0.5494],\n",
      "        [-0.2735],\n",
      "        [-0.2666],\n",
      "        [ 0.4734],\n",
      "        [ 0.2067],\n",
      "        [-0.0084],\n",
      "        [ 0.1948],\n",
      "        [ 1.3919],\n",
      "        [ 0.2183],\n",
      "        [ 0.3202],\n",
      "        [-0.1342],\n",
      "        [-0.1058],\n",
      "        [ 0.9476],\n",
      "        [ 0.1726],\n",
      "        [-0.3720],\n",
      "        [-0.9110],\n",
      "        [ 0.0494],\n",
      "        [ 0.1516],\n",
      "        [ 0.2054],\n",
      "        [ 0.1503],\n",
      "        [ 0.1305],\n",
      "        [-0.0635],\n",
      "        [ 0.3998],\n",
      "        [-0.0661],\n",
      "        [ 0.6653],\n",
      "        [ 0.2341],\n",
      "        [ 0.1322]], device='cuda:0', grad_fn=<SubBackward0>) tensor(8.0941, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 6.898967\n",
      "Validation set: Average loss: 3.114713, Accuracy: 0/32301 (0%)\n",
      "\n",
      "995\n",
      "Epoch: 995\n",
      "tensor([[-2.2981],\n",
      "        [-2.5145],\n",
      "        [-0.9600],\n",
      "        [-0.8678],\n",
      "        [-2.7991],\n",
      "        [-2.8774],\n",
      "        [-0.3954],\n",
      "        [-0.7324],\n",
      "        [-1.8415],\n",
      "        [-1.6523],\n",
      "        [-0.7548],\n",
      "        [-2.0673],\n",
      "        [-2.3778],\n",
      "        [-0.9774],\n",
      "        [-1.9467],\n",
      "        [-2.0722],\n",
      "        [-2.5262],\n",
      "        [-2.7422],\n",
      "        [-0.3023],\n",
      "        [-1.0353],\n",
      "        [-2.1018],\n",
      "        [-0.6298],\n",
      "        [-1.4821],\n",
      "        [-1.2800],\n",
      "        [-0.9562],\n",
      "        [-1.5109],\n",
      "        [-1.8917],\n",
      "        [-0.8399],\n",
      "        [-0.9160],\n",
      "        [-0.9854],\n",
      "        [-1.4706],\n",
      "        [-1.1446],\n",
      "        [-1.4762],\n",
      "        [-1.0336],\n",
      "        [-1.6064],\n",
      "        [-1.6803],\n",
      "        [-1.5417],\n",
      "        [-1.3823],\n",
      "        [-0.2954],\n",
      "        [-1.4507],\n",
      "        [-0.9098],\n",
      "        [-2.2277],\n",
      "        [-1.8629],\n",
      "        [-3.7437],\n",
      "        [-1.2283],\n",
      "        [-1.6240],\n",
      "        [-1.3883],\n",
      "        [-3.2209],\n",
      "        [-2.9926],\n",
      "        [-1.1201]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.2210],\n",
      "        [-2.1855],\n",
      "        [-0.8889],\n",
      "        [-0.4553],\n",
      "        [-3.3577],\n",
      "        [-2.9921],\n",
      "        [-0.6349],\n",
      "        [-0.2358],\n",
      "        [-2.1041],\n",
      "        [-1.6866],\n",
      "        [-0.3249],\n",
      "        [-2.3496],\n",
      "        [-2.8218],\n",
      "        [-0.8723],\n",
      "        [-2.0291],\n",
      "        [-2.2339],\n",
      "        [-3.5716],\n",
      "        [-2.8682],\n",
      "        [-0.6284],\n",
      "        [-1.0119],\n",
      "        [-2.0621],\n",
      "        [-0.7106],\n",
      "        [-1.8165],\n",
      "        [-1.6619],\n",
      "        [-0.7828],\n",
      "        [-1.7739],\n",
      "        [-2.0966],\n",
      "        [-0.4986],\n",
      "        [-1.2301],\n",
      "        [-1.0996],\n",
      "        [-2.4650],\n",
      "        [-1.0124],\n",
      "        [-0.5288],\n",
      "        [-0.7714],\n",
      "        [-1.5482],\n",
      "        [-1.8415],\n",
      "        [-1.4520],\n",
      "        [-1.1458],\n",
      "        [ 0.2138],\n",
      "        [-2.0096],\n",
      "        [-0.7052],\n",
      "        [-3.3861],\n",
      "        [-2.3438],\n",
      "        [-3.6180],\n",
      "        [-0.8426],\n",
      "        [-1.1320],\n",
      "        [-1.0956],\n",
      "        [-3.4090],\n",
      "        [-2.2670],\n",
      "        [-1.2790]], device='cuda:0') tensor([[-0.0770],\n",
      "        [-0.3291],\n",
      "        [-0.0711],\n",
      "        [-0.4125],\n",
      "        [ 0.5585],\n",
      "        [ 0.1147],\n",
      "        [ 0.2394],\n",
      "        [-0.4966],\n",
      "        [ 0.2626],\n",
      "        [ 0.0342],\n",
      "        [-0.4299],\n",
      "        [ 0.2823],\n",
      "        [ 0.4440],\n",
      "        [-0.1051],\n",
      "        [ 0.0824],\n",
      "        [ 0.1618],\n",
      "        [ 1.0455],\n",
      "        [ 0.1260],\n",
      "        [ 0.3261],\n",
      "        [-0.0234],\n",
      "        [-0.0397],\n",
      "        [ 0.0808],\n",
      "        [ 0.3345],\n",
      "        [ 0.3819],\n",
      "        [-0.1734],\n",
      "        [ 0.2629],\n",
      "        [ 0.2049],\n",
      "        [-0.3414],\n",
      "        [ 0.3142],\n",
      "        [ 0.1142],\n",
      "        [ 0.9944],\n",
      "        [-0.1322],\n",
      "        [-0.9475],\n",
      "        [-0.2622],\n",
      "        [-0.0582],\n",
      "        [ 0.1613],\n",
      "        [-0.0897],\n",
      "        [-0.2365],\n",
      "        [-0.5092],\n",
      "        [ 0.5589],\n",
      "        [-0.2046],\n",
      "        [ 1.1584],\n",
      "        [ 0.4809],\n",
      "        [-0.1257],\n",
      "        [-0.3857],\n",
      "        [-0.4920],\n",
      "        [-0.2927],\n",
      "        [ 0.1880],\n",
      "        [-0.7257],\n",
      "        [ 0.1589]], device='cuda:0', grad_fn=<SubBackward0>) tensor(8.6777, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 6.899840\n",
      "Validation set: Average loss: 2.962266, Accuracy: 0/32301 (0%)\n",
      "\n",
      "996\n",
      "Epoch: 996\n",
      "tensor([[-1.9488],\n",
      "        [-1.2160],\n",
      "        [-1.4043],\n",
      "        [-0.8050],\n",
      "        [-2.8747],\n",
      "        [-2.4183],\n",
      "        [-0.2091],\n",
      "        [-0.2091],\n",
      "        [-2.0730],\n",
      "        [-1.8165],\n",
      "        [-0.6795],\n",
      "        [-2.6758],\n",
      "        [-2.7806],\n",
      "        [-0.5798],\n",
      "        [-1.8458],\n",
      "        [-2.3661],\n",
      "        [-3.5103],\n",
      "        [-2.3307],\n",
      "        [-0.8321],\n",
      "        [-0.8998],\n",
      "        [-2.2252],\n",
      "        [-0.9600],\n",
      "        [-1.8872],\n",
      "        [-1.2020],\n",
      "        [-1.0556],\n",
      "        [-2.1221],\n",
      "        [-2.2849],\n",
      "        [-0.7653],\n",
      "        [-1.3364],\n",
      "        [-1.3516],\n",
      "        [-1.2893],\n",
      "        [-0.8226],\n",
      "        [-0.4061],\n",
      "        [-1.1919],\n",
      "        [-1.6755],\n",
      "        [-2.2320],\n",
      "        [-1.2323],\n",
      "        [-1.3837],\n",
      "        [-0.4357],\n",
      "        [-1.6686],\n",
      "        [-0.9763],\n",
      "        [-2.9844],\n",
      "        [-2.4475],\n",
      "        [-3.3041],\n",
      "        [-1.0328],\n",
      "        [-1.1843],\n",
      "        [-1.1842],\n",
      "        [-3.9372],\n",
      "        [-1.9881],\n",
      "        [-1.2940]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.2210],\n",
      "        [-2.1855],\n",
      "        [-0.8889],\n",
      "        [-0.4553],\n",
      "        [-3.3577],\n",
      "        [-2.9921],\n",
      "        [-0.6349],\n",
      "        [-0.2358],\n",
      "        [-2.1041],\n",
      "        [-1.6866],\n",
      "        [-0.3249],\n",
      "        [-2.3496],\n",
      "        [-2.8218],\n",
      "        [-0.8723],\n",
      "        [-2.0291],\n",
      "        [-2.2339],\n",
      "        [-3.5716],\n",
      "        [-2.8682],\n",
      "        [-0.6284],\n",
      "        [-1.0119],\n",
      "        [-2.0621],\n",
      "        [-0.7106],\n",
      "        [-1.8165],\n",
      "        [-1.6619],\n",
      "        [-0.7828],\n",
      "        [-1.7739],\n",
      "        [-2.0966],\n",
      "        [-0.4986],\n",
      "        [-1.2301],\n",
      "        [-1.0996],\n",
      "        [-2.4650],\n",
      "        [-1.0124],\n",
      "        [-0.5288],\n",
      "        [-0.7714],\n",
      "        [-1.5482],\n",
      "        [-1.8415],\n",
      "        [-1.4520],\n",
      "        [-1.1458],\n",
      "        [ 0.2138],\n",
      "        [-2.0096],\n",
      "        [-0.7052],\n",
      "        [-3.3861],\n",
      "        [-2.3438],\n",
      "        [-3.6180],\n",
      "        [-0.8426],\n",
      "        [-1.1320],\n",
      "        [-1.0956],\n",
      "        [-3.4090],\n",
      "        [-2.2670],\n",
      "        [-1.2790]], device='cuda:0') tensor([[ 0.2722],\n",
      "        [ 0.9694],\n",
      "        [-0.5154],\n",
      "        [-0.3497],\n",
      "        [ 0.4829],\n",
      "        [ 0.5738],\n",
      "        [ 0.4258],\n",
      "        [ 0.0267],\n",
      "        [ 0.0310],\n",
      "        [-0.1299],\n",
      "        [-0.3546],\n",
      "        [-0.3262],\n",
      "        [ 0.0412],\n",
      "        [ 0.2925],\n",
      "        [ 0.1833],\n",
      "        [-0.1321],\n",
      "        [ 0.0614],\n",
      "        [ 0.5375],\n",
      "        [-0.2038],\n",
      "        [ 0.1121],\n",
      "        [-0.1631],\n",
      "        [-0.2494],\n",
      "        [-0.0707],\n",
      "        [ 0.4598],\n",
      "        [-0.2728],\n",
      "        [-0.3483],\n",
      "        [-0.1883],\n",
      "        [-0.2668],\n",
      "        [-0.1063],\n",
      "        [-0.2521],\n",
      "        [ 1.1757],\n",
      "        [ 0.1899],\n",
      "        [ 0.1226],\n",
      "        [-0.4204],\n",
      "        [-0.1273],\n",
      "        [-0.3905],\n",
      "        [ 0.2197],\n",
      "        [-0.2379],\n",
      "        [-0.6495],\n",
      "        [ 0.3410],\n",
      "        [-0.2711],\n",
      "        [ 0.4017],\n",
      "        [-0.1037],\n",
      "        [ 0.3139],\n",
      "        [-0.1902],\n",
      "        [-0.0523],\n",
      "        [-0.0886],\n",
      "        [-0.5283],\n",
      "        [ 0.2789],\n",
      "        [-0.0150]], device='cuda:0', grad_fn=<SubBackward0>) tensor(6.7335, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 6.906639\n",
      "Validation set: Average loss: 3.589299, Accuracy: 0/32301 (0%)\n",
      "\n",
      "997\n",
      "Epoch: 997\n",
      "tensor([[-2.6143e+00],\n",
      "        [-1.9548e+00],\n",
      "        [-7.5625e-01],\n",
      "        [-7.1479e-01],\n",
      "        [-3.5442e+00],\n",
      "        [-3.0224e+00],\n",
      "        [-1.2602e+00],\n",
      "        [-2.1145e-01],\n",
      "        [-2.0628e+00],\n",
      "        [-1.6372e+00],\n",
      "        [-2.5246e-01],\n",
      "        [-2.0994e+00],\n",
      "        [-2.0646e+00],\n",
      "        [-3.3753e-01],\n",
      "        [-1.3989e+00],\n",
      "        [-1.6308e+00],\n",
      "        [-3.2587e+00],\n",
      "        [-3.2757e+00],\n",
      "        [-8.7193e-01],\n",
      "        [-1.0276e+00],\n",
      "        [-2.3108e+00],\n",
      "        [-6.6249e-01],\n",
      "        [-1.8886e+00],\n",
      "        [-1.4980e+00],\n",
      "        [-4.5452e-01],\n",
      "        [-1.8676e+00],\n",
      "        [-2.2975e+00],\n",
      "        [ 1.8665e-03],\n",
      "        [-1.3006e+00],\n",
      "        [-1.0848e+00],\n",
      "        [-1.3945e+00],\n",
      "        [-1.2469e+00],\n",
      "        [-5.1367e-01],\n",
      "        [-7.8274e-01],\n",
      "        [-1.5979e+00],\n",
      "        [-1.8271e+00],\n",
      "        [-1.3426e+00],\n",
      "        [-1.5768e+00],\n",
      "        [-3.8839e-01],\n",
      "        [-2.1459e+00],\n",
      "        [-9.7518e-01],\n",
      "        [-3.0739e+00],\n",
      "        [-2.4706e+00],\n",
      "        [-2.2823e+00],\n",
      "        [-8.3722e-01],\n",
      "        [-9.0713e-01],\n",
      "        [-7.6240e-01],\n",
      "        [-2.9791e+00],\n",
      "        [-2.8527e+00],\n",
      "        [-1.4638e+00]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.2210],\n",
      "        [-2.1855],\n",
      "        [-0.8889],\n",
      "        [-0.4553],\n",
      "        [-3.3577],\n",
      "        [-2.9921],\n",
      "        [-0.6349],\n",
      "        [-0.2358],\n",
      "        [-2.1041],\n",
      "        [-1.6866],\n",
      "        [-0.3249],\n",
      "        [-2.3496],\n",
      "        [-2.8218],\n",
      "        [-0.8723],\n",
      "        [-2.0291],\n",
      "        [-2.2339],\n",
      "        [-3.5716],\n",
      "        [-2.8682],\n",
      "        [-0.6284],\n",
      "        [-1.0119],\n",
      "        [-2.0621],\n",
      "        [-0.7106],\n",
      "        [-1.8165],\n",
      "        [-1.6619],\n",
      "        [-0.7828],\n",
      "        [-1.7739],\n",
      "        [-2.0966],\n",
      "        [-0.4986],\n",
      "        [-1.2301],\n",
      "        [-1.0996],\n",
      "        [-2.4650],\n",
      "        [-1.0124],\n",
      "        [-0.5288],\n",
      "        [-0.7714],\n",
      "        [-1.5482],\n",
      "        [-1.8415],\n",
      "        [-1.4520],\n",
      "        [-1.1458],\n",
      "        [ 0.2138],\n",
      "        [-2.0096],\n",
      "        [-0.7052],\n",
      "        [-3.3861],\n",
      "        [-2.3438],\n",
      "        [-3.6180],\n",
      "        [-0.8426],\n",
      "        [-1.1320],\n",
      "        [-1.0956],\n",
      "        [-3.4090],\n",
      "        [-2.2670],\n",
      "        [-1.2790]], device='cuda:0') tensor([[-0.3933],\n",
      "        [ 0.2307],\n",
      "        [ 0.1327],\n",
      "        [-0.2595],\n",
      "        [-0.1866],\n",
      "        [-0.0303],\n",
      "        [-0.6253],\n",
      "        [ 0.0243],\n",
      "        [ 0.0413],\n",
      "        [ 0.0493],\n",
      "        [ 0.0724],\n",
      "        [ 0.2502],\n",
      "        [ 0.7573],\n",
      "        [ 0.5348],\n",
      "        [ 0.6302],\n",
      "        [ 0.6031],\n",
      "        [ 0.3129],\n",
      "        [-0.4076],\n",
      "        [-0.2436],\n",
      "        [-0.0157],\n",
      "        [-0.2487],\n",
      "        [ 0.0481],\n",
      "        [-0.0720],\n",
      "        [ 0.1639],\n",
      "        [ 0.3283],\n",
      "        [-0.0937],\n",
      "        [-0.2009],\n",
      "        [ 0.5004],\n",
      "        [-0.0704],\n",
      "        [ 0.0148],\n",
      "        [ 1.0705],\n",
      "        [-0.2344],\n",
      "        [ 0.0151],\n",
      "        [-0.0113],\n",
      "        [-0.0497],\n",
      "        [ 0.0145],\n",
      "        [ 0.1093],\n",
      "        [-0.4309],\n",
      "        [-0.6022],\n",
      "        [-0.1363],\n",
      "        [-0.2700],\n",
      "        [ 0.3122],\n",
      "        [-0.1268],\n",
      "        [ 1.3357],\n",
      "        [ 0.0054],\n",
      "        [ 0.2249],\n",
      "        [ 0.3332],\n",
      "        [ 0.4299],\n",
      "        [-0.5857],\n",
      "        [-0.1848]], device='cuda:0', grad_fn=<SubBackward0>) tensor(7.7219, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 6.924575\n",
      "Validation set: Average loss: 4.072786, Accuracy: 0/32301 (0%)\n",
      "\n",
      "998\n",
      "Epoch: 998\n",
      "tensor([[-2.0968],\n",
      "        [-2.1012],\n",
      "        [-1.0934],\n",
      "        [-1.0287],\n",
      "        [-2.6486],\n",
      "        [-2.6877],\n",
      "        [-0.6920],\n",
      "        [-0.2098],\n",
      "        [-2.0315],\n",
      "        [-1.9013],\n",
      "        [-0.3920],\n",
      "        [-2.2166],\n",
      "        [-2.4130],\n",
      "        [-0.8416],\n",
      "        [-1.1849],\n",
      "        [-1.8941],\n",
      "        [-3.2148],\n",
      "        [-3.2809],\n",
      "        [-1.0917],\n",
      "        [-1.0356],\n",
      "        [-1.8736],\n",
      "        [-0.3559],\n",
      "        [-1.9342],\n",
      "        [-1.1674],\n",
      "        [-1.0932],\n",
      "        [-1.6843],\n",
      "        [-1.7376],\n",
      "        [-0.4946],\n",
      "        [-1.0108],\n",
      "        [-1.1098],\n",
      "        [-1.5923],\n",
      "        [-0.8489],\n",
      "        [-0.7911],\n",
      "        [-0.5423],\n",
      "        [-1.2281],\n",
      "        [-1.2124],\n",
      "        [-0.7622],\n",
      "        [-1.4399],\n",
      "        [-0.5092],\n",
      "        [-2.0711],\n",
      "        [-0.9766],\n",
      "        [-2.5984],\n",
      "        [-2.4485],\n",
      "        [-3.8451],\n",
      "        [-1.3411],\n",
      "        [-1.3193],\n",
      "        [-1.4049],\n",
      "        [-3.4460],\n",
      "        [-1.9927],\n",
      "        [-1.6355]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.2210],\n",
      "        [-2.1855],\n",
      "        [-0.8889],\n",
      "        [-0.4553],\n",
      "        [-3.3577],\n",
      "        [-2.9921],\n",
      "        [-0.6349],\n",
      "        [-0.2358],\n",
      "        [-2.1041],\n",
      "        [-1.6866],\n",
      "        [-0.3249],\n",
      "        [-2.3496],\n",
      "        [-2.8218],\n",
      "        [-0.8723],\n",
      "        [-2.0291],\n",
      "        [-2.2339],\n",
      "        [-3.5716],\n",
      "        [-2.8682],\n",
      "        [-0.6284],\n",
      "        [-1.0119],\n",
      "        [-2.0621],\n",
      "        [-0.7106],\n",
      "        [-1.8165],\n",
      "        [-1.6619],\n",
      "        [-0.7828],\n",
      "        [-1.7739],\n",
      "        [-2.0966],\n",
      "        [-0.4986],\n",
      "        [-1.2301],\n",
      "        [-1.0996],\n",
      "        [-2.4650],\n",
      "        [-1.0124],\n",
      "        [-0.5288],\n",
      "        [-0.7714],\n",
      "        [-1.5482],\n",
      "        [-1.8415],\n",
      "        [-1.4520],\n",
      "        [-1.1458],\n",
      "        [ 0.2138],\n",
      "        [-2.0096],\n",
      "        [-0.7052],\n",
      "        [-3.3861],\n",
      "        [-2.3438],\n",
      "        [-3.6180],\n",
      "        [-0.8426],\n",
      "        [-1.1320],\n",
      "        [-1.0956],\n",
      "        [-3.4090],\n",
      "        [-2.2670],\n",
      "        [-1.2790]], device='cuda:0') tensor([[ 0.1242],\n",
      "        [ 0.0843],\n",
      "        [-0.2045],\n",
      "        [-0.5735],\n",
      "        [ 0.7090],\n",
      "        [ 0.3043],\n",
      "        [-0.0571],\n",
      "        [ 0.0260],\n",
      "        [ 0.0726],\n",
      "        [-0.2147],\n",
      "        [-0.0671],\n",
      "        [ 0.1329],\n",
      "        [ 0.4088],\n",
      "        [ 0.0307],\n",
      "        [ 0.8442],\n",
      "        [ 0.3399],\n",
      "        [ 0.3569],\n",
      "        [-0.4127],\n",
      "        [-0.4634],\n",
      "        [-0.0237],\n",
      "        [ 0.1885],\n",
      "        [ 0.3548],\n",
      "        [-0.1176],\n",
      "        [ 0.4945],\n",
      "        [-0.3104],\n",
      "        [ 0.0896],\n",
      "        [ 0.3590],\n",
      "        [ 0.0039],\n",
      "        [ 0.2193],\n",
      "        [-0.0102],\n",
      "        [ 0.8727],\n",
      "        [ 0.1636],\n",
      "        [-0.2623],\n",
      "        [ 0.2292],\n",
      "        [ 0.3201],\n",
      "        [ 0.6292],\n",
      "        [ 0.6898],\n",
      "        [-0.2941],\n",
      "        [-0.7230],\n",
      "        [-0.0615],\n",
      "        [-0.2714],\n",
      "        [ 0.7876],\n",
      "        [-0.1047],\n",
      "        [-0.2271],\n",
      "        [-0.4985],\n",
      "        [-0.1873],\n",
      "        [-0.3094],\n",
      "        [-0.0371],\n",
      "        [ 0.2742],\n",
      "        [-0.3565]], device='cuda:0', grad_fn=<SubBackward0>) tensor(7.1126, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 7.041596\n",
      "Validation set: Average loss: 3.430422, Accuracy: 0/32301 (0%)\n",
      "\n",
      "999\n",
      "Epoch: 999\n",
      "tensor([[-2.1324],\n",
      "        [-1.9172],\n",
      "        [-0.9707],\n",
      "        [-0.4595],\n",
      "        [-3.3887],\n",
      "        [-2.7986],\n",
      "        [-0.5300],\n",
      "        [-0.2158],\n",
      "        [-1.8319],\n",
      "        [-1.5066],\n",
      "        [-0.4243],\n",
      "        [-2.5841],\n",
      "        [-2.1214],\n",
      "        [-0.7061],\n",
      "        [-1.3288],\n",
      "        [-2.2032],\n",
      "        [-3.4595],\n",
      "        [-2.9580],\n",
      "        [-0.3387],\n",
      "        [-1.2504],\n",
      "        [-2.3748],\n",
      "        [-0.5641],\n",
      "        [-2.1354],\n",
      "        [-1.2959],\n",
      "        [-1.1106],\n",
      "        [-1.4701],\n",
      "        [-1.7892],\n",
      "        [-0.4303],\n",
      "        [-1.0675],\n",
      "        [-1.6749],\n",
      "        [-1.2453],\n",
      "        [-0.9111],\n",
      "        [-0.8936],\n",
      "        [-0.9631],\n",
      "        [-1.1457],\n",
      "        [-1.1075],\n",
      "        [-0.9710],\n",
      "        [-0.9209],\n",
      "        [-0.2158],\n",
      "        [-1.6519],\n",
      "        [-0.9960],\n",
      "        [-3.0833],\n",
      "        [-2.1601],\n",
      "        [-2.8741],\n",
      "        [-0.6827],\n",
      "        [-1.3164],\n",
      "        [-0.9447],\n",
      "        [-3.5612],\n",
      "        [-1.7774],\n",
      "        [-1.4616]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.2210],\n",
      "        [-2.1855],\n",
      "        [-0.8889],\n",
      "        [-0.4553],\n",
      "        [-3.3577],\n",
      "        [-2.9921],\n",
      "        [-0.6349],\n",
      "        [-0.2358],\n",
      "        [-2.1041],\n",
      "        [-1.6866],\n",
      "        [-0.3249],\n",
      "        [-2.3496],\n",
      "        [-2.8218],\n",
      "        [-0.8723],\n",
      "        [-2.0291],\n",
      "        [-2.2339],\n",
      "        [-3.5716],\n",
      "        [-2.8682],\n",
      "        [-0.6284],\n",
      "        [-1.0119],\n",
      "        [-2.0621],\n",
      "        [-0.7106],\n",
      "        [-1.8165],\n",
      "        [-1.6619],\n",
      "        [-0.7828],\n",
      "        [-1.7739],\n",
      "        [-2.0966],\n",
      "        [-0.4986],\n",
      "        [-1.2301],\n",
      "        [-1.0996],\n",
      "        [-2.4650],\n",
      "        [-1.0124],\n",
      "        [-0.5288],\n",
      "        [-0.7714],\n",
      "        [-1.5482],\n",
      "        [-1.8415],\n",
      "        [-1.4520],\n",
      "        [-1.1458],\n",
      "        [ 0.2138],\n",
      "        [-2.0096],\n",
      "        [-0.7052],\n",
      "        [-3.3861],\n",
      "        [-2.3438],\n",
      "        [-3.6180],\n",
      "        [-0.8426],\n",
      "        [-1.1320],\n",
      "        [-1.0956],\n",
      "        [-3.4090],\n",
      "        [-2.2670],\n",
      "        [-1.2790]], device='cuda:0') tensor([[ 0.0886],\n",
      "        [ 0.2683],\n",
      "        [-0.0818],\n",
      "        [-0.0042],\n",
      "        [-0.0311],\n",
      "        [ 0.1935],\n",
      "        [ 0.1049],\n",
      "        [ 0.0200],\n",
      "        [ 0.2722],\n",
      "        [ 0.1800],\n",
      "        [-0.0994],\n",
      "        [-0.2345],\n",
      "        [ 0.7005],\n",
      "        [ 0.1662],\n",
      "        [ 0.7003],\n",
      "        [ 0.0308],\n",
      "        [ 0.1121],\n",
      "        [-0.0898],\n",
      "        [ 0.2897],\n",
      "        [-0.2386],\n",
      "        [-0.3127],\n",
      "        [ 0.1466],\n",
      "        [-0.3189],\n",
      "        [ 0.3660],\n",
      "        [-0.3278],\n",
      "        [ 0.3037],\n",
      "        [ 0.3073],\n",
      "        [ 0.0682],\n",
      "        [ 0.1626],\n",
      "        [-0.5754],\n",
      "        [ 1.2197],\n",
      "        [ 0.1013],\n",
      "        [-0.3648],\n",
      "        [-0.1917],\n",
      "        [ 0.4025],\n",
      "        [ 0.7341],\n",
      "        [ 0.4810],\n",
      "        [ 0.2250],\n",
      "        [-0.4296],\n",
      "        [ 0.3577],\n",
      "        [-0.2908],\n",
      "        [ 0.3028],\n",
      "        [ 0.1836],\n",
      "        [ 0.7439],\n",
      "        [ 0.1599],\n",
      "        [-0.1844],\n",
      "        [ 0.1509],\n",
      "        [-0.1523],\n",
      "        [ 0.4895],\n",
      "        [-0.1825]], device='cuda:0', grad_fn=<SubBackward0>) tensor(6.5948, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 6.947544\n",
      "Validation set: Average loss: 3.022784, Accuracy: 0/32301 (0%)\n",
      "\n",
      "1000\n",
      "Epoch: 1000\n",
      "tensor([[-2.3160],\n",
      "        [-2.8234],\n",
      "        [-0.7550],\n",
      "        [-0.5352],\n",
      "        [-3.1516],\n",
      "        [-2.8914],\n",
      "        [-0.6566],\n",
      "        [-0.3613],\n",
      "        [-1.4052],\n",
      "        [-1.9425],\n",
      "        [-0.6600],\n",
      "        [-2.2763],\n",
      "        [-2.5474],\n",
      "        [-0.7169],\n",
      "        [-1.6846],\n",
      "        [-1.9564],\n",
      "        [-2.8452],\n",
      "        [-2.4627],\n",
      "        [-0.2976],\n",
      "        [-1.4316],\n",
      "        [-1.7611],\n",
      "        [-1.1544],\n",
      "        [-1.6994],\n",
      "        [-1.3056],\n",
      "        [-1.1362],\n",
      "        [-1.9028],\n",
      "        [-2.4228],\n",
      "        [-0.9891],\n",
      "        [-1.3293],\n",
      "        [-0.8265],\n",
      "        [-1.2053],\n",
      "        [-1.2942],\n",
      "        [-0.6736],\n",
      "        [-0.9492],\n",
      "        [-1.4064],\n",
      "        [-1.2009],\n",
      "        [-1.4802],\n",
      "        [-1.2478],\n",
      "        [-0.7588],\n",
      "        [-1.4963],\n",
      "        [-0.6332],\n",
      "        [-2.4102],\n",
      "        [-2.1373],\n",
      "        [-3.7318],\n",
      "        [-0.8086],\n",
      "        [-1.2873],\n",
      "        [-1.1840],\n",
      "        [-3.1993],\n",
      "        [-1.2049],\n",
      "        [-1.4008]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-2.2210],\n",
      "        [-2.1855],\n",
      "        [-0.8889],\n",
      "        [-0.4553],\n",
      "        [-3.3577],\n",
      "        [-2.9921],\n",
      "        [-0.6349],\n",
      "        [-0.2358],\n",
      "        [-2.1041],\n",
      "        [-1.6866],\n",
      "        [-0.3249],\n",
      "        [-2.3496],\n",
      "        [-2.8218],\n",
      "        [-0.8723],\n",
      "        [-2.0291],\n",
      "        [-2.2339],\n",
      "        [-3.5716],\n",
      "        [-2.8682],\n",
      "        [-0.6284],\n",
      "        [-1.0119],\n",
      "        [-2.0621],\n",
      "        [-0.7106],\n",
      "        [-1.8165],\n",
      "        [-1.6619],\n",
      "        [-0.7828],\n",
      "        [-1.7739],\n",
      "        [-2.0966],\n",
      "        [-0.4986],\n",
      "        [-1.2301],\n",
      "        [-1.0996],\n",
      "        [-2.4650],\n",
      "        [-1.0124],\n",
      "        [-0.5288],\n",
      "        [-0.7714],\n",
      "        [-1.5482],\n",
      "        [-1.8415],\n",
      "        [-1.4520],\n",
      "        [-1.1458],\n",
      "        [ 0.2138],\n",
      "        [-2.0096],\n",
      "        [-0.7052],\n",
      "        [-3.3861],\n",
      "        [-2.3438],\n",
      "        [-3.6180],\n",
      "        [-0.8426],\n",
      "        [-1.1320],\n",
      "        [-1.0956],\n",
      "        [-3.4090],\n",
      "        [-2.2670],\n",
      "        [-1.2790]], device='cuda:0') tensor([[-0.0950],\n",
      "        [-0.6379],\n",
      "        [ 0.1340],\n",
      "        [-0.0800],\n",
      "        [ 0.2061],\n",
      "        [ 0.1007],\n",
      "        [-0.0218],\n",
      "        [-0.1255],\n",
      "        [ 0.6989],\n",
      "        [-0.2559],\n",
      "        [-0.3351],\n",
      "        [ 0.0733],\n",
      "        [ 0.2745],\n",
      "        [ 0.1554],\n",
      "        [ 0.3445],\n",
      "        [ 0.2775],\n",
      "        [ 0.7265],\n",
      "        [ 0.4055],\n",
      "        [ 0.3308],\n",
      "        [-0.4197],\n",
      "        [ 0.3010],\n",
      "        [-0.4438],\n",
      "        [ 0.1172],\n",
      "        [ 0.3563],\n",
      "        [-0.3534],\n",
      "        [-0.1290],\n",
      "        [-0.3262],\n",
      "        [-0.4906],\n",
      "        [-0.0992],\n",
      "        [ 0.2730],\n",
      "        [ 1.2597],\n",
      "        [-0.2817],\n",
      "        [-0.1449],\n",
      "        [-0.1778],\n",
      "        [ 0.1418],\n",
      "        [ 0.6407],\n",
      "        [-0.0282],\n",
      "        [-0.1019],\n",
      "        [-0.9726],\n",
      "        [ 0.5133],\n",
      "        [ 0.0720],\n",
      "        [ 0.9759],\n",
      "        [ 0.2064],\n",
      "        [-0.1139],\n",
      "        [ 0.0340],\n",
      "        [-0.1553],\n",
      "        [-0.0884],\n",
      "        [ 0.2097],\n",
      "        [ 1.0621],\n",
      "        [-0.1218]], device='cuda:0', grad_fn=<SubBackward0>) tensor(9.0577, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 6.922155\n",
      "Validation set: Average loss: 3.215220, Accuracy: 0/32301 (0%)\n",
      "\n",
      "239.4679659485817\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 1000\n",
    "print('Training on', device)\n",
    "time_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "time_end = time.time()\n",
    "total_time = (time_end-time_start)/60 \n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking How Well the Model is Preforming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truths_train = [] \n",
    "all_preds_train = [] \n",
    "for (data,target) in train_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_train.append(target.cpu().detach().numpy())\n",
    "    all_preds_train.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_train=len(all_truths_train)-1\n",
    "\n",
    "remainder_train=len(all_truths_train[incomplete_batch_id_train])\n",
    "\n",
    "total_values_train=(len(all_truths_train)*batch_size)-(batch_size-remainder_train)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_train_array=np.zeros(total_values_train)\n",
    "all_preds_train_array=np.zeros(total_values_train)\n",
    "k=0\n",
    "while k < total_values_train:\n",
    "    for i in range(len(all_truths_train)):\n",
    "        if i<incomplete_batch_id_train:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_train\n",
    "            for j in range(remainder_train):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# all_truths_train=all_truths_train_array\n",
    "# all_preds_train=all_preds_train_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7977967652124356\n",
      "[-1.5908219]\n",
      "-1.7977967652124356\n",
      "-1.5908218622207642\n"
     ]
    }
   ],
   "source": [
    "all_truths_test = [] \n",
    "all_preds_test = [] \n",
    "for (data,target) in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_test.append(target.cpu().detach().numpy())\n",
    "    all_preds_test.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_test=len(all_truths_test)-1\n",
    "\n",
    "remainder_test=len(all_truths_test[incomplete_batch_id_test])\n",
    "\n",
    "\n",
    "total_values_test=(len(all_truths_test)*batch_size)-(batch_size-remainder_test)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_test_array=np.zeros(total_values_test)\n",
    "all_preds_test_array=np.zeros(total_values_test)\n",
    "k=0\n",
    "while k < total_values_test:\n",
    "    for i in range(len(all_truths_test)):\n",
    "        if i<incomplete_batch_id_test:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_test\n",
    "            for j in range(remainder_test):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "print(all_truths_test[3][43])\n",
    "print(all_preds_test[3][43])\n",
    "print(all_truths_test_array[193])\n",
    "print(all_preds_test_array[193])\n",
    "# all_truths_test=all_truths_test_array\n",
    "# all_preds_test=all_preds_test_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train=r2_score(all_truths_train_array,all_preds_train_array)\n",
    "r2_test=r2_score(all_truths_test_array,all_preds_test_array)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.subplot(1,2,1) \n",
    "plt.title('Test Data Set')\n",
    "plt.scatter(all_truths_test_array, all_preds_test_array,color = 'b', alpha = 0.1*7/3)\n",
    "plt.xlabel('Test True Values')\n",
    "plt.ylabel('Test Predicted Value')\n",
    "plt.text(0,-5,'R$^2$='+ str(round(r2_test,4)))\n",
    "plt.plot([-5,1],[-5,1],'r--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Train Data Set')\n",
    "plt.scatter(all_truths_train_array, all_preds_train_array,color = 'k', alpha = 0.1)\n",
    "plt.xlabel('Train True Values')\n",
    "plt.ylabel('Train Predicted Value')\n",
    "plt.plot([-5,1],[-5,1],'r--') \n",
    "plt.text(0,-5.5,'R$^2$='+str(round(r2_train,3)))\n",
    "plt.show()\n",
    "plt.savefig('/home/juanp/Documents/SURP-2021/Plots/Model 3/Rotations 30 Copies of Images.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/juanp/Documents/SURP-2021/Models/SFR_Model_Rotation_30_Copies_Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss as a Function of Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.plot(epoch_nums, np.log10(training_loss))\n",
    "# plt.plot(epoch_nums, np.log10(validation_loss))\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('Log of loss')\n",
    "# plt.legend(['training', 'validation'], loc='upper right')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Galaxy Images and their True SFR vs CNN Predicted SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(picture[0][0,0:,0:])\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(picture[1][0,0:,0:])\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(picture[2][0,0:,0:])\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}