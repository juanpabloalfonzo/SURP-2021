{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 3: Predict Star Formation Variables (sSFR, SFR, M*, age) Based on Visual Morphology (galaxy image)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import decode_image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import marvin\n",
    "from marvin.tools.maps import Maps\n",
    "from marvin.tools.image import Image\n",
    "from marvin.utils.general.images import get_images_by_list\n",
    "from marvin import config\n",
    "from marvin.tools.cube import Cube\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image as image_PIL\n",
    "import time \n",
    "\n",
    "#set config attributes and turn on global downloads of Marvin data\n",
    "config.setRelease('DR15')\n",
    "config.mode = 'local'\n",
    "config.download = True\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.linear1 = torch.nn.Linear(outputSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "\n",
    "#Problem with image associated with manga id at galaxy_list[3548], mangaid- 1-135668\n",
    "galaxy_list=np.delete(galaxy_list,3548)\n",
    "\n",
    "galaxy_list=np.unique(galaxy_list)\n",
    "\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "galaxies=galaxies.sort_values(by=['plateifu']) #Sorting galaxies by plateifu to match ImageFolder Output \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n"
   ]
  },
  {
   "source": [
    "# Importing Images from their Downloaded Locations \n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_locations=[]\n",
    "# for i in range (len(galaxy_list)):\n",
    "#     image_locations.append(Image(galaxy_list[i]).filename)\n",
    "    \n",
    "# image_locations=np.array(image_locations,dtype=str)\n",
    "# np.savetxt('Image Directories',image_locations,fmt='%s')\n",
    "\n",
    "image_locations=np.loadtxt('Image Directories',dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n"
   ]
  },
  {
   "source": [
    "# Putting the Images into DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17945\nData loaders ready to read /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n"
     ]
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "\n",
    "# image=ImageFolder('/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/') #Picks up 3590 pictures, directory list has lenght 3637 however \n",
    "\n",
    "image_directory='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3'\n",
    "classes= sorted(os.listdir(image_directory))\n",
    "\n",
    "batch_size=50\n",
    "image_copies=5\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        #Rotates the image by some angle\n",
    "        # transforms.RandomRotation(0,360),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        # transform=transformation\n",
    "    )\n",
    "    \n",
    "    #This loop transforms the images and assigns them the correct label \n",
    "\n",
    "    full_dataset_v2 = []   \n",
    "    for i in range(len(full_dataset)): \n",
    "        for j in range(image_copies): #This loops makes it so we have 5 copies of each galaxy image with different orientations \n",
    "            temp=transformation(resize_image(full_dataset[i][0]))\n",
    "            full_dataset_v2.append((temp,log_SFR.iloc[i])) \n",
    "    \n",
    "    full_dataset=full_dataset_v2\n",
    "\n",
    "    print(len(full_dataset))\n",
    "\n",
    "\n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(image_directory)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset ImageFolder\n    Number of datapoints: 3589\n    Root location: /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n3637\n3589\n3589\n"
     ]
    }
   ],
   "source": [
    "print(ImageFolder(image_directory))\n",
    "print(len(image_locations))\n",
    "print(len(np.unique(image_locations)))\n",
    "print(len(np.unique(galaxy_list)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_dataset = torchvision.datasets.ImageFolder(\n",
    "#         root='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/7968',\n",
    "#         # transform=transformation\n",
    "#     ) \n",
    "\n",
    "# full_dataset_v2 = []   \n",
    "# for i in range(len(full_dataset)): \n",
    "#     full_dataset_v2.append((resize_image(full_dataset[i][0]) , i)) \n",
    "\n",
    "# print(full_dataset_v2)\n",
    "\n",
    "\n",
    "# print(full_dataset[0][1])\n",
    "# for i in range(len(full_dataset)): \n",
    "#         full_dataset[i] = (resize_image(full_dataset[i][0]) , 0) \n",
    "        # full_dataset[i][0] = resize_image(full_dataset[i][0])\n",
    "        # full_dataset[i][1] = correct_label(correct_manga_id) "
   ]
  },
  {
   "source": [
    "# Defining the Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (drop): Dropout2d(p=0.2, inplace=False)\n  (fc): Linear(in_features=24576, out_features=10, bias=True)\n  (fc1): Linear(in_features=10, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 128 x 128, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=10)\n",
    "\n",
    "        self.fc1= nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x+1\n",
    "        \n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # print(x)\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x=self.conv1(x)\n",
    "        \n",
    "        x=self.pool(x)\n",
    "       \n",
    "\n",
    "        x=F.relu(x)\n",
    "\n",
    "        \n",
    "      \n",
    " \n",
    "     \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x))) \n",
    "        \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return class probabilities via a log_softmax function\n",
    "        x=F.relu(x) \n",
    "        \n",
    "        x= self.fc1(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(SFR)).to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "# Creating Training Loop/Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target=target.float()\n",
    "        # print(target.shape)\n",
    "        target=target.reshape(-1,1)\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "    \n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        if batch_idx==0:\n",
    "            print(output,target,output-target,loss)\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Create Test Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            target=target.float()\n",
    "            target=target.reshape(-1,1)\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.2084],\n",
      "        [-0.0272],\n",
      "        [-0.0541],\n",
      "        [ 0.0660],\n",
      "        [-0.1412],\n",
      "        [ 0.0657],\n",
      "        [ 0.4628],\n",
      "        [-0.1203],\n",
      "        [ 0.0599],\n",
      "        [ 0.2610],\n",
      "        [ 0.5643],\n",
      "        [ 0.2186],\n",
      "        [ 0.1155],\n",
      "        [-0.3379],\n",
      "        [ 0.2936],\n",
      "        [ 0.0740],\n",
      "        [ 0.2949],\n",
      "        [ 0.3590],\n",
      "        [-0.5728],\n",
      "        [ 0.0862],\n",
      "        [-0.4090],\n",
      "        [ 0.0696],\n",
      "        [-0.0252],\n",
      "        [-0.0752],\n",
      "        [ 0.2596],\n",
      "        [ 0.1523],\n",
      "        [ 0.0131],\n",
      "        [ 0.3467],\n",
      "        [-0.2390],\n",
      "        [ 0.0812],\n",
      "        [-0.1211],\n",
      "        [-0.2033],\n",
      "        [ 0.1164],\n",
      "        [ 0.1131],\n",
      "        [ 0.4085],\n",
      "        [ 0.2695],\n",
      "        [ 0.1690],\n",
      "        [-0.1792],\n",
      "        [ 0.2556],\n",
      "        [-0.0631],\n",
      "        [-0.1464],\n",
      "        [ 0.3173],\n",
      "        [-0.4196]], device='cuda:0', grad_fn=<SubBackward0>) tensor(2.7928, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 4.334727\n",
      "Validation set: Average loss: 16.956257, Accuracy: 0/5384 (0%)\n",
      "\n",
      "995\n",
      "Epoch: 995\n",
      "tensor([[-1.2982],\n",
      "        [-1.1258],\n",
      "        [-1.1772],\n",
      "        [-0.7010],\n",
      "        [-1.4397],\n",
      "        [-0.5696],\n",
      "        [-1.8643],\n",
      "        [-1.6375],\n",
      "        [-0.9963],\n",
      "        [-1.3475],\n",
      "        [-1.7478],\n",
      "        [-1.1283],\n",
      "        [-1.8023],\n",
      "        [-3.0264],\n",
      "        [-0.8753],\n",
      "        [-0.7306],\n",
      "        [-1.2872],\n",
      "        [-2.8395],\n",
      "        [-1.9391],\n",
      "        [-1.5588],\n",
      "        [-0.2282],\n",
      "        [-2.4851],\n",
      "        [-1.3835],\n",
      "        [-1.5704],\n",
      "        [-0.6512],\n",
      "        [-1.2738],\n",
      "        [-2.0703],\n",
      "        [-1.3793],\n",
      "        [-2.0625],\n",
      "        [-1.6662],\n",
      "        [-3.1389],\n",
      "        [-1.9157],\n",
      "        [-0.9734],\n",
      "        [-1.0059],\n",
      "        [-1.4733],\n",
      "        [-2.1839],\n",
      "        [-2.6279],\n",
      "        [-2.0774],\n",
      "        [-1.0007],\n",
      "        [-1.5100],\n",
      "        [-1.5675],\n",
      "        [-1.3853],\n",
      "        [-3.4992],\n",
      "        [-1.5925],\n",
      "        [-1.2506],\n",
      "        [-1.0859],\n",
      "        [-0.9023],\n",
      "        [-1.5351],\n",
      "        [-1.3086],\n",
      "        [-2.7864]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.4491],\n",
      "        [-1.1758],\n",
      "        [-0.9056],\n",
      "        [-0.6703],\n",
      "        [-1.1293],\n",
      "        [-0.2895],\n",
      "        [-1.5853],\n",
      "        [-1.3661],\n",
      "        [-0.9037],\n",
      "        [-1.5323],\n",
      "        [-2.0673],\n",
      "        [-1.1783],\n",
      "        [-1.3801],\n",
      "        [-3.4631],\n",
      "        [-0.4401],\n",
      "        [-0.3487],\n",
      "        [-1.0826],\n",
      "        [-3.2878],\n",
      "        [-2.0784],\n",
      "        [-1.5043],\n",
      "        [-0.1773],\n",
      "        [-2.9052],\n",
      "        [-1.0634],\n",
      "        [-1.6869],\n",
      "        [-0.6514],\n",
      "        [-1.3469],\n",
      "        [-1.7208],\n",
      "        [-0.6907],\n",
      "        [-1.7115],\n",
      "        [-1.1143],\n",
      "        [-3.4700],\n",
      "        [-1.7650],\n",
      "        [-0.8422],\n",
      "        [-0.9119],\n",
      "        [-1.1896],\n",
      "        [-2.2607],\n",
      "        [-2.7156],\n",
      "        [-1.8781],\n",
      "        [-0.9453],\n",
      "        [-1.5568],\n",
      "        [-1.7164],\n",
      "        [-1.4228],\n",
      "        [-3.4410],\n",
      "        [-1.6473],\n",
      "        [-1.4312],\n",
      "        [-1.5020],\n",
      "        [-0.7714],\n",
      "        [-1.2349],\n",
      "        [-1.0480],\n",
      "        [-3.2479]], device='cuda:0') tensor([[ 1.5095e-01],\n",
      "        [ 4.9935e-02],\n",
      "        [-2.7159e-01],\n",
      "        [-3.0725e-02],\n",
      "        [-3.1046e-01],\n",
      "        [-2.8001e-01],\n",
      "        [-2.7894e-01],\n",
      "        [-2.7134e-01],\n",
      "        [-9.2614e-02],\n",
      "        [ 1.8482e-01],\n",
      "        [ 3.1943e-01],\n",
      "        [ 4.9974e-02],\n",
      "        [-4.2216e-01],\n",
      "        [ 4.3677e-01],\n",
      "        [-4.3527e-01],\n",
      "        [-3.8188e-01],\n",
      "        [-2.0461e-01],\n",
      "        [ 4.4831e-01],\n",
      "        [ 1.3921e-01],\n",
      "        [-5.4443e-02],\n",
      "        [-5.0891e-02],\n",
      "        [ 4.2005e-01],\n",
      "        [-3.2007e-01],\n",
      "        [ 1.1645e-01],\n",
      "        [ 2.2829e-04],\n",
      "        [ 7.3145e-02],\n",
      "        [-3.4952e-01],\n",
      "        [-6.8854e-01],\n",
      "        [-3.5100e-01],\n",
      "        [-5.5194e-01],\n",
      "        [ 3.3107e-01],\n",
      "        [-1.5072e-01],\n",
      "        [-1.3124e-01],\n",
      "        [-9.4088e-02],\n",
      "        [-2.8371e-01],\n",
      "        [ 7.6825e-02],\n",
      "        [ 8.7696e-02],\n",
      "        [-1.9923e-01],\n",
      "        [-5.5440e-02],\n",
      "        [ 4.6869e-02],\n",
      "        [ 1.4897e-01],\n",
      "        [ 3.7526e-02],\n",
      "        [-5.8149e-02],\n",
      "        [ 5.4796e-02],\n",
      "        [ 1.8058e-01],\n",
      "        [ 4.1608e-01],\n",
      "        [-1.3083e-01],\n",
      "        [-3.0014e-01],\n",
      "        [-2.6052e-01],\n",
      "        [ 4.6149e-01]], device='cuda:0', grad_fn=<SubBackward0>) tensor(3.7886, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 4.389296\n",
      "Validation set: Average loss: 16.955530, Accuracy: 0/5384 (0%)\n",
      "\n",
      "996\n",
      "Epoch: 996\n",
      "tensor([[-0.6315],\n",
      "        [-1.2415],\n",
      "        [-0.6721],\n",
      "        [-0.8136],\n",
      "        [-1.0905],\n",
      "        [-0.2746],\n",
      "        [-1.5511],\n",
      "        [-0.9977],\n",
      "        [-1.1551],\n",
      "        [-1.1180],\n",
      "        [-2.1990],\n",
      "        [-1.3881],\n",
      "        [-1.3338],\n",
      "        [-3.1610],\n",
      "        [-0.6518],\n",
      "        [-0.4765],\n",
      "        [-0.9021],\n",
      "        [-2.6180],\n",
      "        [-2.4382],\n",
      "        [-1.4170],\n",
      "        [-0.4279],\n",
      "        [-2.7549],\n",
      "        [-1.5662],\n",
      "        [-1.6432],\n",
      "        [-0.6125],\n",
      "        [-1.2058],\n",
      "        [-1.9545],\n",
      "        [-0.8013],\n",
      "        [-1.5916],\n",
      "        [-1.3851],\n",
      "        [-2.7632],\n",
      "        [-1.4838],\n",
      "        [-0.7803],\n",
      "        [-1.2879],\n",
      "        [-1.1882],\n",
      "        [-2.2768],\n",
      "        [-3.1986],\n",
      "        [-1.6752],\n",
      "        [-0.6810],\n",
      "        [-1.7598],\n",
      "        [-1.8823],\n",
      "        [-1.4456],\n",
      "        [-3.6086],\n",
      "        [-2.0313],\n",
      "        [-1.6562],\n",
      "        [-1.9451],\n",
      "        [-0.9150],\n",
      "        [-1.3275],\n",
      "        [-1.6096],\n",
      "        [-3.6711]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.4491],\n",
      "        [-1.1758],\n",
      "        [-0.9056],\n",
      "        [-0.6703],\n",
      "        [-1.1293],\n",
      "        [-0.2895],\n",
      "        [-1.5853],\n",
      "        [-1.3661],\n",
      "        [-0.9037],\n",
      "        [-1.5323],\n",
      "        [-2.0673],\n",
      "        [-1.1783],\n",
      "        [-1.3801],\n",
      "        [-3.4631],\n",
      "        [-0.4401],\n",
      "        [-0.3487],\n",
      "        [-1.0826],\n",
      "        [-3.2878],\n",
      "        [-2.0784],\n",
      "        [-1.5043],\n",
      "        [-0.1773],\n",
      "        [-2.9052],\n",
      "        [-1.0634],\n",
      "        [-1.6869],\n",
      "        [-0.6514],\n",
      "        [-1.3469],\n",
      "        [-1.7208],\n",
      "        [-0.6907],\n",
      "        [-1.7115],\n",
      "        [-1.1143],\n",
      "        [-3.4700],\n",
      "        [-1.7650],\n",
      "        [-0.8422],\n",
      "        [-0.9119],\n",
      "        [-1.1896],\n",
      "        [-2.2607],\n",
      "        [-2.7156],\n",
      "        [-1.8781],\n",
      "        [-0.9453],\n",
      "        [-1.5568],\n",
      "        [-1.7164],\n",
      "        [-1.4228],\n",
      "        [-3.4410],\n",
      "        [-1.6473],\n",
      "        [-1.4312],\n",
      "        [-1.5020],\n",
      "        [-0.7714],\n",
      "        [-1.2349],\n",
      "        [-1.0480],\n",
      "        [-3.2479]], device='cuda:0') tensor([[ 0.8177],\n",
      "        [-0.0658],\n",
      "        [ 0.2335],\n",
      "        [-0.1433],\n",
      "        [ 0.0388],\n",
      "        [ 0.0149],\n",
      "        [ 0.0342],\n",
      "        [ 0.3685],\n",
      "        [-0.2514],\n",
      "        [ 0.4143],\n",
      "        [-0.1317],\n",
      "        [-0.2098],\n",
      "        [ 0.0463],\n",
      "        [ 0.3021],\n",
      "        [-0.2117],\n",
      "        [-0.1278],\n",
      "        [ 0.1805],\n",
      "        [ 0.6699],\n",
      "        [-0.3598],\n",
      "        [ 0.0873],\n",
      "        [-0.2505],\n",
      "        [ 0.1503],\n",
      "        [-0.5028],\n",
      "        [ 0.0437],\n",
      "        [ 0.0390],\n",
      "        [ 0.1411],\n",
      "        [-0.2338],\n",
      "        [-0.1105],\n",
      "        [ 0.1200],\n",
      "        [-0.2708],\n",
      "        [ 0.7068],\n",
      "        [ 0.2812],\n",
      "        [ 0.0619],\n",
      "        [-0.3761],\n",
      "        [ 0.0014],\n",
      "        [-0.0161],\n",
      "        [-0.4830],\n",
      "        [ 0.2029],\n",
      "        [ 0.2643],\n",
      "        [-0.2030],\n",
      "        [-0.1659],\n",
      "        [-0.0229],\n",
      "        [-0.1676],\n",
      "        [-0.3840],\n",
      "        [-0.2250],\n",
      "        [-0.4431],\n",
      "        [-0.1435],\n",
      "        [-0.0925],\n",
      "        [-0.5616],\n",
      "        [-0.4232]], device='cuda:0', grad_fn=<SubBackward0>) tensor(4.5559, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 4.391007\n",
      "Validation set: Average loss: 17.041272, Accuracy: 0/5384 (0%)\n",
      "\n",
      "997\n",
      "Epoch: 997\n",
      "tensor([[-1.3126],\n",
      "        [-1.1294],\n",
      "        [-0.7786],\n",
      "        [-0.9807],\n",
      "        [-0.5798],\n",
      "        [-0.2522],\n",
      "        [-1.4161],\n",
      "        [-1.2142],\n",
      "        [-1.0496],\n",
      "        [-1.7981],\n",
      "        [-2.0497],\n",
      "        [-1.3669],\n",
      "        [-1.5130],\n",
      "        [-3.9992],\n",
      "        [-0.7836],\n",
      "        [-0.6177],\n",
      "        [-1.3227],\n",
      "        [-3.2952],\n",
      "        [-2.4405],\n",
      "        [-1.5237],\n",
      "        [-1.4362],\n",
      "        [-2.9392],\n",
      "        [-0.9571],\n",
      "        [-1.6276],\n",
      "        [-0.6224],\n",
      "        [-1.6461],\n",
      "        [-1.4785],\n",
      "        [-0.2246],\n",
      "        [-1.5454],\n",
      "        [-1.6346],\n",
      "        [-2.9840],\n",
      "        [-1.8855],\n",
      "        [-1.0016],\n",
      "        [-1.3115],\n",
      "        [-1.5409],\n",
      "        [-2.3507],\n",
      "        [-2.3620],\n",
      "        [-2.2789],\n",
      "        [-0.8885],\n",
      "        [-1.9636],\n",
      "        [-1.6685],\n",
      "        [-1.3961],\n",
      "        [-3.3582],\n",
      "        [-1.6572],\n",
      "        [-1.3638],\n",
      "        [-1.4199],\n",
      "        [-0.6238],\n",
      "        [-1.2613],\n",
      "        [-1.0057],\n",
      "        [-2.4431]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.4491],\n",
      "        [-1.1758],\n",
      "        [-0.9056],\n",
      "        [-0.6703],\n",
      "        [-1.1293],\n",
      "        [-0.2895],\n",
      "        [-1.5853],\n",
      "        [-1.3661],\n",
      "        [-0.9037],\n",
      "        [-1.5323],\n",
      "        [-2.0673],\n",
      "        [-1.1783],\n",
      "        [-1.3801],\n",
      "        [-3.4631],\n",
      "        [-0.4401],\n",
      "        [-0.3487],\n",
      "        [-1.0826],\n",
      "        [-3.2878],\n",
      "        [-2.0784],\n",
      "        [-1.5043],\n",
      "        [-0.1773],\n",
      "        [-2.9052],\n",
      "        [-1.0634],\n",
      "        [-1.6869],\n",
      "        [-0.6514],\n",
      "        [-1.3469],\n",
      "        [-1.7208],\n",
      "        [-0.6907],\n",
      "        [-1.7115],\n",
      "        [-1.1143],\n",
      "        [-3.4700],\n",
      "        [-1.7650],\n",
      "        [-0.8422],\n",
      "        [-0.9119],\n",
      "        [-1.1896],\n",
      "        [-2.2607],\n",
      "        [-2.7156],\n",
      "        [-1.8781],\n",
      "        [-0.9453],\n",
      "        [-1.5568],\n",
      "        [-1.7164],\n",
      "        [-1.4228],\n",
      "        [-3.4410],\n",
      "        [-1.6473],\n",
      "        [-1.4312],\n",
      "        [-1.5020],\n",
      "        [-0.7714],\n",
      "        [-1.2349],\n",
      "        [-1.0480],\n",
      "        [-3.2479]], device='cuda:0') tensor([[ 0.1365],\n",
      "        [ 0.0464],\n",
      "        [ 0.1270],\n",
      "        [-0.3104],\n",
      "        [ 0.5494],\n",
      "        [ 0.0373],\n",
      "        [ 0.1692],\n",
      "        [ 0.1519],\n",
      "        [-0.1459],\n",
      "        [-0.2658],\n",
      "        [ 0.0176],\n",
      "        [-0.1886],\n",
      "        [-0.1329],\n",
      "        [-0.5361],\n",
      "        [-0.3435],\n",
      "        [-0.2690],\n",
      "        [-0.2401],\n",
      "        [-0.0073],\n",
      "        [-0.3622],\n",
      "        [-0.0194],\n",
      "        [-1.2589],\n",
      "        [-0.0340],\n",
      "        [ 0.1064],\n",
      "        [ 0.0593],\n",
      "        [ 0.0290],\n",
      "        [-0.2992],\n",
      "        [ 0.2422],\n",
      "        [ 0.4661],\n",
      "        [ 0.1662],\n",
      "        [-0.5203],\n",
      "        [ 0.4860],\n",
      "        [-0.1205],\n",
      "        [-0.1594],\n",
      "        [-0.3996],\n",
      "        [-0.3513],\n",
      "        [-0.0900],\n",
      "        [ 0.3536],\n",
      "        [-0.4008],\n",
      "        [ 0.0567],\n",
      "        [-0.4068],\n",
      "        [ 0.0479],\n",
      "        [ 0.0267],\n",
      "        [ 0.0828],\n",
      "        [-0.0099],\n",
      "        [ 0.0674],\n",
      "        [ 0.0821],\n",
      "        [ 0.1476],\n",
      "        [-0.0264],\n",
      "        [ 0.0423],\n",
      "        [ 0.8048]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.2810, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 4.498560\n",
      "Validation set: Average loss: 17.637666, Accuracy: 0/5384 (0%)\n",
      "\n",
      "998\n",
      "Epoch: 998\n",
      "tensor([[-1.8488],\n",
      "        [-0.4725],\n",
      "        [-0.9648],\n",
      "        [-1.2060],\n",
      "        [-1.1848],\n",
      "        [-0.2405],\n",
      "        [-1.5453],\n",
      "        [-1.0252],\n",
      "        [-0.7629],\n",
      "        [-1.5078],\n",
      "        [-1.9308],\n",
      "        [-1.2512],\n",
      "        [-1.0658],\n",
      "        [-2.7842],\n",
      "        [-0.9116],\n",
      "        [-0.5538],\n",
      "        [-1.5866],\n",
      "        [-3.0896],\n",
      "        [-1.8530],\n",
      "        [-1.5452],\n",
      "        [-0.2242],\n",
      "        [-2.7644],\n",
      "        [-1.0989],\n",
      "        [-1.8557],\n",
      "        [-0.7970],\n",
      "        [-1.3318],\n",
      "        [-1.3948],\n",
      "        [-1.3110],\n",
      "        [-1.7958],\n",
      "        [-0.7364],\n",
      "        [-3.9103],\n",
      "        [-1.5668],\n",
      "        [-0.4820],\n",
      "        [-0.7481],\n",
      "        [-1.1219],\n",
      "        [-2.6875],\n",
      "        [-2.5420],\n",
      "        [-1.7198],\n",
      "        [-0.8944],\n",
      "        [-1.6766],\n",
      "        [-1.5966],\n",
      "        [-1.2984],\n",
      "        [-2.7567],\n",
      "        [-1.5934],\n",
      "        [-1.2675],\n",
      "        [-0.8881],\n",
      "        [-0.8064],\n",
      "        [-1.3167],\n",
      "        [-1.2716],\n",
      "        [-2.6658]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.4491],\n",
      "        [-1.1758],\n",
      "        [-0.9056],\n",
      "        [-0.6703],\n",
      "        [-1.1293],\n",
      "        [-0.2895],\n",
      "        [-1.5853],\n",
      "        [-1.3661],\n",
      "        [-0.9037],\n",
      "        [-1.5323],\n",
      "        [-2.0673],\n",
      "        [-1.1783],\n",
      "        [-1.3801],\n",
      "        [-3.4631],\n",
      "        [-0.4401],\n",
      "        [-0.3487],\n",
      "        [-1.0826],\n",
      "        [-3.2878],\n",
      "        [-2.0784],\n",
      "        [-1.5043],\n",
      "        [-0.1773],\n",
      "        [-2.9052],\n",
      "        [-1.0634],\n",
      "        [-1.6869],\n",
      "        [-0.6514],\n",
      "        [-1.3469],\n",
      "        [-1.7208],\n",
      "        [-0.6907],\n",
      "        [-1.7115],\n",
      "        [-1.1143],\n",
      "        [-3.4700],\n",
      "        [-1.7650],\n",
      "        [-0.8422],\n",
      "        [-0.9119],\n",
      "        [-1.1896],\n",
      "        [-2.2607],\n",
      "        [-2.7156],\n",
      "        [-1.8781],\n",
      "        [-0.9453],\n",
      "        [-1.5568],\n",
      "        [-1.7164],\n",
      "        [-1.4228],\n",
      "        [-3.4410],\n",
      "        [-1.6473],\n",
      "        [-1.4312],\n",
      "        [-1.5020],\n",
      "        [-0.7714],\n",
      "        [-1.2349],\n",
      "        [-1.0480],\n",
      "        [-3.2479]], device='cuda:0') tensor([[-0.3997],\n",
      "        [ 0.7033],\n",
      "        [-0.0592],\n",
      "        [-0.5357],\n",
      "        [-0.0555],\n",
      "        [ 0.0491],\n",
      "        [ 0.0400],\n",
      "        [ 0.3409],\n",
      "        [ 0.1408],\n",
      "        [ 0.0245],\n",
      "        [ 0.1365],\n",
      "        [-0.0729],\n",
      "        [ 0.3143],\n",
      "        [ 0.6789],\n",
      "        [-0.4715],\n",
      "        [-0.2051],\n",
      "        [-0.5039],\n",
      "        [ 0.1982],\n",
      "        [ 0.2253],\n",
      "        [-0.0409],\n",
      "        [-0.0469],\n",
      "        [ 0.1408],\n",
      "        [-0.0355],\n",
      "        [-0.1688],\n",
      "        [-0.1455],\n",
      "        [ 0.0152],\n",
      "        [ 0.3260],\n",
      "        [-0.6203],\n",
      "        [-0.0843],\n",
      "        [ 0.3779],\n",
      "        [-0.4403],\n",
      "        [ 0.1981],\n",
      "        [ 0.3602],\n",
      "        [ 0.1638],\n",
      "        [ 0.0677],\n",
      "        [-0.4268],\n",
      "        [ 0.1737],\n",
      "        [ 0.1583],\n",
      "        [ 0.0509],\n",
      "        [-0.1197],\n",
      "        [ 0.1199],\n",
      "        [ 0.1244],\n",
      "        [ 0.6843],\n",
      "        [ 0.0539],\n",
      "        [ 0.1637],\n",
      "        [ 0.6139],\n",
      "        [-0.0350],\n",
      "        [-0.0818],\n",
      "        [-0.2236],\n",
      "        [ 0.5821]], device='cuda:0', grad_fn=<SubBackward0>) tensor(4.9465, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 4.395709\n",
      "Validation set: Average loss: 17.184376, Accuracy: 0/5384 (0%)\n",
      "\n",
      "999\n",
      "Epoch: 999\n",
      "tensor([[-1.5803],\n",
      "        [-1.6213],\n",
      "        [-0.7938],\n",
      "        [-0.7866],\n",
      "        [-1.0171],\n",
      "        [-0.2151],\n",
      "        [-1.4622],\n",
      "        [-1.4661],\n",
      "        [-0.7042],\n",
      "        [-1.5003],\n",
      "        [-2.0289],\n",
      "        [-0.6979],\n",
      "        [-1.2810],\n",
      "        [-3.0965],\n",
      "        [-0.4154],\n",
      "        [-0.4404],\n",
      "        [-1.4069],\n",
      "        [-2.3729],\n",
      "        [-2.3108],\n",
      "        [-1.1759],\n",
      "        [-0.2975],\n",
      "        [-2.7387],\n",
      "        [-1.3797],\n",
      "        [-1.6890],\n",
      "        [-0.4731],\n",
      "        [-1.1033],\n",
      "        [-1.4904],\n",
      "        [-1.0492],\n",
      "        [-1.5274],\n",
      "        [-1.4575],\n",
      "        [-3.1981],\n",
      "        [-1.5850],\n",
      "        [-0.6703],\n",
      "        [-1.1671],\n",
      "        [-1.3235],\n",
      "        [-2.1057],\n",
      "        [-2.6610],\n",
      "        [-1.7123],\n",
      "        [-0.8678],\n",
      "        [-1.3763],\n",
      "        [-1.9556],\n",
      "        [-1.8298],\n",
      "        [-2.6313],\n",
      "        [-1.7350],\n",
      "        [-1.5496],\n",
      "        [-1.6289],\n",
      "        [-0.9195],\n",
      "        [-1.3450],\n",
      "        [-1.5817],\n",
      "        [-3.3793]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.4491],\n",
      "        [-1.1758],\n",
      "        [-0.9056],\n",
      "        [-0.6703],\n",
      "        [-1.1293],\n",
      "        [-0.2895],\n",
      "        [-1.5853],\n",
      "        [-1.3661],\n",
      "        [-0.9037],\n",
      "        [-1.5323],\n",
      "        [-2.0673],\n",
      "        [-1.1783],\n",
      "        [-1.3801],\n",
      "        [-3.4631],\n",
      "        [-0.4401],\n",
      "        [-0.3487],\n",
      "        [-1.0826],\n",
      "        [-3.2878],\n",
      "        [-2.0784],\n",
      "        [-1.5043],\n",
      "        [-0.1773],\n",
      "        [-2.9052],\n",
      "        [-1.0634],\n",
      "        [-1.6869],\n",
      "        [-0.6514],\n",
      "        [-1.3469],\n",
      "        [-1.7208],\n",
      "        [-0.6907],\n",
      "        [-1.7115],\n",
      "        [-1.1143],\n",
      "        [-3.4700],\n",
      "        [-1.7650],\n",
      "        [-0.8422],\n",
      "        [-0.9119],\n",
      "        [-1.1896],\n",
      "        [-2.2607],\n",
      "        [-2.7156],\n",
      "        [-1.8781],\n",
      "        [-0.9453],\n",
      "        [-1.5568],\n",
      "        [-1.7164],\n",
      "        [-1.4228],\n",
      "        [-3.4410],\n",
      "        [-1.6473],\n",
      "        [-1.4312],\n",
      "        [-1.5020],\n",
      "        [-0.7714],\n",
      "        [-1.2349],\n",
      "        [-1.0480],\n",
      "        [-3.2479]], device='cuda:0') tensor([[-0.1311],\n",
      "        [-0.4456],\n",
      "        [ 0.1117],\n",
      "        [-0.1163],\n",
      "        [ 0.1121],\n",
      "        [ 0.0745],\n",
      "        [ 0.1231],\n",
      "        [-0.0999],\n",
      "        [ 0.1995],\n",
      "        [ 0.0320],\n",
      "        [ 0.0383],\n",
      "        [ 0.4804],\n",
      "        [ 0.0992],\n",
      "        [ 0.3667],\n",
      "        [ 0.0247],\n",
      "        [-0.0917],\n",
      "        [-0.3242],\n",
      "        [ 0.9150],\n",
      "        [-0.2324],\n",
      "        [ 0.3284],\n",
      "        [-0.1202],\n",
      "        [ 0.1665],\n",
      "        [-0.3162],\n",
      "        [-0.0021],\n",
      "        [ 0.1783],\n",
      "        [ 0.2436],\n",
      "        [ 0.2303],\n",
      "        [-0.3585],\n",
      "        [ 0.1841],\n",
      "        [-0.3432],\n",
      "        [ 0.2719],\n",
      "        [ 0.1800],\n",
      "        [ 0.1719],\n",
      "        [-0.2553],\n",
      "        [-0.1339],\n",
      "        [ 0.1549],\n",
      "        [ 0.0547],\n",
      "        [ 0.1658],\n",
      "        [ 0.0774],\n",
      "        [ 0.1805],\n",
      "        [-0.2392],\n",
      "        [-0.4070],\n",
      "        [ 0.8098],\n",
      "        [-0.0877],\n",
      "        [-0.1184],\n",
      "        [-0.1269],\n",
      "        [-0.1481],\n",
      "        [-0.1101],\n",
      "        [-0.5337],\n",
      "        [-0.1313]], device='cuda:0', grad_fn=<SubBackward0>) tensor(3.9466, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 4.547146\n",
      "Validation set: Average loss: 16.817605, Accuracy: 0/5384 (0%)\n",
      "\n",
      "1000\n",
      "Epoch: 1000\n",
      "tensor([[-1.4715],\n",
      "        [-1.5517],\n",
      "        [-1.1702],\n",
      "        [-0.7535],\n",
      "        [-1.3046],\n",
      "        [-0.1485],\n",
      "        [-1.4831],\n",
      "        [-1.3362],\n",
      "        [-0.8574],\n",
      "        [-1.4377],\n",
      "        [-2.2379],\n",
      "        [-1.7787],\n",
      "        [-1.3594],\n",
      "        [-3.7752],\n",
      "        [-1.2254],\n",
      "        [-0.4098],\n",
      "        [-1.3134],\n",
      "        [-4.2226],\n",
      "        [-2.1780],\n",
      "        [-1.1132],\n",
      "        [-0.2312],\n",
      "        [-2.6563],\n",
      "        [-0.8648],\n",
      "        [-1.3638],\n",
      "        [-0.6576],\n",
      "        [-1.6745],\n",
      "        [-1.4334],\n",
      "        [-0.6181],\n",
      "        [-1.8970],\n",
      "        [-1.1554],\n",
      "        [-3.2028],\n",
      "        [-1.9205],\n",
      "        [-1.2479],\n",
      "        [-1.6071],\n",
      "        [-1.3729],\n",
      "        [-2.3587],\n",
      "        [-2.2608],\n",
      "        [-1.5457],\n",
      "        [-1.1100],\n",
      "        [-1.6058],\n",
      "        [-1.4409],\n",
      "        [-1.9496],\n",
      "        [-3.4893],\n",
      "        [-2.0870],\n",
      "        [-1.5270],\n",
      "        [-0.9930],\n",
      "        [-0.6200],\n",
      "        [-1.3074],\n",
      "        [-1.2929],\n",
      "        [-2.9795]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-1.4491],\n",
      "        [-1.1758],\n",
      "        [-0.9056],\n",
      "        [-0.6703],\n",
      "        [-1.1293],\n",
      "        [-0.2895],\n",
      "        [-1.5853],\n",
      "        [-1.3661],\n",
      "        [-0.9037],\n",
      "        [-1.5323],\n",
      "        [-2.0673],\n",
      "        [-1.1783],\n",
      "        [-1.3801],\n",
      "        [-3.4631],\n",
      "        [-0.4401],\n",
      "        [-0.3487],\n",
      "        [-1.0826],\n",
      "        [-3.2878],\n",
      "        [-2.0784],\n",
      "        [-1.5043],\n",
      "        [-0.1773],\n",
      "        [-2.9052],\n",
      "        [-1.0634],\n",
      "        [-1.6869],\n",
      "        [-0.6514],\n",
      "        [-1.3469],\n",
      "        [-1.7208],\n",
      "        [-0.6907],\n",
      "        [-1.7115],\n",
      "        [-1.1143],\n",
      "        [-3.4700],\n",
      "        [-1.7650],\n",
      "        [-0.8422],\n",
      "        [-0.9119],\n",
      "        [-1.1896],\n",
      "        [-2.2607],\n",
      "        [-2.7156],\n",
      "        [-1.8781],\n",
      "        [-0.9453],\n",
      "        [-1.5568],\n",
      "        [-1.7164],\n",
      "        [-1.4228],\n",
      "        [-3.4410],\n",
      "        [-1.6473],\n",
      "        [-1.4312],\n",
      "        [-1.5020],\n",
      "        [-0.7714],\n",
      "        [-1.2349],\n",
      "        [-1.0480],\n",
      "        [-3.2479]], device='cuda:0') tensor([[-0.0224],\n",
      "        [-0.3760],\n",
      "        [-0.2646],\n",
      "        [-0.0832],\n",
      "        [-0.1753],\n",
      "        [ 0.1411],\n",
      "        [ 0.1022],\n",
      "        [ 0.0299],\n",
      "        [ 0.0463],\n",
      "        [ 0.0947],\n",
      "        [-0.1707],\n",
      "        [-0.6004],\n",
      "        [ 0.0208],\n",
      "        [-0.3120],\n",
      "        [-0.7853],\n",
      "        [-0.0610],\n",
      "        [-0.2308],\n",
      "        [-0.9348],\n",
      "        [-0.0997],\n",
      "        [ 0.3912],\n",
      "        [-0.0539],\n",
      "        [ 0.2489],\n",
      "        [ 0.1986],\n",
      "        [ 0.3231],\n",
      "        [-0.0061],\n",
      "        [-0.3276],\n",
      "        [ 0.2873],\n",
      "        [ 0.0727],\n",
      "        [-0.1854],\n",
      "        [-0.0411],\n",
      "        [ 0.2672],\n",
      "        [-0.1555],\n",
      "        [-0.4057],\n",
      "        [-0.6953],\n",
      "        [-0.1833],\n",
      "        [-0.0980],\n",
      "        [ 0.4549],\n",
      "        [ 0.3324],\n",
      "        [-0.1648],\n",
      "        [-0.0490],\n",
      "        [ 0.2756],\n",
      "        [-0.5268],\n",
      "        [-0.0483],\n",
      "        [-0.4397],\n",
      "        [-0.0958],\n",
      "        [ 0.5090],\n",
      "        [ 0.1515],\n",
      "        [-0.0724],\n",
      "        [-0.2449],\n",
      "        [ 0.2684]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.0411, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 4.356017\n",
      "Validation set: Average loss: 17.443029, Accuracy: 0/5384 (0%)\n",
      "\n",
      "43.866333083311716\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 1000\n",
    "print('Training on', device)\n",
    "time_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "time_end = time.time()\n",
    "total_time = (time_end-time_start)/60 \n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truths_train = [] \n",
    "all_preds_train = [] \n",
    "for (data,target) in train_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_train.append(target.cpu().detach().numpy())\n",
    "    all_preds_train.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_train=len(all_truths_train)-1\n",
    "\n",
    "remainder_train=len(all_truths_train[incomplete_batch_id_train])\n",
    "\n",
    "total_values_train=(len(all_truths_train)*batch_size)-(batch_size-remainder_train)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_train_array=np.zeros(total_values_train)\n",
    "all_preds_train_array=np.zeros(total_values_train)\n",
    "k=0\n",
    "while k < total_values_train:\n",
    "    for i in range(len(all_truths_train)):\n",
    "        if i<incomplete_batch_id_train:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_train\n",
    "            for j in range(remainder_train):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# all_truths_train=all_truths_train_array\n",
    "# all_preds_train=all_preds_train_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-3.5397881025174818\n[-3.3899632]\n-3.5397881025174818\n-3.389963150024414\n"
     ]
    }
   ],
   "source": [
    "all_truths_test = [] \n",
    "all_preds_test = [] \n",
    "for (data,target) in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_test.append(target.cpu().detach().numpy())\n",
    "    all_preds_test.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_test=len(all_truths_test)-1\n",
    "\n",
    "remainder_test=len(all_truths_test[incomplete_batch_id_test])\n",
    "\n",
    "\n",
    "total_values_test=(len(all_truths_test)*batch_size)-(batch_size-remainder_test)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_test_array=np.zeros(total_values_test)\n",
    "all_preds_test_array=np.zeros(total_values_test)\n",
    "k=0\n",
    "while k < total_values_test:\n",
    "    for i in range(len(all_truths_test)):\n",
    "        if i<incomplete_batch_id_test:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_test\n",
    "            for j in range(remainder_test):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "print(all_truths_test[3][43])\n",
    "print(all_preds_test[3][43])\n",
    "print(all_truths_test_array[193])\n",
    "print(all_preds_test_array[193])\n",
    "# all_truths_test=all_truths_test_array\n",
    "# all_preds_test=all_preds_test_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train=r2_score(all_truths_train_array,all_preds_train_array)\n",
    "r2_test=r2_score(all_truths_test_array,all_preds_test_array)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1) \n",
    "plt.scatter(all_truths_test_array, all_preds_test_array,color = 'b', alpha = 0.1*7/3)\n",
    "plt.xlabel('Test True Values')\n",
    "plt.ylabel('Test Predicted Value')\n",
    "plt.text(-1,-5,str(r2_test))\n",
    "plt.plot([-5,1],[-5,1],'r--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(all_truths_train_array, all_preds_train_array,color = 'k', alpha = 0.1)\n",
    "plt.xlabel('Train True Values')\n",
    "plt.ylabel('Train Predicted Value')\n",
    "plt.plot([-5,1],[-5,1],'r--') \n",
    "plt.text(-1,-5,str(r2_train))\n",
    "plt.show()\n",
    "plt.savefig('/home/juanp/Documents/SURP-2021/Plots/Model 3/5 Copies of Images.pdf')"
   ]
  },
  {
   "source": [
    "# Loss as a Function of Epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.plot(epoch_nums, np.log10(training_loss))\n",
    "# plt.plot(epoch_nums, np.log10(validation_loss))\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('Log of loss')\n",
    "# plt.legend(['training', 'validation'], loc='upper right')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Plotting Galaxy Images and their True SFR vs CNN Predicted SFR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(picture[0][0,0:,0:])\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(picture[1][0,0:,0:])\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(picture[2][0,0:,0:])\n",
    "#     plt.show()"
   ]
  }
 ]
}