{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
   }
  },
  "interpreter": {
   "hash": "332de2e570cd2f6fcfd1dc3718047bca654fabf3a28e9f4985b0d5046bfd1195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 3: Predict Star Formation Variables (sSFR, SFR, M*, age) Based on Visual Morphology (galaxy image)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[0;34m[INFO]: \u001b[0mNo release version set. Setting default to DR15\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/redux/v2_4_3/drpall-v2_4_3.fits cannot be found. Setting drpall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n",
      "\u001b[1;33m[WARNING]: \u001b[0m\u001b[0;39mpath /home/juanp/sas/mangawork/manga/spectro/analysis/v2_4_3/2.2.1/dapall-v2_4_3-2.2.1.fits cannot be found. Setting dapall to None.\u001b[0m \u001b[0;36m(MarvinUserWarning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Loading needed modules and classes/functions \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import decode_image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import marvin\n",
    "from marvin.tools.maps import Maps\n",
    "from marvin.tools.image import Image\n",
    "from marvin.utils.general.images import get_images_by_list\n",
    "from marvin import config\n",
    "from marvin.tools.cube import Cube\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image as image_PIL\n",
    "import time \n",
    "\n",
    "#set config attributes and turn on global downloads of Marvin data\n",
    "config.setRelease('DR15')\n",
    "config.mode = 'local'\n",
    "config.download = True\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "#3 Linear layers NN, 1 hidden \n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.linear1 = torch.nn.Linear(outputSize, outputSize)\n",
    "        self.ReLU= torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "source": [
    "# Importing Data from Schema Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('CompleteTable.csv')  #Importing All MaNGA Data from DPRall Schema\n",
    "\n",
    "galaxy_list=np.loadtxt('Query Results',dtype=str) #Pulling Manga ID's of galaxies which satisfy log(M) > 9 and 0 < z < 0.1\n",
    "\n",
    "\n",
    "#Problem with image associated with manga id at galaxy_list[3548], mangaid- 1-135668\n",
    "galaxy_list=np.delete(galaxy_list,3548)\n",
    "\n",
    "galaxy_list=np.unique(galaxy_list)\n",
    "\n",
    "\n",
    "galaxy_index=np.zeros(len(galaxy_list)) \n",
    "for i in range (len(galaxy_list)): #Getting the index of these galaxies in the schema table\n",
    "    galaxy_index[i]=np.where(data.loc[:,'mangaid']==galaxy_list[i])[0][0]\n",
    "\n",
    "galaxy_index=np.array(galaxy_index,dtype=int) #Ensuring we have array that can be used to index, force int \n",
    "\n",
    "galaxies=data.iloc[galaxy_index] #DF of galaxies which satisfies the condition, contains all relevant schema data \n",
    "\n",
    "galaxies=galaxies.sort_values(by=['plateifu']) #Sorting galaxies by plateifu to match ImageFolder Output \n",
    "\n",
    "#Creating the arrays of the independent variables were are interested in, and dependent variable n \n",
    "\n",
    "mass=galaxies.loc[:,'nsa_sersic_mass']\n",
    "log_mass=np.log10(mass)\n",
    "\n",
    "SFR=galaxies.loc[:,'sfr_tot']\n",
    "log_SFR=np.log10(SFR)\n",
    "\n",
    "ha_flux=galaxies.loc[:,'emline_gflux_tot_ha_6564']\n",
    "\n",
    "n=galaxies.loc[:,'nsa_sersic_n']\n",
    "n=np.array(n,dtype=np.float32)\n",
    "n=torch.from_numpy(n).to('cuda:0').reshape(-1,1)\n"
   ]
  },
  {
   "source": [
    "# Importing Images from their Downloaded Locations \n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_locations=[]\n",
    "# for i in range (len(galaxy_list)):\n",
    "#     image_locations.append(Image(galaxy_list[i]).filename)\n",
    "    \n",
    "# image_locations=np.array(image_locations,dtype=str)\n",
    "# np.savetxt('Image Directories',image_locations,fmt='%s')\n",
    "\n",
    "image_locations=np.loadtxt('Image Directories',dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n"
   ]
  },
  {
   "source": [
    "# Putting the Images into DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "71780\nData loaders ready to read /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n"
     ]
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "\n",
    "# image=ImageFolder('/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/') #Picks up 3590 pictures, directory list has lenght 3637 however \n",
    "\n",
    "image_directory='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3'\n",
    "classes= sorted(os.listdir(image_directory))\n",
    "\n",
    "batch_size=50\n",
    "image_copies=20\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        #Rotates the image by some angle\n",
    "        # transforms.RandomRotation(0,360),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        # transform=transformation\n",
    "    )\n",
    "    \n",
    "    #This loop transforms the images and assigns them the correct label \n",
    "\n",
    "    full_dataset_v2 = []   \n",
    "    for i in range(len(full_dataset)): \n",
    "        for j in range(image_copies): #This loops makes it so we have 5 copies of each galaxy image with different orientations \n",
    "            temp=transformation(resize_image(full_dataset[i][0]))\n",
    "            full_dataset_v2.append((temp,log_SFR.iloc[i])) \n",
    "    \n",
    "    full_dataset=full_dataset_v2\n",
    "\n",
    "    print(len(full_dataset))\n",
    "\n",
    "\n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(image_directory)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset ImageFolder\n    Number of datapoints: 3589\n    Root location: /home/juanp/sas/dr15/manga/spectro/redux/v2_4_3\n3637\n3589\n3589\n"
     ]
    }
   ],
   "source": [
    "print(ImageFolder(image_directory))\n",
    "print(len(image_locations))\n",
    "print(len(np.unique(image_locations)))\n",
    "print(len(np.unique(galaxy_list)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_dataset = torchvision.datasets.ImageFolder(\n",
    "#         root='/home/juanp/sas/dr15/manga/spectro/redux/v2_4_3/7968',\n",
    "#         # transform=transformation\n",
    "#     ) \n",
    "\n",
    "# full_dataset_v2 = []   \n",
    "# for i in range(len(full_dataset)): \n",
    "#     full_dataset_v2.append((resize_image(full_dataset[i][0]) , i)) \n",
    "\n",
    "# print(full_dataset_v2)\n",
    "\n",
    "\n",
    "# print(full_dataset[0][1])\n",
    "# for i in range(len(full_dataset)): \n",
    "#         full_dataset[i] = (resize_image(full_dataset[i][0]) , 0) \n",
    "        # full_dataset[i][0] = resize_image(full_dataset[i][0])\n",
    "        # full_dataset[i][1] = correct_label(correct_manga_id) "
   ]
  },
  {
   "source": [
    "# Defining the Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (drop): Dropout2d(p=0.2, inplace=False)\n  (fc): Linear(in_features=24576, out_features=10, bias=True)\n  (fc1): Linear(in_features=10, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 128 x 128, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=10)\n",
    "\n",
    "        self.fc1= nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x+1\n",
    "        \n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # print(x)\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x=self.conv1(x)\n",
    "        \n",
    "        x=self.pool(x)\n",
    "       \n",
    "\n",
    "        x=F.relu(x)\n",
    "\n",
    "        \n",
    "      \n",
    " \n",
    "     \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x))) \n",
    "        \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return class probabilities via a log_softmax function\n",
    "        x=F.relu(x) \n",
    "        \n",
    "        x= self.fc1(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(SFR)).to('cuda')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "# Creating Training Loop/Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target=target.float()\n",
    "        # print(target.shape)\n",
    "        target=target.reshape(-1,1)\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "    \n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        if batch_idx==0:\n",
    "            print(output,target,output-target,loss)\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Create Test Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            target=target.float()\n",
    "            target=target.reshape(-1,1)\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "source": [
    "# Training the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1613],\n",
      "        [-0.2496],\n",
      "        [-0.2612],\n",
      "        [ 0.3380],\n",
      "        [-0.0416],\n",
      "        [-0.2912],\n",
      "        [ 0.2952],\n",
      "        [ 0.4316],\n",
      "        [ 0.7960],\n",
      "        [-0.1519],\n",
      "        [ 0.1584],\n",
      "        [-0.2839],\n",
      "        [ 0.1669],\n",
      "        [ 0.1386],\n",
      "        [-0.4176],\n",
      "        [-0.2279],\n",
      "        [-0.6554],\n",
      "        [-0.5408],\n",
      "        [-0.1150],\n",
      "        [ 0.6940],\n",
      "        [-0.1656],\n",
      "        [-0.0349],\n",
      "        [ 0.0529],\n",
      "        [-0.0462],\n",
      "        [ 0.7912],\n",
      "        [-0.1153],\n",
      "        [ 0.7800],\n",
      "        [-0.0568],\n",
      "        [ 0.1596],\n",
      "        [-0.3084],\n",
      "        [ 0.2609],\n",
      "        [-0.2714],\n",
      "        [ 0.1863],\n",
      "        [-0.2556],\n",
      "        [-0.2451],\n",
      "        [ 0.5890],\n",
      "        [ 0.3585],\n",
      "        [ 0.2358],\n",
      "        [-0.3229],\n",
      "        [ 0.5300],\n",
      "        [-0.0865],\n",
      "        [ 0.0308],\n",
      "        [ 0.0535]], device='cuda:0', grad_fn=<SubBackward0>) tensor(6.6155, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 8.181372\n",
      "Validation set: Average loss: 4.974257, Accuracy: 0/21534 (0%)\n",
      "\n",
      "995\n",
      "Epoch: 995\n",
      "tensor([[-0.2686],\n",
      "        [-2.1265],\n",
      "        [-1.7064],\n",
      "        [-0.7488],\n",
      "        [-2.2663],\n",
      "        [-1.3012],\n",
      "        [-2.9595],\n",
      "        [-1.5700],\n",
      "        [-0.8964],\n",
      "        [-0.6444],\n",
      "        [-1.4039],\n",
      "        [-1.2724],\n",
      "        [-3.3123],\n",
      "        [-1.5616],\n",
      "        [-1.4561],\n",
      "        [-3.8409],\n",
      "        [-0.9365],\n",
      "        [-1.4312],\n",
      "        [-1.5743],\n",
      "        [-1.3715],\n",
      "        [-1.5001],\n",
      "        [-0.4770],\n",
      "        [-0.7655],\n",
      "        [-0.5914],\n",
      "        [-1.6398],\n",
      "        [-0.7620],\n",
      "        [-2.0945],\n",
      "        [-1.1111],\n",
      "        [-2.4439],\n",
      "        [-3.1912],\n",
      "        [-1.5674],\n",
      "        [-1.9094],\n",
      "        [-1.4174],\n",
      "        [-1.5873],\n",
      "        [-0.6166],\n",
      "        [-3.3525],\n",
      "        [-2.1158],\n",
      "        [-0.9908],\n",
      "        [-0.7716],\n",
      "        [-1.8002],\n",
      "        [-1.1923],\n",
      "        [-0.8906],\n",
      "        [-1.7874],\n",
      "        [-3.4324],\n",
      "        [-0.7957],\n",
      "        [-0.6391],\n",
      "        [-1.2743],\n",
      "        [-2.7672],\n",
      "        [-1.9712],\n",
      "        [-0.4728]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-0.0436],\n",
      "        [-1.3889],\n",
      "        [-1.8150],\n",
      "        [-0.2121],\n",
      "        [-2.1525],\n",
      "        [-1.0724],\n",
      "        [-2.3127],\n",
      "        [-2.3005],\n",
      "        [-0.6907],\n",
      "        [-0.4143],\n",
      "        [-1.3152],\n",
      "        [-1.1758],\n",
      "        [-3.3412],\n",
      "        [-1.6687],\n",
      "        [-2.0037],\n",
      "        [-3.6969],\n",
      "        [-0.8460],\n",
      "        [-2.2386],\n",
      "        [-1.6707],\n",
      "        [-1.5945],\n",
      "        [-1.5640],\n",
      "        [-0.3255],\n",
      "        [-0.6086],\n",
      "        [-0.2274],\n",
      "        [-1.9239],\n",
      "        [-0.8385],\n",
      "        [-1.9431],\n",
      "        [-0.9211],\n",
      "        [-2.9656],\n",
      "        [-3.0667],\n",
      "        [-1.9170],\n",
      "        [-1.6429],\n",
      "        [-1.9329],\n",
      "        [-1.6962],\n",
      "        [-0.9630],\n",
      "        [-3.2051],\n",
      "        [-2.3799],\n",
      "        [-0.8019],\n",
      "        [-0.4984],\n",
      "        [-1.8632],\n",
      "        [-0.6907],\n",
      "        [-0.9290],\n",
      "        [-2.6954],\n",
      "        [-2.9052],\n",
      "        [-0.8422],\n",
      "        [-0.2552],\n",
      "        [-1.3505],\n",
      "        [-3.7199],\n",
      "        [-2.2621],\n",
      "        [-0.1851]], device='cuda:0') tensor([[-0.2250],\n",
      "        [-0.7376],\n",
      "        [ 0.1086],\n",
      "        [-0.5367],\n",
      "        [-0.1138],\n",
      "        [-0.2289],\n",
      "        [-0.6468],\n",
      "        [ 0.7304],\n",
      "        [-0.2057],\n",
      "        [-0.2301],\n",
      "        [-0.0887],\n",
      "        [-0.0966],\n",
      "        [ 0.0288],\n",
      "        [ 0.1072],\n",
      "        [ 0.5476],\n",
      "        [-0.1441],\n",
      "        [-0.0906],\n",
      "        [ 0.8075],\n",
      "        [ 0.0964],\n",
      "        [ 0.2230],\n",
      "        [ 0.0639],\n",
      "        [-0.1516],\n",
      "        [-0.1569],\n",
      "        [-0.3640],\n",
      "        [ 0.2841],\n",
      "        [ 0.0765],\n",
      "        [-0.1513],\n",
      "        [-0.1900],\n",
      "        [ 0.5217],\n",
      "        [-0.1244],\n",
      "        [ 0.3496],\n",
      "        [-0.2664],\n",
      "        [ 0.5155],\n",
      "        [ 0.1090],\n",
      "        [ 0.3464],\n",
      "        [-0.1474],\n",
      "        [ 0.2641],\n",
      "        [-0.1889],\n",
      "        [-0.2732],\n",
      "        [ 0.0631],\n",
      "        [-0.5016],\n",
      "        [ 0.0384],\n",
      "        [ 0.9080],\n",
      "        [-0.5272],\n",
      "        [ 0.0465],\n",
      "        [-0.3838],\n",
      "        [ 0.0761],\n",
      "        [ 0.9527],\n",
      "        [ 0.2910],\n",
      "        [-0.2877]], device='cuda:0', grad_fn=<SubBackward0>) tensor(7.0761, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 8.309526\n",
      "Validation set: Average loss: 4.790121, Accuracy: 0/21534 (0%)\n",
      "\n",
      "996\n",
      "Epoch: 996\n",
      "tensor([[-0.1348],\n",
      "        [-1.3363],\n",
      "        [-1.5844],\n",
      "        [-0.4883],\n",
      "        [-2.5730],\n",
      "        [-0.6350],\n",
      "        [-2.3220],\n",
      "        [-3.4878],\n",
      "        [-0.7687],\n",
      "        [-0.9341],\n",
      "        [-1.4057],\n",
      "        [-0.9671],\n",
      "        [-3.2419],\n",
      "        [-1.5614],\n",
      "        [-1.3322],\n",
      "        [-3.3965],\n",
      "        [-1.0934],\n",
      "        [-2.6870],\n",
      "        [-0.8843],\n",
      "        [-1.1376],\n",
      "        [-1.6712],\n",
      "        [-0.7620],\n",
      "        [-1.1201],\n",
      "        [-0.4207],\n",
      "        [-1.8974],\n",
      "        [-0.9448],\n",
      "        [-1.5135],\n",
      "        [-0.8392],\n",
      "        [-2.4673],\n",
      "        [-1.8786],\n",
      "        [-1.1489],\n",
      "        [-1.5002],\n",
      "        [-1.6611],\n",
      "        [-1.4718],\n",
      "        [-0.8054],\n",
      "        [-2.2259],\n",
      "        [-1.5491],\n",
      "        [-0.9690],\n",
      "        [-0.4943],\n",
      "        [-1.9900],\n",
      "        [-0.9078],\n",
      "        [-1.0049],\n",
      "        [-2.1764],\n",
      "        [-2.7867],\n",
      "        [-0.5614],\n",
      "        [-0.6456],\n",
      "        [-0.4770],\n",
      "        [-3.9263],\n",
      "        [-2.3513],\n",
      "        [-0.1348]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-0.0436],\n",
      "        [-1.3889],\n",
      "        [-1.8150],\n",
      "        [-0.2121],\n",
      "        [-2.1525],\n",
      "        [-1.0724],\n",
      "        [-2.3127],\n",
      "        [-2.3005],\n",
      "        [-0.6907],\n",
      "        [-0.4143],\n",
      "        [-1.3152],\n",
      "        [-1.1758],\n",
      "        [-3.3412],\n",
      "        [-1.6687],\n",
      "        [-2.0037],\n",
      "        [-3.6969],\n",
      "        [-0.8460],\n",
      "        [-2.2386],\n",
      "        [-1.6707],\n",
      "        [-1.5945],\n",
      "        [-1.5640],\n",
      "        [-0.3255],\n",
      "        [-0.6086],\n",
      "        [-0.2274],\n",
      "        [-1.9239],\n",
      "        [-0.8385],\n",
      "        [-1.9431],\n",
      "        [-0.9211],\n",
      "        [-2.9656],\n",
      "        [-3.0667],\n",
      "        [-1.9170],\n",
      "        [-1.6429],\n",
      "        [-1.9329],\n",
      "        [-1.6962],\n",
      "        [-0.9630],\n",
      "        [-3.2051],\n",
      "        [-2.3799],\n",
      "        [-0.8019],\n",
      "        [-0.4984],\n",
      "        [-1.8632],\n",
      "        [-0.6907],\n",
      "        [-0.9290],\n",
      "        [-2.6954],\n",
      "        [-2.9052],\n",
      "        [-0.8422],\n",
      "        [-0.2552],\n",
      "        [-1.3505],\n",
      "        [-3.7199],\n",
      "        [-2.2621],\n",
      "        [-0.1851]], device='cuda:0') tensor([[-0.0912],\n",
      "        [ 0.0526],\n",
      "        [ 0.2306],\n",
      "        [-0.2762],\n",
      "        [-0.4205],\n",
      "        [ 0.4374],\n",
      "        [-0.0093],\n",
      "        [-1.1874],\n",
      "        [-0.0780],\n",
      "        [-0.5198],\n",
      "        [-0.0905],\n",
      "        [ 0.2086],\n",
      "        [ 0.0993],\n",
      "        [ 0.1074],\n",
      "        [ 0.6715],\n",
      "        [ 0.3004],\n",
      "        [-0.2474],\n",
      "        [-0.4483],\n",
      "        [ 0.7864],\n",
      "        [ 0.4569],\n",
      "        [-0.1073],\n",
      "        [-0.4366],\n",
      "        [-0.5115],\n",
      "        [-0.1933],\n",
      "        [ 0.0265],\n",
      "        [-0.1063],\n",
      "        [ 0.4297],\n",
      "        [ 0.0819],\n",
      "        [ 0.4983],\n",
      "        [ 1.1882],\n",
      "        [ 0.7681],\n",
      "        [ 0.1427],\n",
      "        [ 0.2718],\n",
      "        [ 0.2244],\n",
      "        [ 0.1576],\n",
      "        [ 0.9792],\n",
      "        [ 0.8307],\n",
      "        [-0.1671],\n",
      "        [ 0.0041],\n",
      "        [-0.1268],\n",
      "        [-0.2170],\n",
      "        [-0.0759],\n",
      "        [ 0.5190],\n",
      "        [ 0.1185],\n",
      "        [ 0.2808],\n",
      "        [-0.3904],\n",
      "        [ 0.8735],\n",
      "        [-0.2063],\n",
      "        [-0.0892],\n",
      "        [ 0.0503]], device='cuda:0', grad_fn=<SubBackward0>) tensor(10.0993, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 8.199297\n",
      "Validation set: Average loss: 4.847104, Accuracy: 0/21534 (0%)\n",
      "\n",
      "997\n",
      "Epoch: 997\n",
      "tensor([[-0.4235],\n",
      "        [-0.9596],\n",
      "        [-1.8455],\n",
      "        [-0.7538],\n",
      "        [-1.8114],\n",
      "        [-1.0566],\n",
      "        [-2.1730],\n",
      "        [-1.9878],\n",
      "        [-0.7035],\n",
      "        [-0.4694],\n",
      "        [-1.2819],\n",
      "        [-0.8823],\n",
      "        [-2.7043],\n",
      "        [-1.4468],\n",
      "        [-1.3873],\n",
      "        [-3.7700],\n",
      "        [-0.7634],\n",
      "        [-2.8138],\n",
      "        [-1.9507],\n",
      "        [-2.1818],\n",
      "        [-1.2736],\n",
      "        [-0.2552],\n",
      "        [-0.5405],\n",
      "        [-0.3332],\n",
      "        [-2.3024],\n",
      "        [-1.1306],\n",
      "        [-1.9923],\n",
      "        [-1.9691],\n",
      "        [-3.1258],\n",
      "        [-2.3694],\n",
      "        [-1.9279],\n",
      "        [-2.1040],\n",
      "        [-1.5337],\n",
      "        [-1.7700],\n",
      "        [-0.6542],\n",
      "        [-3.6257],\n",
      "        [-2.4797],\n",
      "        [-0.7278],\n",
      "        [-0.5725],\n",
      "        [-2.2198],\n",
      "        [-0.7646],\n",
      "        [-1.2703],\n",
      "        [-2.9645],\n",
      "        [-2.0271],\n",
      "        [-1.1221],\n",
      "        [-0.7621],\n",
      "        [-1.5635],\n",
      "        [-3.5246],\n",
      "        [-2.3171],\n",
      "        [-0.6215]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-0.0436],\n",
      "        [-1.3889],\n",
      "        [-1.8150],\n",
      "        [-0.2121],\n",
      "        [-2.1525],\n",
      "        [-1.0724],\n",
      "        [-2.3127],\n",
      "        [-2.3005],\n",
      "        [-0.6907],\n",
      "        [-0.4143],\n",
      "        [-1.3152],\n",
      "        [-1.1758],\n",
      "        [-3.3412],\n",
      "        [-1.6687],\n",
      "        [-2.0037],\n",
      "        [-3.6969],\n",
      "        [-0.8460],\n",
      "        [-2.2386],\n",
      "        [-1.6707],\n",
      "        [-1.5945],\n",
      "        [-1.5640],\n",
      "        [-0.3255],\n",
      "        [-0.6086],\n",
      "        [-0.2274],\n",
      "        [-1.9239],\n",
      "        [-0.8385],\n",
      "        [-1.9431],\n",
      "        [-0.9211],\n",
      "        [-2.9656],\n",
      "        [-3.0667],\n",
      "        [-1.9170],\n",
      "        [-1.6429],\n",
      "        [-1.9329],\n",
      "        [-1.6962],\n",
      "        [-0.9630],\n",
      "        [-3.2051],\n",
      "        [-2.3799],\n",
      "        [-0.8019],\n",
      "        [-0.4984],\n",
      "        [-1.8632],\n",
      "        [-0.6907],\n",
      "        [-0.9290],\n",
      "        [-2.6954],\n",
      "        [-2.9052],\n",
      "        [-0.8422],\n",
      "        [-0.2552],\n",
      "        [-1.3505],\n",
      "        [-3.7199],\n",
      "        [-2.2621],\n",
      "        [-0.1851]], device='cuda:0') tensor([[-0.3798],\n",
      "        [ 0.4293],\n",
      "        [-0.0305],\n",
      "        [-0.5417],\n",
      "        [ 0.3411],\n",
      "        [ 0.0158],\n",
      "        [ 0.1398],\n",
      "        [ 0.3127],\n",
      "        [-0.0128],\n",
      "        [-0.0551],\n",
      "        [ 0.0333],\n",
      "        [ 0.2935],\n",
      "        [ 0.6368],\n",
      "        [ 0.2220],\n",
      "        [ 0.6163],\n",
      "        [-0.0732],\n",
      "        [ 0.0826],\n",
      "        [-0.5752],\n",
      "        [-0.2800],\n",
      "        [-0.5873],\n",
      "        [ 0.2904],\n",
      "        [ 0.0703],\n",
      "        [ 0.0680],\n",
      "        [-0.1058],\n",
      "        [-0.3785],\n",
      "        [-0.2921],\n",
      "        [-0.0491],\n",
      "        [-1.0480],\n",
      "        [-0.1602],\n",
      "        [ 0.6973],\n",
      "        [-0.0109],\n",
      "        [-0.4611],\n",
      "        [ 0.3992],\n",
      "        [-0.0738],\n",
      "        [ 0.3088],\n",
      "        [-0.4206],\n",
      "        [-0.0998],\n",
      "        [ 0.0741],\n",
      "        [-0.0741],\n",
      "        [-0.3565],\n",
      "        [-0.0739],\n",
      "        [-0.3413],\n",
      "        [-0.2691],\n",
      "        [ 0.8781],\n",
      "        [-0.2799],\n",
      "        [-0.5069],\n",
      "        [-0.2131],\n",
      "        [ 0.1953],\n",
      "        [-0.0550],\n",
      "        [-0.4364]], device='cuda:0', grad_fn=<SubBackward0>) tensor(6.8703, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 8.221397\n",
      "Validation set: Average loss: 5.111198, Accuracy: 0/21534 (0%)\n",
      "\n",
      "998\n",
      "Epoch: 998\n",
      "tensor([[-0.1737],\n",
      "        [-1.2095],\n",
      "        [-1.6997],\n",
      "        [-0.3358],\n",
      "        [-1.9711],\n",
      "        [-0.8168],\n",
      "        [-1.8920],\n",
      "        [-1.7650],\n",
      "        [-0.8666],\n",
      "        [-0.8057],\n",
      "        [-1.8315],\n",
      "        [-1.3006],\n",
      "        [-3.3907],\n",
      "        [-1.3296],\n",
      "        [-1.9939],\n",
      "        [-3.3230],\n",
      "        [-0.9097],\n",
      "        [-1.4744],\n",
      "        [-1.7107],\n",
      "        [-1.0096],\n",
      "        [-2.0203],\n",
      "        [-0.7940],\n",
      "        [-1.1045],\n",
      "        [-0.7824],\n",
      "        [-1.5230],\n",
      "        [-0.8760],\n",
      "        [-1.6702],\n",
      "        [-0.7083],\n",
      "        [-2.8727],\n",
      "        [-3.4361],\n",
      "        [-2.2500],\n",
      "        [-1.8809],\n",
      "        [-1.9052],\n",
      "        [-1.4834],\n",
      "        [-1.1253],\n",
      "        [-3.7994],\n",
      "        [-1.9422],\n",
      "        [-0.8489],\n",
      "        [-0.6007],\n",
      "        [-1.5414],\n",
      "        [-0.7633],\n",
      "        [-1.2976],\n",
      "        [-2.2815],\n",
      "        [-2.5598],\n",
      "        [-0.4760],\n",
      "        [-0.4172],\n",
      "        [-1.1551],\n",
      "        [-3.1912],\n",
      "        [-1.9622],\n",
      "        [-0.2130]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-0.0436],\n",
      "        [-1.3889],\n",
      "        [-1.8150],\n",
      "        [-0.2121],\n",
      "        [-2.1525],\n",
      "        [-1.0724],\n",
      "        [-2.3127],\n",
      "        [-2.3005],\n",
      "        [-0.6907],\n",
      "        [-0.4143],\n",
      "        [-1.3152],\n",
      "        [-1.1758],\n",
      "        [-3.3412],\n",
      "        [-1.6687],\n",
      "        [-2.0037],\n",
      "        [-3.6969],\n",
      "        [-0.8460],\n",
      "        [-2.2386],\n",
      "        [-1.6707],\n",
      "        [-1.5945],\n",
      "        [-1.5640],\n",
      "        [-0.3255],\n",
      "        [-0.6086],\n",
      "        [-0.2274],\n",
      "        [-1.9239],\n",
      "        [-0.8385],\n",
      "        [-1.9431],\n",
      "        [-0.9211],\n",
      "        [-2.9656],\n",
      "        [-3.0667],\n",
      "        [-1.9170],\n",
      "        [-1.6429],\n",
      "        [-1.9329],\n",
      "        [-1.6962],\n",
      "        [-0.9630],\n",
      "        [-3.2051],\n",
      "        [-2.3799],\n",
      "        [-0.8019],\n",
      "        [-0.4984],\n",
      "        [-1.8632],\n",
      "        [-0.6907],\n",
      "        [-0.9290],\n",
      "        [-2.6954],\n",
      "        [-2.9052],\n",
      "        [-0.8422],\n",
      "        [-0.2552],\n",
      "        [-1.3505],\n",
      "        [-3.7199],\n",
      "        [-2.2621],\n",
      "        [-0.1851]], device='cuda:0') tensor([[-0.1301],\n",
      "        [ 0.1794],\n",
      "        [ 0.1153],\n",
      "        [-0.1238],\n",
      "        [ 0.1814],\n",
      "        [ 0.2556],\n",
      "        [ 0.4207],\n",
      "        [ 0.5355],\n",
      "        [-0.1759],\n",
      "        [-0.3914],\n",
      "        [-0.5163],\n",
      "        [-0.1248],\n",
      "        [-0.0496],\n",
      "        [ 0.3391],\n",
      "        [ 0.0097],\n",
      "        [ 0.3739],\n",
      "        [-0.0638],\n",
      "        [ 0.7643],\n",
      "        [-0.0399],\n",
      "        [ 0.5850],\n",
      "        [-0.4564],\n",
      "        [-0.4686],\n",
      "        [-0.4959],\n",
      "        [-0.5550],\n",
      "        [ 0.4009],\n",
      "        [-0.0375],\n",
      "        [ 0.2729],\n",
      "        [ 0.2128],\n",
      "        [ 0.0929],\n",
      "        [-0.3694],\n",
      "        [-0.3330],\n",
      "        [-0.2380],\n",
      "        [ 0.0277],\n",
      "        [ 0.2128],\n",
      "        [-0.1624],\n",
      "        [-0.5943],\n",
      "        [ 0.4376],\n",
      "        [-0.0470],\n",
      "        [-0.1023],\n",
      "        [ 0.3218],\n",
      "        [-0.0726],\n",
      "        [-0.3687],\n",
      "        [ 0.4139],\n",
      "        [ 0.3454],\n",
      "        [ 0.3662],\n",
      "        [-0.1620],\n",
      "        [ 0.1954],\n",
      "        [ 0.5287],\n",
      "        [ 0.2999],\n",
      "        [-0.0279]], device='cuda:0', grad_fn=<SubBackward0>) tensor(5.6054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 8.344729\n",
      "Validation set: Average loss: 5.324718, Accuracy: 0/21534 (0%)\n",
      "\n",
      "999\n",
      "Epoch: 999\n",
      "tensor([[-0.3031],\n",
      "        [-1.7101],\n",
      "        [-1.8662],\n",
      "        [-0.3488],\n",
      "        [-1.7072],\n",
      "        [-1.4224],\n",
      "        [-2.1861],\n",
      "        [-1.8600],\n",
      "        [-0.6895],\n",
      "        [-0.5938],\n",
      "        [-1.0185],\n",
      "        [-1.3691],\n",
      "        [-3.2961],\n",
      "        [-1.2647],\n",
      "        [-1.7903],\n",
      "        [-4.1855],\n",
      "        [-0.6420],\n",
      "        [-1.3857],\n",
      "        [-2.0604],\n",
      "        [-1.6057],\n",
      "        [-1.1943],\n",
      "        [-0.4925],\n",
      "        [-0.8136],\n",
      "        [-0.3194],\n",
      "        [-2.1019],\n",
      "        [-1.1112],\n",
      "        [-1.6261],\n",
      "        [-1.0398],\n",
      "        [-2.5419],\n",
      "        [-2.9019],\n",
      "        [-2.0583],\n",
      "        [-1.4790],\n",
      "        [-1.7659],\n",
      "        [-1.0497],\n",
      "        [-0.8417],\n",
      "        [-2.9718],\n",
      "        [-2.1935],\n",
      "        [-1.2210],\n",
      "        [-0.6447],\n",
      "        [-1.5835],\n",
      "        [-0.4486],\n",
      "        [-0.8840],\n",
      "        [-2.1847],\n",
      "        [-3.3224],\n",
      "        [-0.7988],\n",
      "        [-0.3263],\n",
      "        [-1.4299],\n",
      "        [-2.0231],\n",
      "        [-1.5979],\n",
      "        [-0.5932]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-0.0436],\n",
      "        [-1.3889],\n",
      "        [-1.8150],\n",
      "        [-0.2121],\n",
      "        [-2.1525],\n",
      "        [-1.0724],\n",
      "        [-2.3127],\n",
      "        [-2.3005],\n",
      "        [-0.6907],\n",
      "        [-0.4143],\n",
      "        [-1.3152],\n",
      "        [-1.1758],\n",
      "        [-3.3412],\n",
      "        [-1.6687],\n",
      "        [-2.0037],\n",
      "        [-3.6969],\n",
      "        [-0.8460],\n",
      "        [-2.2386],\n",
      "        [-1.6707],\n",
      "        [-1.5945],\n",
      "        [-1.5640],\n",
      "        [-0.3255],\n",
      "        [-0.6086],\n",
      "        [-0.2274],\n",
      "        [-1.9239],\n",
      "        [-0.8385],\n",
      "        [-1.9431],\n",
      "        [-0.9211],\n",
      "        [-2.9656],\n",
      "        [-3.0667],\n",
      "        [-1.9170],\n",
      "        [-1.6429],\n",
      "        [-1.9329],\n",
      "        [-1.6962],\n",
      "        [-0.9630],\n",
      "        [-3.2051],\n",
      "        [-2.3799],\n",
      "        [-0.8019],\n",
      "        [-0.4984],\n",
      "        [-1.8632],\n",
      "        [-0.6907],\n",
      "        [-0.9290],\n",
      "        [-2.6954],\n",
      "        [-2.9052],\n",
      "        [-0.8422],\n",
      "        [-0.2552],\n",
      "        [-1.3505],\n",
      "        [-3.7199],\n",
      "        [-2.2621],\n",
      "        [-0.1851]], device='cuda:0') tensor([[-2.5945e-01],\n",
      "        [-3.2123e-01],\n",
      "        [-5.1238e-02],\n",
      "        [-1.3673e-01],\n",
      "        [ 4.4525e-01],\n",
      "        [-3.5003e-01],\n",
      "        [ 1.2660e-01],\n",
      "        [ 4.4042e-01],\n",
      "        [ 1.2428e-03],\n",
      "        [-1.7946e-01],\n",
      "        [ 2.9667e-01],\n",
      "        [-1.9338e-01],\n",
      "        [ 4.5101e-02],\n",
      "        [ 4.0405e-01],\n",
      "        [ 2.1339e-01],\n",
      "        [-4.8863e-01],\n",
      "        [ 2.0392e-01],\n",
      "        [ 8.5290e-01],\n",
      "        [-3.8962e-01],\n",
      "        [-1.1157e-02],\n",
      "        [ 3.6967e-01],\n",
      "        [-1.6699e-01],\n",
      "        [-2.0499e-01],\n",
      "        [-9.1950e-02],\n",
      "        [-1.7802e-01],\n",
      "        [-2.7273e-01],\n",
      "        [ 3.1707e-01],\n",
      "        [-1.1873e-01],\n",
      "        [ 4.2366e-01],\n",
      "        [ 1.6484e-01],\n",
      "        [-1.4129e-01],\n",
      "        [ 1.6388e-01],\n",
      "        [ 1.6699e-01],\n",
      "        [ 6.4650e-01],\n",
      "        [ 1.2126e-01],\n",
      "        [ 2.3331e-01],\n",
      "        [ 1.8632e-01],\n",
      "        [-4.1907e-01],\n",
      "        [-1.4632e-01],\n",
      "        [ 2.7972e-01],\n",
      "        [ 2.4219e-01],\n",
      "        [ 4.4955e-02],\n",
      "        [ 5.1068e-01],\n",
      "        [-4.1721e-01],\n",
      "        [ 4.3373e-02],\n",
      "        [-7.1089e-02],\n",
      "        [-7.9404e-02],\n",
      "        [ 1.6968e+00],\n",
      "        [ 6.6424e-01],\n",
      "        [-4.0813e-01]], device='cuda:0', grad_fn=<SubBackward0>) tensor(7.7607, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 8.269011\n",
      "Validation set: Average loss: 5.090470, Accuracy: 0/21534 (0%)\n",
      "\n",
      "1000\n",
      "Epoch: 1000\n",
      "tensor([[-0.2246],\n",
      "        [-1.1797],\n",
      "        [-1.2081],\n",
      "        [-0.2030],\n",
      "        [-1.9605],\n",
      "        [-0.4714],\n",
      "        [-1.8345],\n",
      "        [-1.8144],\n",
      "        [-0.7716],\n",
      "        [-0.6252],\n",
      "        [-1.0876],\n",
      "        [-1.1038],\n",
      "        [-2.2236],\n",
      "        [-1.2211],\n",
      "        [-1.1627],\n",
      "        [-3.7622],\n",
      "        [-0.8122],\n",
      "        [-1.7402],\n",
      "        [-1.5535],\n",
      "        [-1.8049],\n",
      "        [-0.7188],\n",
      "        [-0.1273],\n",
      "        [-0.2510],\n",
      "        [-0.4216],\n",
      "        [-2.2282],\n",
      "        [-0.8866],\n",
      "        [-2.1567],\n",
      "        [-1.2287],\n",
      "        [-3.5292],\n",
      "        [-1.2952],\n",
      "        [-2.2970],\n",
      "        [-1.2939],\n",
      "        [-2.1996],\n",
      "        [-1.6158],\n",
      "        [-0.5546],\n",
      "        [-2.2736],\n",
      "        [-2.0832],\n",
      "        [-1.1342],\n",
      "        [-0.8515],\n",
      "        [-1.3997],\n",
      "        [-0.9541],\n",
      "        [-0.9604],\n",
      "        [-2.2159],\n",
      "        [-3.3451],\n",
      "        [-1.0254],\n",
      "        [-0.3695],\n",
      "        [-1.4496],\n",
      "        [-3.4567],\n",
      "        [-1.7996],\n",
      "        [-0.5327]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([[-0.0436],\n",
      "        [-1.3889],\n",
      "        [-1.8150],\n",
      "        [-0.2121],\n",
      "        [-2.1525],\n",
      "        [-1.0724],\n",
      "        [-2.3127],\n",
      "        [-2.3005],\n",
      "        [-0.6907],\n",
      "        [-0.4143],\n",
      "        [-1.3152],\n",
      "        [-1.1758],\n",
      "        [-3.3412],\n",
      "        [-1.6687],\n",
      "        [-2.0037],\n",
      "        [-3.6969],\n",
      "        [-0.8460],\n",
      "        [-2.2386],\n",
      "        [-1.6707],\n",
      "        [-1.5945],\n",
      "        [-1.5640],\n",
      "        [-0.3255],\n",
      "        [-0.6086],\n",
      "        [-0.2274],\n",
      "        [-1.9239],\n",
      "        [-0.8385],\n",
      "        [-1.9431],\n",
      "        [-0.9211],\n",
      "        [-2.9656],\n",
      "        [-3.0667],\n",
      "        [-1.9170],\n",
      "        [-1.6429],\n",
      "        [-1.9329],\n",
      "        [-1.6962],\n",
      "        [-0.9630],\n",
      "        [-3.2051],\n",
      "        [-2.3799],\n",
      "        [-0.8019],\n",
      "        [-0.4984],\n",
      "        [-1.8632],\n",
      "        [-0.6907],\n",
      "        [-0.9290],\n",
      "        [-2.6954],\n",
      "        [-2.9052],\n",
      "        [-0.8422],\n",
      "        [-0.2552],\n",
      "        [-1.3505],\n",
      "        [-3.7199],\n",
      "        [-2.2621],\n",
      "        [-0.1851]], device='cuda:0') tensor([[-0.1810],\n",
      "        [ 0.2091],\n",
      "        [ 0.6069],\n",
      "        [ 0.0091],\n",
      "        [ 0.1920],\n",
      "        [ 0.6010],\n",
      "        [ 0.4782],\n",
      "        [ 0.4860],\n",
      "        [-0.0809],\n",
      "        [-0.2109],\n",
      "        [ 0.2276],\n",
      "        [ 0.0719],\n",
      "        [ 1.1175],\n",
      "        [ 0.4477],\n",
      "        [ 0.8410],\n",
      "        [-0.0654],\n",
      "        [ 0.0338],\n",
      "        [ 0.4984],\n",
      "        [ 0.1172],\n",
      "        [-0.2104],\n",
      "        [ 0.8451],\n",
      "        [ 0.1982],\n",
      "        [ 0.3575],\n",
      "        [-0.1941],\n",
      "        [-0.3043],\n",
      "        [-0.0481],\n",
      "        [-0.2135],\n",
      "        [-0.3076],\n",
      "        [-0.5636],\n",
      "        [ 1.7715],\n",
      "        [-0.3800],\n",
      "        [ 0.3490],\n",
      "        [-0.2667],\n",
      "        [ 0.0805],\n",
      "        [ 0.4084],\n",
      "        [ 0.9315],\n",
      "        [ 0.2966],\n",
      "        [-0.3323],\n",
      "        [-0.3531],\n",
      "        [ 0.4635],\n",
      "        [-0.2633],\n",
      "        [-0.0314],\n",
      "        [ 0.4795],\n",
      "        [-0.4399],\n",
      "        [-0.1832],\n",
      "        [-0.1143],\n",
      "        [-0.0991],\n",
      "        [ 0.2632],\n",
      "        [ 0.4626],\n",
      "        [-0.3476]], device='cuda:0', grad_fn=<SubBackward0>) tensor(11.3650, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Training set: Average loss: 8.312553\n",
      "Validation set: Average loss: 5.364169, Accuracy: 0/21534 (0%)\n",
      "\n",
      "162.90415734450022\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 1000\n",
    "print('Training on', device)\n",
    "time_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "time_end = time.time()\n",
    "total_time = (time_end-time_start)/60 \n",
    "print(total_time)"
   ]
  },
  {
   "source": [
    "# Checking How Well the Model is Preforming "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truths_train = [] \n",
    "all_preds_train = [] \n",
    "for (data,target) in train_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_train.append(target.cpu().detach().numpy())\n",
    "    all_preds_train.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_train=len(all_truths_train)-1\n",
    "\n",
    "remainder_train=len(all_truths_train[incomplete_batch_id_train])\n",
    "\n",
    "total_values_train=(len(all_truths_train)*batch_size)-(batch_size-remainder_train)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_train_array=np.zeros(total_values_train)\n",
    "all_preds_train_array=np.zeros(total_values_train)\n",
    "k=0\n",
    "while k < total_values_train:\n",
    "    for i in range(len(all_truths_train)):\n",
    "        if i<incomplete_batch_id_train:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_train\n",
    "            for j in range(remainder_train):\n",
    "                all_truths_train_array[k]=all_truths_train[i][j]\n",
    "                all_preds_train_array[k]=all_preds_train[i][j]\n",
    "                k=k+1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# all_truths_train=all_truths_train_array\n",
    "# all_preds_train=all_preds_train_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.7989362358327752\n[-1.5136962]\n-1.7989362358327752\n-1.5136961936950684\n"
     ]
    }
   ],
   "source": [
    "all_truths_test = [] \n",
    "all_preds_test = [] \n",
    "for (data,target) in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output=model(data)\n",
    "    all_truths_test.append(target.cpu().detach().numpy())\n",
    "    all_preds_test.append(output.cpu().detach().numpy()) \n",
    "\n",
    "\n",
    "incomplete_batch_id_test=len(all_truths_test)-1\n",
    "\n",
    "remainder_test=len(all_truths_test[incomplete_batch_id_test])\n",
    "\n",
    "\n",
    "total_values_test=(len(all_truths_test)*batch_size)-(batch_size-remainder_test)\n",
    "\n",
    "\n",
    "\n",
    "all_truths_test_array=np.zeros(total_values_test)\n",
    "all_preds_test_array=np.zeros(total_values_test)\n",
    "k=0\n",
    "while k < total_values_test:\n",
    "    for i in range(len(all_truths_test)):\n",
    "        if i<incomplete_batch_id_test:\n",
    "            for j in range(batch_size):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        else:\n",
    "            i=incomplete_batch_id_test\n",
    "            for j in range(remainder_test):\n",
    "                all_truths_test_array[k]=all_truths_test[i][j]\n",
    "                all_preds_test_array[k]=all_preds_test[i][j]\n",
    "                # print(i,j,k)\n",
    "                k=k+1\n",
    "                \n",
    "                \n",
    "\n",
    "print(all_truths_test[3][43])\n",
    "print(all_preds_test[3][43])\n",
    "print(all_truths_test_array[193])\n",
    "print(all_preds_test_array[193])\n",
    "# all_truths_test=all_truths_test_array\n",
    "# all_preds_test=all_preds_test_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train=r2_score(all_truths_train_array,all_preds_train_array)\n",
    "r2_test=r2_score(all_truths_test_array,all_preds_test_array)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.subplot(1,2,1) \n",
    "plt.title('Test Data Set')\n",
    "plt.scatter(all_truths_test_array, all_preds_test_array,color = 'b', alpha = 0.1*7/3)\n",
    "plt.xlabel('Test True Values')\n",
    "plt.ylabel('Test Predicted Value')\n",
    "plt.text(0,-5,'R$^2$='+ str(round(r2_test,4)))\n",
    "plt.plot([-5,1],[-5,1],'r--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Train Data Set')\n",
    "plt.scatter(all_truths_train_array, all_preds_train_array,color = 'k', alpha = 0.1)\n",
    "plt.xlabel('Train True Values')\n",
    "plt.ylabel('Train Predicted Value')\n",
    "plt.plot([-5,1],[-5,1],'r--') \n",
    "plt.text(0,-5.5,'R$^2$='+str(round(r2_train,3)))\n",
    "plt.show()\n",
    "plt.savefig('/home/juanp/Documents/SURP-2021/Plots/Model 3/20 Copies of Images.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/juanp/Documents/SURP-2021/Models/SFR_Model_20_Copies_Images')"
   ]
  },
  {
   "source": [
    "# Loss as a Function of Epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.plot(epoch_nums, np.log10(training_loss))\n",
    "# plt.plot(epoch_nums, np.log10(validation_loss))\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('Log of loss')\n",
    "# plt.legend(['training', 'validation'], loc='upper right')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Plotting Galaxy Images and their True SFR vs CNN Predicted SFR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(picture[0][0,0:,0:])\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(picture[1][0,0:,0:])\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(picture[2][0,0:,0:])\n",
    "#     plt.show()"
   ]
  }
 ]
}